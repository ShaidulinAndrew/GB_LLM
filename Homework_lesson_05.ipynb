{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4497c3ed",
   "metadata": {},
   "source": [
    "## ДЗ_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6b555",
   "metadata": {},
   "source": [
    "### Тема «POS-tagger и NER»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752064c5",
   "metadata": {},
   "source": [
    "**Задание 1. Написать теггер на данных с русским языком.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2bf4b",
   "metadata": {},
   "source": [
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации.\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов.\n",
    "3. сравнить все реализованные методы, сделать выводы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472097f2",
   "metadata": {},
   "source": [
    "**Задание 2. Проверить, насколько хорошо работает NER.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0466dd1",
   "metadata": {},
   "source": [
    "1. проверить NER из nltk/spacy/deeppavlov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d105d23",
   "metadata": {},
   "source": [
    "2. написать свой NER, попробовать разные подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb226f",
   "metadata": {},
   "source": [
    "*a. передаём в сетку токен и его соседей.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3952d",
   "metadata": {},
   "source": [
    "*b. передаём в сетку только токен.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b680e",
   "metadata": {},
   "source": [
    "*c. свой вариант.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fb742",
   "metadata": {},
   "source": [
    "3. сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f889e",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c430b1f",
   "metadata": {},
   "source": [
    "**Загрузим необходимые библиотеки и данные.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2425ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\n",
      "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29a589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-25 12:41:21--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8002::154, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 40736565 (39M) [text/plain]\n",
      "Сохранение в: «ru_syntagrus-ud-train.conllu»\n",
      "\n",
      "ru_syntagrus-ud-tra 100%[===================>]  38,85M  15,1MB/s    за 2,6s    \n",
      "\n",
      "2022-10-25 12:41:27 (15,1 MB/s) - «ru_syntagrus-ud-train.conllu» сохранён [40736565/40736565]\n",
      "\n",
      "--2022-10-25 12:41:27--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14704579 (14M) [text/plain]\n",
      "Сохранение в: «ru_syntagrus-ud-dev.conllu»\n",
      "\n",
      "ru_syntagrus-ud-dev 100%[===================>]  14,02M  14,3MB/s    за 1,0s    \n",
      "\n",
      "2022-10-25 12:41:29 (14,3 MB/s) - «ru_syntagrus-ud-dev.conllu» сохранён [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9148fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger, RegexpTagger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daf456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f171783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a1fd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24516, 8906, 8906)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849d0189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Анкета', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Начальник', 'NOUN'),\n",
       "  ('областного', 'ADJ'),\n",
       "  ('управления', 'NOUN'),\n",
       "  ('связи', 'NOUN'),\n",
       "  ('Семен', 'PROPN'),\n",
       "  ('Еремеевич', 'PROPN'),\n",
       "  ('был', 'AUX'),\n",
       "  ('человек', 'NOUN'),\n",
       "  ('простой', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('приходил', 'VERB'),\n",
       "  ('на', 'ADP'),\n",
       "  ('работу', 'NOUN'),\n",
       "  ('всегда', 'ADV'),\n",
       "  ('вовремя', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('здоровался', 'VERB'),\n",
       "  ('с', 'ADP'),\n",
       "  ('секретаршей', 'NOUN'),\n",
       "  ('за', 'ADP'),\n",
       "  ('руку', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('иногда', 'ADV'),\n",
       "  ('даже', 'PART'),\n",
       "  ('писал', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('стенгазету', 'NOUN'),\n",
       "  ('заметки', 'NOUN'),\n",
       "  ('под', 'ADP'),\n",
       "  ('псевдонимом', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('Муха', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fdata_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce401f",
   "metadata": {},
   "source": [
    "**Проверим работу всех теггеров поочередно.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a2e767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Default Tagger: 0.0,\n",
      "Unigram Tagger: 0.824,\n",
      "Bigram Tagger: 0.60939,\n",
      "Trigram Tagger: 0.178,\n",
      "Bigram and Unigram Tagger: 0.82928,\n",
      "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_acc = default_tagger.evaluate(fdata_test)\n",
    "\n",
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'Accuracy:\\nDefault Tagger: {round(default_acc, 3)},\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
    "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
    "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e279e",
   "metadata": {},
   "source": [
    "**Различные комбинации теггеров могут давать прирост качества.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8e481",
   "metadata": {},
   "source": [
    "**Для эксперимента попробуем объединить работу всех теггеров с помощью функции.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c5a6181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827905462595221"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def union_taggers(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = union_taggers(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3180f4",
   "metadata": {},
   "source": [
    "**Получили некий усредненный результат. Не самый высокий.**\n",
    "**ВЫВОД: на каждом корпусе пробовать все варианты и выбирать наилучший.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5b4c7",
   "metadata": {},
   "source": [
    "### Попробуем написать теггер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25363624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89f288",
   "metadata": {},
   "source": [
    "**Преобразуем тренировочный датасет в списки слов и списки POS-разметки.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "580036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a54080ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета', '.', 'Начальник', 'областного', 'управления', 'связи', 'Семен'],\n",
       " ['NOUN', 'PUNCT', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PROPN'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:7], train_label[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76b46219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  7, ...,  1, 11, 13])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "train_enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2e4861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  1, ...,  0,  7, 13])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_labels = le.transform(test_label)\n",
    "test_enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "075c4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "824b4045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "CountVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.94      0.92      0.93     15103\n",
      "         ADP       0.98      1.00      0.99     13717\n",
      "         ADV       0.91      0.93      0.92      7783\n",
      "         AUX       0.82      0.96      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.83      0.79      0.81      4265\n",
      "        INTJ       0.39      0.29      0.33        24\n",
      "        NOUN       0.94      0.97      0.96     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.86      0.89      0.88      1734\n",
      "        PART       0.95      0.77      0.85      5125\n",
      "        PRON       0.90      0.84      0.87      7444\n",
      "       PROPN       0.84      0.66      0.73      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.75      0.97      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.97      0.96      0.96     17110\n",
      "           X       0.43      0.07      0.13       134\n",
      "\n",
      "    accuracy                           0.94    153590\n",
      "   macro avg       0.86      0.81      0.82    153590\n",
      "weighted avg       0.94      0.94      0.94    153590\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.91      0.92     15103\n",
      "         ADP       0.99      1.00      0.99     13717\n",
      "         ADV       0.91      0.89      0.90      7783\n",
      "         AUX       0.82      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.84      0.78      0.81      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.91      0.97      0.94     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.90      0.87      1734\n",
      "        PART       0.94      0.78      0.85      5125\n",
      "        PRON       0.89      0.85      0.87      7444\n",
      "       PROPN       0.82      0.54      0.65      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.76      0.97      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.95      0.93      0.94     17110\n",
      "           X       0.24      0.13      0.17       134\n",
      "\n",
      "    accuracy                           0.93    153590\n",
      "   macro avg       0.82      0.79      0.80    153590\n",
      "weighted avg       0.93      0.93      0.93    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.82      0.80      0.81     15103\n",
      "         ADP       0.97      0.99      0.98     13717\n",
      "         ADV       0.80      0.80      0.80      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.87      1.00      0.93      5672\n",
      "         DET       0.82      0.76      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.81      0.89      0.85     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.80      0.81      1734\n",
      "        PART       0.96      0.74      0.83      5125\n",
      "        PRON       0.84      0.87      0.86      7444\n",
      "       PROPN       0.65      0.37      0.47      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.85      0.80      0.83     17110\n",
      "           X       0.22      0.04      0.06       134\n",
      "\n",
      "    accuracy                           0.87    153590\n",
      "   macro avg       0.78      0.73      0.75    153590\n",
      "weighted avg       0.87      0.87      0.87    153590\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.38      0.53     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.91      0.77      0.84      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.89      0.20      0.33      5672\n",
      "         DET       0.92      0.59      0.71      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.87      0.55      0.68      1734\n",
      "        PART       0.96      0.73      0.83      5125\n",
      "        PRON       0.79      0.85      0.82      7444\n",
      "       PROPN       0.93      0.15      0.25      5473\n",
      "       PUNCT       0.36      1.00      0.53     29186\n",
      "       SCONJ       0.78      0.78      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.44      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.63    153590\n",
      "   macro avg       0.67      0.47      0.51    153590\n",
      "weighted avg       0.83      0.63      0.64    153590\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.41      0.57     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.94      0.77      0.85      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.88      0.20      0.33      5672\n",
      "         DET       0.91      0.61      0.73      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.88      0.55      0.68      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.79      0.84      0.82      7444\n",
      "       PROPN       0.93      0.16      0.27      5473\n",
      "       PUNCT       0.37      1.00      0.54     29186\n",
      "       SCONJ       0.78      0.85      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.44      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.64    153590\n",
      "   macro avg       0.67      0.48      0.52    153590\n",
      "weighted avg       0.83      0.64      0.65    153590\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.41      0.20      0.27     15103\n",
      "         ADP       0.83      0.47      0.60     13717\n",
      "         ADV       0.56      0.62      0.59      7783\n",
      "         AUX       0.72      0.86      0.78      1390\n",
      "       CCONJ       0.88      0.18      0.29      5672\n",
      "         DET       0.49      0.65      0.56      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.25      0.53      0.34     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.39      0.43      0.41      1734\n",
      "        PART       0.84      0.75      0.79      5125\n",
      "        PRON       0.68      0.68      0.68      7444\n",
      "       PROPN       0.28      0.08      0.12      5473\n",
      "       PUNCT       0.00      0.00      0.00     29186\n",
      "       SCONJ       0.66      0.96      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.45      0.25      0.32     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.36    153590\n",
      "   macro avg       0.41      0.37      0.36    153590\n",
      "weighted avg       0.39      0.36      0.34    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.86      0.86      0.86     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.87      0.82      0.84      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.83      0.77      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.85      0.92      0.89     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.85      0.84      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.85      0.87      0.86      7444\n",
      "       PROPN       0.69      0.41      0.52      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.89      0.87      0.88     17110\n",
      "           X       0.30      0.04      0.08       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.80      0.75      0.76    153590\n",
      "weighted avg       0.89      0.90      0.89    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.88      0.84      0.86      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.88      0.98      0.93      5672\n",
      "         DET       0.85      0.76      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.87      0.93      0.90     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.83      0.84      1734\n",
      "        PART       0.94      0.76      0.84      5125\n",
      "        PRON       0.83      0.87      0.85      7444\n",
      "       PROPN       0.73      0.42      0.53      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.82      0.90        62\n",
      "        VERB       0.89      0.88      0.89     17110\n",
      "           X       0.53      0.06      0.11       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.82      0.76      0.77    153590\n",
      "weighted avg       0.90      0.90      0.90    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.89      0.89     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.90      0.84      0.87      7783\n",
      "         AUX       0.82      0.97      0.89      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.88      0.72      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.88      0.95      0.91     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.84      0.87      0.85      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.83      0.90      0.86      7444\n",
      "       PROPN       0.79      0.46      0.58      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.92      0.91      0.91     17110\n",
      "           X       0.29      0.07      0.12       134\n",
      "\n",
      "    accuracy                           0.91    153590\n",
      "   macro avg       0.81      0.77      0.78    153590\n",
      "weighted avg       0.91      0.91      0.91    153590\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.47      0.25      0.32     15103\n",
      "         ADP       0.89      0.47      0.62     13717\n",
      "         ADV       0.66      0.67      0.66      7783\n",
      "         AUX       0.75      0.94      0.84      1390\n",
      "       CCONJ       0.89      0.18      0.31      5672\n",
      "         DET       0.64      0.57      0.60      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.60      0.58      0.59     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.51      0.48      0.49      1734\n",
      "        PART       0.91      0.74      0.82      5125\n",
      "        PRON       0.71      0.79      0.75      7444\n",
      "       PROPN       0.37      0.09      0.15      5473\n",
      "       PUNCT       0.48      1.00      0.65     29186\n",
      "       SCONJ       0.76      0.90      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.54      0.30      0.38     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.58    153590\n",
      "   macro avg       0.51      0.44      0.44    153590\n",
      "weighted avg       0.61      0.58      0.55    153590\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.53      0.28      0.37     15103\n",
      "         ADP       0.92      0.47      0.62     13717\n",
      "         ADV       0.74      0.70      0.72      7783\n",
      "         AUX       0.77      0.94      0.85      1390\n",
      "       CCONJ       0.93      0.18      0.30      5672\n",
      "         DET       0.68      0.62      0.65      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.64      0.59      0.61     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.58      0.50      0.54      1734\n",
      "        PART       0.89      0.77      0.82      5125\n",
      "        PRON       0.76      0.78      0.77      7444\n",
      "       PROPN       0.42      0.12      0.18      5473\n",
      "       PUNCT       0.46      1.00      0.63     29186\n",
      "       SCONJ       0.73      0.95      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.59      0.32      0.42     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.59    153590\n",
      "   macro avg       0.54      0.46      0.46    153590\n",
      "weighted avg       0.64      0.59      0.57    153590\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.60      0.31      0.41     15103\n",
      "         ADP       0.92      0.48      0.63     13717\n",
      "         ADV       0.80      0.70      0.75      7783\n",
      "         AUX       0.81      0.86      0.83      1390\n",
      "       CCONJ       0.85      0.20      0.33      5672\n",
      "         DET       0.77      0.55      0.64      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.70      0.58      0.64     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.63      0.49      0.55      1734\n",
      "        PART       0.96      0.72      0.82      5125\n",
      "        PRON       0.74      0.84      0.79      7444\n",
      "       PROPN       0.51      0.14      0.22      5473\n",
      "       PUNCT       0.43      1.00      0.60     29186\n",
      "       SCONJ       0.76      0.91      0.83      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.65      0.35      0.46     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.60    153590\n",
      "   macro avg       0.56      0.45      0.47    153590\n",
      "weighted avg       0.67      0.60      0.58    153590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "vectorizers = [CountVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels, pred)\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ef775",
   "metadata": {},
   "source": [
    "**Для удобства представим данные в виде таблицы:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58394e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd3a78e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.938119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.927187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.909308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.898586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.892840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.866751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.650066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.644079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.581934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.568540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.550696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.341387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  f1_score\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.938119\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.927187\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.909308\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.898586\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.892840\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.866751\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.650066\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.644079\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.581934\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.568540\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.550696\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.341387"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'f1_score': f1_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5b04824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.929390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.912540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.902110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.896243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.870571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.639729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.634520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.598301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.592408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.579068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.360648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  Accuracy\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.939469\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.929390\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.912540\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.902110\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.896243\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.870571\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.639729\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.634520\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.598301\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.592408\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.579068\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.360648"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'Accuracy': accuracy_scores})\n",
    "result_model_acc.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818cfc2c",
   "metadata": {},
   "source": [
    "### В результате эксперимента проверили различные векторайзеры для разного количества символов. Наилучший результат показали символьные N-граммы. Результат N-грамм для слов оказался значительно хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff2cba",
   "metadata": {},
   "source": [
    "### Задание 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14f25a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corus in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.9.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3f9531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.5.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.2-cp38-cp38-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-macosx_10_9_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: setuptools in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (63.4.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp38-cp38-macosx_10_9_x86_64.whl (489 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp38-cp38-macosx_10_9_x86_64.whl (754 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.4/754.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m402.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.2.0\n",
      "    Uninstalling smart-open-6.2.0:\n",
      "      Successfully uninstalled smart-open-6.2.0\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 typer-0.4.2 wasabi-0.10.1\n",
      "2022-10-25 14:46:38.086876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.4.2                         \n",
      "Location         /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/spacy\n",
      "Platform         macOS-10.15.7-x86_64-i386-64bit\n",
      "Python version   3.8.13                        \n",
      "Pipelines                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel\n",
    "!pip install -U spacy\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7b9eb",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18e78eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "import pyconll\n",
    "import corus\n",
    "from corus import load_ne5\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3ec0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18f358fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92fce672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-25 16:06:23--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
      "Распознаётся www.labinform.ru (www.labinform.ru)… 95.181.230.181\n",
      "Подключение к www.labinform.ru (www.labinform.ru)|95.181.230.181|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 1899530 (1,8M) [application/zip]\n",
      "Сохранение в: «collection5.zip»\n",
      "\n",
      "collection5.zip     100%[===================>]   1,81M  10,6MB/s    за 0,2s    \n",
      "\n",
      "2022-10-25 16:06:23 (10,6 MB/s) - «collection5.zip» сохранён [1899530/1899530]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c19ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  collection5.zip\n",
      "   creating: Collection5/\n",
      "  inflating: Collection5/001.ann     \n",
      "  inflating: Collection5/001.txt     \n",
      "  inflating: Collection5/002.ann     \n",
      "  inflating: Collection5/002.txt     \n",
      "  inflating: Collection5/003.ann     \n",
      "  inflating: Collection5/003.txt     \n",
      "  inflating: Collection5/004.ann     \n",
      "  inflating: Collection5/004.txt     \n",
      "  inflating: Collection5/005.ann     \n",
      "  inflating: Collection5/005.txt     \n",
      "  inflating: Collection5/006.ann     \n",
      "  inflating: Collection5/006.txt     \n",
      "  inflating: Collection5/007.ann     \n",
      "  inflating: Collection5/007.txt     \n",
      "  inflating: Collection5/008.ann     \n",
      "  inflating: Collection5/008.txt     \n",
      "  inflating: Collection5/009.ann     \n",
      "  inflating: Collection5/009.txt     \n",
      "  inflating: Collection5/010.ann     \n",
      "  inflating: Collection5/010.txt     \n",
      "  inflating: Collection5/011.ann     \n",
      "  inflating: Collection5/011.txt     \n",
      "  inflating: Collection5/012.ann     \n",
      "  inflating: Collection5/012.txt     \n",
      "  inflating: Collection5/013.ann     \n",
      "  inflating: Collection5/013.txt     \n",
      "  inflating: Collection5/014.ann     \n",
      "  inflating: Collection5/014.txt     \n",
      "  inflating: Collection5/015 (!).ann  \n",
      "  inflating: Collection5/015 (!).txt  \n",
      "  inflating: Collection5/016.ann     \n",
      "  inflating: Collection5/016.txt     \n",
      "  inflating: Collection5/017.ann     \n",
      "  inflating: Collection5/017.txt     \n",
      "  inflating: Collection5/018.ann     \n",
      "  inflating: Collection5/018.txt     \n",
      "  inflating: Collection5/019.ann     \n",
      "  inflating: Collection5/019.txt     \n",
      "  inflating: Collection5/020.ann     \n",
      "  inflating: Collection5/020.txt     \n",
      "  inflating: Collection5/021.ann     \n",
      "  inflating: Collection5/021.txt     \n",
      "  inflating: Collection5/022.ann     \n",
      "  inflating: Collection5/022.txt     \n",
      "  inflating: Collection5/023.ann     \n",
      "  inflating: Collection5/023.txt     \n",
      "  inflating: Collection5/025.ann     \n",
      "  inflating: Collection5/025.txt     \n",
      "  inflating: Collection5/026.ann     \n",
      "  inflating: Collection5/026.txt     \n",
      "  inflating: Collection5/027.ann     \n",
      "  inflating: Collection5/027.txt     \n",
      "  inflating: Collection5/028.ann     \n",
      "  inflating: Collection5/028.txt     \n",
      "  inflating: Collection5/029.ann     \n",
      "  inflating: Collection5/029.txt     \n",
      "  inflating: Collection5/030.ann     \n",
      "  inflating: Collection5/030.txt     \n",
      "  inflating: Collection5/031.ann     \n",
      "  inflating: Collection5/031.txt     \n",
      "  inflating: Collection5/032.ann     \n",
      "  inflating: Collection5/032.txt     \n",
      "  inflating: Collection5/033.ann     \n",
      "  inflating: Collection5/033.txt     \n",
      "  inflating: Collection5/034.ann     \n",
      "  inflating: Collection5/034.txt     \n",
      "  inflating: Collection5/035.ann     \n",
      "  inflating: Collection5/035.txt     \n",
      "  inflating: Collection5/036.ann     \n",
      "  inflating: Collection5/036.txt     \n",
      "  inflating: Collection5/037.ann     \n",
      "  inflating: Collection5/037.txt     \n",
      "  inflating: Collection5/038.ann     \n",
      "  inflating: Collection5/038.txt     \n",
      "  inflating: Collection5/039.ann     \n",
      "  inflating: Collection5/039.txt     \n",
      "  inflating: Collection5/03_12_12a.ann  \n",
      "  inflating: Collection5/03_12_12a.txt  \n",
      "  inflating: Collection5/03_12_12b.ann  \n",
      "  inflating: Collection5/03_12_12b.txt  \n",
      "  inflating: Collection5/03_12_12c.ann  \n",
      "  inflating: Collection5/03_12_12c.txt  \n",
      "  inflating: Collection5/03_12_12d.ann  \n",
      "  inflating: Collection5/03_12_12d.txt  \n",
      "  inflating: Collection5/03_12_12g.ann  \n",
      "  inflating: Collection5/03_12_12g.txt  \n",
      "  inflating: Collection5/03_12_12h.ann  \n",
      "  inflating: Collection5/03_12_12h.txt  \n",
      "  inflating: Collection5/040.ann     \n",
      "  inflating: Collection5/040.txt     \n",
      "  inflating: Collection5/041.ann     \n",
      "  inflating: Collection5/041.txt     \n",
      "  inflating: Collection5/042.ann     \n",
      "  inflating: Collection5/042.txt     \n",
      "  inflating: Collection5/043.ann     \n",
      "  inflating: Collection5/043.txt     \n",
      "  inflating: Collection5/044.ann     \n",
      "  inflating: Collection5/044.txt     \n",
      "  inflating: Collection5/045.ann     \n",
      "  inflating: Collection5/045.txt     \n",
      "  inflating: Collection5/046.ann     \n",
      "  inflating: Collection5/046.txt     \n",
      "  inflating: Collection5/047.ann     \n",
      "  inflating: Collection5/047.txt     \n",
      "  inflating: Collection5/048.ann     \n",
      "  inflating: Collection5/048.txt     \n",
      "  inflating: Collection5/049.ann     \n",
      "  inflating: Collection5/049.txt     \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.ann  \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.txt  \n",
      "  inflating: Collection5/04_03_13a_sorokin.ann  \n",
      "  inflating: Collection5/04_03_13a_sorokin.txt  \n",
      "  inflating: Collection5/04_12_12b.ann  \n",
      "  inflating: Collection5/04_12_12b.txt  \n",
      "  inflating: Collection5/04_12_12d.ann  \n",
      "  inflating: Collection5/04_12_12d.txt  \n",
      "  inflating: Collection5/04_12_12f.ann  \n",
      "  inflating: Collection5/04_12_12f.txt  \n",
      "  inflating: Collection5/04_12_12g.ann  \n",
      "  inflating: Collection5/04_12_12g.txt  \n",
      "  inflating: Collection5/04_12_12h_corr.ann  \n",
      "  inflating: Collection5/04_12_12h_corr.txt  \n",
      "  inflating: Collection5/050.ann     \n",
      "  inflating: Collection5/050.txt     \n",
      "  inflating: Collection5/051.ann     \n",
      "  inflating: Collection5/051.txt     \n",
      "  inflating: Collection5/052.ann     \n",
      "  inflating: Collection5/052.txt     \n",
      "  inflating: Collection5/053.ann     \n",
      "  inflating: Collection5/053.txt     \n",
      "  inflating: Collection5/054.ann     \n",
      "  inflating: Collection5/054.txt     \n",
      "  inflating: Collection5/055.ann     \n",
      "  inflating: Collection5/055.txt     \n",
      "  inflating: Collection5/056.ann     \n",
      "  inflating: Collection5/056.txt     \n",
      "  inflating: Collection5/057.ann     \n",
      "  inflating: Collection5/057.txt     \n",
      "  inflating: Collection5/058.ann     \n",
      "  inflating: Collection5/058.txt     \n",
      "  inflating: Collection5/059.ann     \n",
      "  inflating: Collection5/059.txt     \n",
      "  inflating: Collection5/060.ann     \n",
      "  inflating: Collection5/060.txt     \n",
      "  inflating: Collection5/061.ann     \n",
      "  inflating: Collection5/061.txt     \n",
      "  inflating: Collection5/062.ann     \n",
      "  inflating: Collection5/062.txt     \n",
      "  inflating: Collection5/063.ann     \n",
      "  inflating: Collection5/063.txt     \n",
      "  inflating: Collection5/064.ann     \n",
      "  inflating: Collection5/064.txt     \n",
      "  inflating: Collection5/065.ann     \n",
      "  inflating: Collection5/065.txt     \n",
      "  inflating: Collection5/066.ann     \n",
      "  inflating: Collection5/066.txt     \n",
      "  inflating: Collection5/067.ann     \n",
      "  inflating: Collection5/067.txt     \n",
      "  inflating: Collection5/068.ann     \n",
      "  inflating: Collection5/068.txt     \n",
      "  inflating: Collection5/069.ann     \n",
      "  inflating: Collection5/069.txt     \n",
      "  inflating: Collection5/070.ann     \n",
      "  inflating: Collection5/070.txt     \n",
      "  inflating: Collection5/071.ann     \n",
      "  inflating: Collection5/071.txt     \n",
      "  inflating: Collection5/072.ann     \n",
      "  inflating: Collection5/072.txt     \n",
      "  inflating: Collection5/073.ann     \n",
      "  inflating: Collection5/073.txt     \n",
      "  inflating: Collection5/074.ann     \n",
      "  inflating: Collection5/074.txt     \n",
      "  inflating: Collection5/075.ann     \n",
      "  inflating: Collection5/075.txt     \n",
      "  inflating: Collection5/076.ann     \n",
      "  inflating: Collection5/076.txt     \n",
      "  inflating: Collection5/077.ann     \n",
      "  inflating: Collection5/077.txt     \n",
      "  inflating: Collection5/078.ann     \n",
      "  inflating: Collection5/078.txt     \n",
      "  inflating: Collection5/079.ann     \n",
      "  inflating: Collection5/079.txt     \n",
      "  inflating: Collection5/080.ann     \n",
      "  inflating: Collection5/080.txt     \n",
      "  inflating: Collection5/081.ann     \n",
      "  inflating: Collection5/081.txt     \n",
      "  inflating: Collection5/082.ann     \n",
      "  inflating: Collection5/082.txt     \n",
      "  inflating: Collection5/083.ann     \n",
      "  inflating: Collection5/083.txt     \n",
      "  inflating: Collection5/084.ann     \n",
      "  inflating: Collection5/084.txt     \n",
      "  inflating: Collection5/085.ann     \n",
      "  inflating: Collection5/085.txt     \n",
      "  inflating: Collection5/086.ann     \n",
      "  inflating: Collection5/086.txt     \n",
      "  inflating: Collection5/087.ann     \n",
      "  inflating: Collection5/087.txt     \n",
      "  inflating: Collection5/088.ann     \n",
      "  inflating: Collection5/088.txt     \n",
      "  inflating: Collection5/089.ann     \n",
      "  inflating: Collection5/089.txt     \n",
      "  inflating: Collection5/090.ann     \n",
      "  inflating: Collection5/090.txt     \n",
      "  inflating: Collection5/091.ann     \n",
      "  inflating: Collection5/091.txt     \n",
      "  inflating: Collection5/092.ann     \n",
      "  inflating: Collection5/092.txt     \n",
      "  inflating: Collection5/093.ann     \n",
      "  inflating: Collection5/093.txt     \n",
      "  inflating: Collection5/094.ann     \n",
      "  inflating: Collection5/094.txt     \n",
      "  inflating: Collection5/095.ann     \n",
      "  inflating: Collection5/095.txt     \n",
      "  inflating: Collection5/096.ann     \n",
      "  inflating: Collection5/096.txt     \n",
      "  inflating: Collection5/097.ann     \n",
      "  inflating: Collection5/097.txt     \n",
      "  inflating: Collection5/098.ann     \n",
      "  inflating: Collection5/098.txt     \n",
      "  inflating: Collection5/099.ann     \n",
      "  inflating: Collection5/099.txt     \n",
      "  inflating: Collection5/09_01_13.ann  \n",
      "  inflating: Collection5/09_01_13.txt  \n",
      "  inflating: Collection5/09_01_13a.ann  \n",
      "  inflating: Collection5/09_01_13a.txt  \n",
      "  inflating: Collection5/09_01_13c.ann  \n",
      "  inflating: Collection5/09_01_13c.txt  \n",
      "  inflating: Collection5/09_01_13d.ann  \n",
      "  inflating: Collection5/09_01_13d.txt  \n",
      "  inflating: Collection5/09_01_13e.ann  \n",
      "  inflating: Collection5/09_01_13e.txt  \n",
      "  inflating: Collection5/09_01_13h.ann  \n",
      "  inflating: Collection5/09_01_13h.txt  \n",
      "  inflating: Collection5/09_01_13i.ann  \n",
      "  inflating: Collection5/09_01_13i.txt  \n",
      "  inflating: Collection5/100.ann     \n",
      "  inflating: Collection5/100.txt     \n",
      "  inflating: Collection5/1000.ann    \n",
      "  inflating: Collection5/1000.txt    \n",
      "  inflating: Collection5/1001.ann    \n",
      "  inflating: Collection5/1001.txt    \n",
      "  inflating: Collection5/1002.ann    \n",
      "  inflating: Collection5/1002.txt    \n",
      "  inflating: Collection5/1003.ann    \n",
      "  inflating: Collection5/1003.txt    \n",
      "  inflating: Collection5/1004.ann    \n",
      "  inflating: Collection5/1004.txt    \n",
      "  inflating: Collection5/1005.ann    \n",
      "  inflating: Collection5/1005.txt    \n",
      "  inflating: Collection5/1006.ann    \n",
      "  inflating: Collection5/1006.txt    \n",
      "  inflating: Collection5/1007.ann    \n",
      "  inflating: Collection5/1007.txt    \n",
      "  inflating: Collection5/1008.ann    \n",
      "  inflating: Collection5/1008.txt    \n",
      "  inflating: Collection5/1009.ann    \n",
      "  inflating: Collection5/1009.txt    \n",
      "  inflating: Collection5/101.ann     \n",
      "  inflating: Collection5/101.txt     \n",
      "  inflating: Collection5/1010.ann    \n",
      "  inflating: Collection5/1010.txt    \n",
      "  inflating: Collection5/1011.ann    \n",
      "  inflating: Collection5/1011.txt    \n",
      "  inflating: Collection5/1012.ann    \n",
      "  inflating: Collection5/1012.txt    \n",
      "  inflating: Collection5/1013.ann    \n",
      "  inflating: Collection5/1013.txt    \n",
      "  inflating: Collection5/1014.ann    \n",
      "  inflating: Collection5/1014.txt    \n",
      "  inflating: Collection5/1015.ann    \n",
      "  inflating: Collection5/1015.txt    \n",
      "  inflating: Collection5/1016.ann    \n",
      "  inflating: Collection5/1016.txt    \n",
      "  inflating: Collection5/1017.ann    \n",
      "  inflating: Collection5/1017.txt    \n",
      "  inflating: Collection5/1018.ann    \n",
      "  inflating: Collection5/1018.txt    \n",
      "  inflating: Collection5/1019.ann    \n",
      "  inflating: Collection5/1019.txt    \n",
      "  inflating: Collection5/102.ann     \n",
      "  inflating: Collection5/102.txt     \n",
      "  inflating: Collection5/1020.ann    \n",
      "  inflating: Collection5/1020.txt    \n",
      "  inflating: Collection5/1021.ann    \n",
      "  inflating: Collection5/1021.txt    \n",
      "  inflating: Collection5/1022.ann    \n",
      "  inflating: Collection5/1022.txt    \n",
      "  inflating: Collection5/1023.ann    \n",
      "  inflating: Collection5/1023.txt    \n",
      "  inflating: Collection5/1024.ann    \n",
      "  inflating: Collection5/1024.txt    \n",
      "  inflating: Collection5/1025.ann    \n",
      "  inflating: Collection5/1025.txt    \n",
      "  inflating: Collection5/1026.ann    \n",
      "  inflating: Collection5/1026.txt    \n",
      "  inflating: Collection5/1027.ann    \n",
      "  inflating: Collection5/1027.txt    \n",
      "  inflating: Collection5/1028.ann    \n",
      "  inflating: Collection5/1028.txt    \n",
      "  inflating: Collection5/1029.ann    \n",
      "  inflating: Collection5/1029.txt    \n",
      "  inflating: Collection5/103.ann     \n",
      "  inflating: Collection5/103.txt     \n",
      "  inflating: Collection5/1030.ann    \n",
      "  inflating: Collection5/1030.txt    \n",
      "  inflating: Collection5/1031.ann    \n",
      "  inflating: Collection5/1031.txt    \n",
      "  inflating: Collection5/1032.ann    \n",
      "  inflating: Collection5/1032.txt    \n",
      "  inflating: Collection5/1033.ann    \n",
      "  inflating: Collection5/1033.txt    \n",
      "  inflating: Collection5/1034.ann    \n",
      "  inflating: Collection5/1034.txt    \n",
      "  inflating: Collection5/1035.ann    \n",
      "  inflating: Collection5/1035.txt    \n",
      "  inflating: Collection5/1036.ann    \n",
      "  inflating: Collection5/1036.txt    \n",
      "  inflating: Collection5/1037.ann    \n",
      "  inflating: Collection5/1037.txt    \n",
      "  inflating: Collection5/1038.ann    \n",
      "  inflating: Collection5/1038.txt    \n",
      "  inflating: Collection5/1039.ann    \n",
      "  inflating: Collection5/1039.txt    \n",
      "  inflating: Collection5/104.ann     \n",
      "  inflating: Collection5/104.txt     \n",
      "  inflating: Collection5/1040.ann    \n",
      "  inflating: Collection5/1040.txt    \n",
      "  inflating: Collection5/1041.ann    \n",
      "  inflating: Collection5/1041.txt    \n",
      "  inflating: Collection5/1042.ann    \n",
      "  inflating: Collection5/1042.txt    \n",
      "  inflating: Collection5/1043.ann    \n",
      "  inflating: Collection5/1043.txt    \n",
      "  inflating: Collection5/1044.ann    \n",
      "  inflating: Collection5/1044.txt    \n",
      "  inflating: Collection5/1045.ann    \n",
      "  inflating: Collection5/1045.txt    \n",
      "  inflating: Collection5/1046.ann    \n",
      "  inflating: Collection5/1046.txt    \n",
      "  inflating: Collection5/1047.ann    \n",
      "  inflating: Collection5/1047.txt    \n",
      "  inflating: Collection5/1048.ann    \n",
      "  inflating: Collection5/1048.txt    \n",
      "  inflating: Collection5/1049.ann    \n",
      "  inflating: Collection5/1049.txt    \n",
      "  inflating: Collection5/105.ann     \n",
      "  inflating: Collection5/105.txt     \n",
      "  inflating: Collection5/1050.ann    \n",
      "  inflating: Collection5/1050.txt    \n",
      "  inflating: Collection5/106.ann     \n",
      "  inflating: Collection5/106.txt     \n",
      "  inflating: Collection5/107.ann     \n",
      "  inflating: Collection5/107.txt     \n",
      "  inflating: Collection5/108.ann     \n",
      "  inflating: Collection5/108.txt     \n",
      "  inflating: Collection5/109.ann     \n",
      "  inflating: Collection5/109.txt     \n",
      "  inflating: Collection5/10_01_13a.ann  \n",
      "  inflating: Collection5/10_01_13a.txt  \n",
      "  inflating: Collection5/10_01_13d.ann  \n",
      "  inflating: Collection5/10_01_13d.txt  \n",
      "  inflating: Collection5/10_01_13i.ann  \n",
      "  inflating: Collection5/10_01_13i.txt  \n",
      "  inflating: Collection5/110.ann     \n",
      "  inflating: Collection5/110.txt     \n",
      "  inflating: Collection5/1100.ann    \n",
      "  inflating: Collection5/1100.txt    \n",
      "  inflating: Collection5/1101.ann    \n",
      "  inflating: Collection5/1101.txt    \n",
      "  inflating: Collection5/1102.ann    \n",
      "  inflating: Collection5/1102.txt    \n",
      "  inflating: Collection5/1103.ann    \n",
      "  inflating: Collection5/1103.txt    \n",
      "  inflating: Collection5/1104.ann    \n",
      "  inflating: Collection5/1104.txt    \n",
      "  inflating: Collection5/1105.ann    \n",
      "  inflating: Collection5/1105.txt    \n",
      "  inflating: Collection5/1106.ann    \n",
      "  inflating: Collection5/1106.txt    \n",
      "  inflating: Collection5/1107.ann    \n",
      "  inflating: Collection5/1107.txt    \n",
      "  inflating: Collection5/1108.ann    \n",
      "  inflating: Collection5/1108.txt    \n",
      "  inflating: Collection5/1109.ann    \n",
      "  inflating: Collection5/1109.txt    \n",
      "  inflating: Collection5/111.ann     \n",
      "  inflating: Collection5/111.txt     \n",
      "  inflating: Collection5/1110.ann    \n",
      "  inflating: Collection5/1110.txt    \n",
      "  inflating: Collection5/1111.ann    \n",
      "  inflating: Collection5/1111.txt    \n",
      "  inflating: Collection5/1112.ann    \n",
      "  inflating: Collection5/1112.txt    \n",
      "  inflating: Collection5/1113.ann    \n",
      "  inflating: Collection5/1113.txt    \n",
      "  inflating: Collection5/1114.ann    \n",
      "  inflating: Collection5/1114.txt    \n",
      "  inflating: Collection5/1115.ann    \n",
      "  inflating: Collection5/1115.txt    \n",
      "  inflating: Collection5/1116.ann    \n",
      "  inflating: Collection5/1116.txt    \n",
      "  inflating: Collection5/1117.ann    \n",
      "  inflating: Collection5/1117.txt    \n",
      "  inflating: Collection5/1118.ann    \n",
      "  inflating: Collection5/1118.txt    \n",
      "  inflating: Collection5/1119.ann    \n",
      "  inflating: Collection5/1119.txt    \n",
      "  inflating: Collection5/112.ann     \n",
      "  inflating: Collection5/112.txt     \n",
      "  inflating: Collection5/1120.ann    \n",
      "  inflating: Collection5/1120.txt    \n",
      "  inflating: Collection5/1121.ann    \n",
      "  inflating: Collection5/1121.txt    \n",
      "  inflating: Collection5/1122.ann    \n",
      "  inflating: Collection5/1122.txt    \n",
      "  inflating: Collection5/1123.ann    \n",
      "  inflating: Collection5/1123.txt    \n",
      "  inflating: Collection5/1124.ann    \n",
      "  inflating: Collection5/1124.txt    \n",
      "  inflating: Collection5/1125.ann    \n",
      "  inflating: Collection5/1125.txt    \n",
      "  inflating: Collection5/1126.ann    \n",
      "  inflating: Collection5/1126.txt    \n",
      "  inflating: Collection5/1127.ann    \n",
      "  inflating: Collection5/1127.txt    \n",
      "  inflating: Collection5/1128.ann    \n",
      "  inflating: Collection5/1128.txt    \n",
      "  inflating: Collection5/113.ann     \n",
      "  inflating: Collection5/113.txt     \n",
      "  inflating: Collection5/1130.ann    \n",
      "  inflating: Collection5/1130.txt    \n",
      "  inflating: Collection5/1131.ann    \n",
      "  inflating: Collection5/1131.txt    \n",
      "  inflating: Collection5/1132.ann    \n",
      "  inflating: Collection5/1132.txt    \n",
      "  inflating: Collection5/1133.ann    \n",
      "  inflating: Collection5/1133.txt    \n",
      "  inflating: Collection5/1134.ann    \n",
      "  inflating: Collection5/1134.txt    \n",
      "  inflating: Collection5/1135.ann    \n",
      "  inflating: Collection5/1135.txt    \n",
      "  inflating: Collection5/1136.ann    \n",
      "  inflating: Collection5/1136.txt    \n",
      "  inflating: Collection5/1137.ann    \n",
      "  inflating: Collection5/1137.txt    \n",
      "  inflating: Collection5/1138.ann    \n",
      "  inflating: Collection5/1138.txt    \n",
      "  inflating: Collection5/1139.ann    \n",
      "  inflating: Collection5/1139.txt    \n",
      "  inflating: Collection5/114.ann     \n",
      "  inflating: Collection5/114.txt     \n",
      "  inflating: Collection5/1140.ann    \n",
      "  inflating: Collection5/1140.txt    \n",
      "  inflating: Collection5/1141.ann    \n",
      "  inflating: Collection5/1141.txt    \n",
      "  inflating: Collection5/1142.ann    \n",
      "  inflating: Collection5/1142.txt    \n",
      "  inflating: Collection5/1143.ann    \n",
      "  inflating: Collection5/1143.txt    \n",
      "  inflating: Collection5/1144.ann    \n",
      "  inflating: Collection5/1144.txt    \n",
      "  inflating: Collection5/1145.ann    \n",
      "  inflating: Collection5/1145.txt    \n",
      "  inflating: Collection5/1146.ann    \n",
      "  inflating: Collection5/1146.txt    \n",
      "  inflating: Collection5/1147.ann    \n",
      "  inflating: Collection5/1147.txt    \n",
      "  inflating: Collection5/1148.ann    \n",
      "  inflating: Collection5/1148.txt    \n",
      "  inflating: Collection5/1149.ann    \n",
      "  inflating: Collection5/1149.txt    \n",
      "  inflating: Collection5/115.ann     \n",
      "  inflating: Collection5/115.txt     \n",
      "  inflating: Collection5/1150.ann    \n",
      "  inflating: Collection5/1150.txt    \n",
      "  inflating: Collection5/1151.ann    \n",
      "  inflating: Collection5/1151.txt    \n",
      "  inflating: Collection5/1152.ann    \n",
      "  inflating: Collection5/1152.txt    \n",
      "  inflating: Collection5/1153.ann    \n",
      "  inflating: Collection5/1153.txt    \n",
      "  inflating: Collection5/1154.ann    \n",
      "  inflating: Collection5/1154.txt    \n",
      "  inflating: Collection5/1155.ann    \n",
      "  inflating: Collection5/1155.txt    \n",
      "  inflating: Collection5/1156.ann    \n",
      "  inflating: Collection5/1156.txt    \n",
      "  inflating: Collection5/1157.ann    \n",
      "  inflating: Collection5/1157.txt    \n",
      "  inflating: Collection5/1158.ann    \n",
      "  inflating: Collection5/1158.txt    \n",
      "  inflating: Collection5/1159.ann    \n",
      "  inflating: Collection5/1159.txt    \n",
      "  inflating: Collection5/116.ann     \n",
      "  inflating: Collection5/116.txt     \n",
      "  inflating: Collection5/1160.ann    \n",
      "  inflating: Collection5/1160.txt    \n",
      "  inflating: Collection5/1161.ann    \n",
      "  inflating: Collection5/1161.txt    \n",
      "  inflating: Collection5/1162.ann    \n",
      "  inflating: Collection5/1162.txt    \n",
      "  inflating: Collection5/1163.ann    \n",
      "  inflating: Collection5/1163.txt    \n",
      "  inflating: Collection5/1164.ann    \n",
      "  inflating: Collection5/1164.txt    \n",
      "  inflating: Collection5/1165.ann    \n",
      "  inflating: Collection5/1165.txt    \n",
      "  inflating: Collection5/1166.ann    \n",
      "  inflating: Collection5/1166.txt    \n",
      "  inflating: Collection5/1167.ann    \n",
      "  inflating: Collection5/1167.txt    \n",
      "  inflating: Collection5/1168.ann    \n",
      "  inflating: Collection5/1168.txt    \n",
      "  inflating: Collection5/1169.ann    \n",
      "  inflating: Collection5/1169.txt    \n",
      "  inflating: Collection5/117.ann     \n",
      "  inflating: Collection5/117.txt     \n",
      "  inflating: Collection5/1170.ann    \n",
      "  inflating: Collection5/1170.txt    \n",
      "  inflating: Collection5/1171.ann    \n",
      "  inflating: Collection5/1171.txt    \n",
      "  inflating: Collection5/1172.ann    \n",
      "  inflating: Collection5/1172.txt    \n",
      "  inflating: Collection5/1173.ann    \n",
      "  inflating: Collection5/1173.txt    \n",
      "  inflating: Collection5/1174.ann    \n",
      "  inflating: Collection5/1174.txt    \n",
      "  inflating: Collection5/1175.ann    \n",
      "  inflating: Collection5/1175.txt    \n",
      "  inflating: Collection5/1176.ann    \n",
      "  inflating: Collection5/1176.txt    \n",
      "  inflating: Collection5/1177.ann    \n",
      "  inflating: Collection5/1177.txt    \n",
      "  inflating: Collection5/1178.ann    \n",
      "  inflating: Collection5/1178.txt    \n",
      "  inflating: Collection5/1179.ann    \n",
      "  inflating: Collection5/1179.txt    \n",
      "  inflating: Collection5/118.ann     \n",
      "  inflating: Collection5/118.txt     \n",
      "  inflating: Collection5/1180.ann    \n",
      "  inflating: Collection5/1180.txt    \n",
      "  inflating: Collection5/1181.ann    \n",
      "  inflating: Collection5/1181.txt    \n",
      "  inflating: Collection5/1182.ann    \n",
      "  inflating: Collection5/1182.txt    \n",
      "  inflating: Collection5/1183.ann    \n",
      "  inflating: Collection5/1183.txt    \n",
      "  inflating: Collection5/1184.ann    \n",
      "  inflating: Collection5/1184.txt    \n",
      "  inflating: Collection5/1185.ann    \n",
      "  inflating: Collection5/1185.txt    \n",
      "  inflating: Collection5/1186.ann    \n",
      "  inflating: Collection5/1186.txt    \n",
      "  inflating: Collection5/1187.ann    \n",
      "  inflating: Collection5/1187.txt    \n",
      "  inflating: Collection5/1188.ann    \n",
      "  inflating: Collection5/1188.txt    \n",
      "  inflating: Collection5/1189.ann    \n",
      "  inflating: Collection5/1189.txt    \n",
      "  inflating: Collection5/119.ann     \n",
      "  inflating: Collection5/119.txt     \n",
      "  inflating: Collection5/1190.ann    \n",
      "  inflating: Collection5/1190.txt    \n",
      "  inflating: Collection5/1191.ann    \n",
      "  inflating: Collection5/1191.txt    \n",
      "  inflating: Collection5/1192.ann    \n",
      "  inflating: Collection5/1192.txt    \n",
      "  inflating: Collection5/1193.ann    \n",
      "  inflating: Collection5/1193.txt    \n",
      "  inflating: Collection5/1194.ann    \n",
      "  inflating: Collection5/1194.txt    \n",
      "  inflating: Collection5/1195.ann    \n",
      "  inflating: Collection5/1195.txt    \n",
      "  inflating: Collection5/1196.ann    \n",
      "  inflating: Collection5/1196.txt    \n",
      "  inflating: Collection5/1197.ann    \n",
      "  inflating: Collection5/1197.txt    \n",
      "  inflating: Collection5/1198.ann    \n",
      "  inflating: Collection5/1198.txt    \n",
      "  inflating: Collection5/1199.ann    \n",
      "  inflating: Collection5/1199.txt    \n",
      "  inflating: Collection5/11_01_13b.ann  \n",
      "  inflating: Collection5/11_01_13b.txt  \n",
      "  inflating: Collection5/11_01_13e.ann  \n",
      "  inflating: Collection5/11_01_13e.txt  \n",
      "  inflating: Collection5/120.ann     \n",
      "  inflating: Collection5/120.txt     \n",
      "  inflating: Collection5/1200.ann    \n",
      "  inflating: Collection5/1200.txt    \n",
      "  inflating: Collection5/121.ann     \n",
      "  inflating: Collection5/121.txt     \n",
      "  inflating: Collection5/122.ann     \n",
      "  inflating: Collection5/122.txt     \n",
      "  inflating: Collection5/123.ann     \n",
      "  inflating: Collection5/123.txt     \n",
      "  inflating: Collection5/124.ann     \n",
      "  inflating: Collection5/124.txt     \n",
      "  inflating: Collection5/125.ann     \n",
      "  inflating: Collection5/125.txt     \n",
      "  inflating: Collection5/126.ann     \n",
      "  inflating: Collection5/126.txt     \n",
      "  inflating: Collection5/127.ann     \n",
      "  inflating: Collection5/127.txt     \n",
      "  inflating: Collection5/128.ann     \n",
      "  inflating: Collection5/128.txt     \n",
      "  inflating: Collection5/129.ann     \n",
      "  inflating: Collection5/129.txt     \n",
      "  inflating: Collection5/130.ann     \n",
      "  inflating: Collection5/130.txt     \n",
      "  inflating: Collection5/131.ann     \n",
      "  inflating: Collection5/131.txt     \n",
      "  inflating: Collection5/132.ann     \n",
      "  inflating: Collection5/132.txt     \n",
      "  inflating: Collection5/133.ann     \n",
      "  inflating: Collection5/133.txt     \n",
      "  inflating: Collection5/134.ann     \n",
      "  inflating: Collection5/134.txt     \n",
      "  inflating: Collection5/135.ann     \n",
      "  inflating: Collection5/135.txt     \n",
      "  inflating: Collection5/136.ann     \n",
      "  inflating: Collection5/136.txt     \n",
      "  inflating: Collection5/137.ann     \n",
      "  inflating: Collection5/137.txt     \n",
      "  inflating: Collection5/138.ann     \n",
      "  inflating: Collection5/138.txt     \n",
      "  inflating: Collection5/139.ann     \n",
      "  inflating: Collection5/139.txt     \n",
      "  inflating: Collection5/140.ann     \n",
      "  inflating: Collection5/140.txt     \n",
      "  inflating: Collection5/141.ann     \n",
      "  inflating: Collection5/141.txt     \n",
      "  inflating: Collection5/142.ann     \n",
      "  inflating: Collection5/142.txt     \n",
      "  inflating: Collection5/143.ann     \n",
      "  inflating: Collection5/143.txt     \n",
      "  inflating: Collection5/144.ann     \n",
      "  inflating: Collection5/144.txt     \n",
      "  inflating: Collection5/145.ann     \n",
      "  inflating: Collection5/145.txt     \n",
      "  inflating: Collection5/146.ann     \n",
      "  inflating: Collection5/146.txt     \n",
      "  inflating: Collection5/147.ann     \n",
      "  inflating: Collection5/147.txt     \n",
      "  inflating: Collection5/148.ann     \n",
      "  inflating: Collection5/148.txt     \n",
      "  inflating: Collection5/149.ann     \n",
      "  inflating: Collection5/149.txt     \n",
      "  inflating: Collection5/14_01_13c.ann  \n",
      "  inflating: Collection5/14_01_13c.txt  \n",
      "  inflating: Collection5/14_01_13g.ann  \n",
      "  inflating: Collection5/14_01_13g.txt  \n",
      "  inflating: Collection5/14_01_13i.ann  \n",
      "  inflating: Collection5/14_01_13i.txt  \n",
      "  inflating: Collection5/150.ann     \n",
      "  inflating: Collection5/150.txt     \n",
      "  inflating: Collection5/151.ann     \n",
      "  inflating: Collection5/151.txt     \n",
      "  inflating: Collection5/152.ann     \n",
      "  inflating: Collection5/152.txt     \n",
      "  inflating: Collection5/153.ann     \n",
      "  inflating: Collection5/153.txt     \n",
      "  inflating: Collection5/154.ann     \n",
      "  inflating: Collection5/154.txt     \n",
      "  inflating: Collection5/155.ann     \n",
      "  inflating: Collection5/155.txt     \n",
      "  inflating: Collection5/156.ann     \n",
      "  inflating: Collection5/156.txt     \n",
      "  inflating: Collection5/157.ann     \n",
      "  inflating: Collection5/157.txt     \n",
      "  inflating: Collection5/158.ann     \n",
      "  inflating: Collection5/158.txt     \n",
      "  inflating: Collection5/159.ann     \n",
      "  inflating: Collection5/159.txt     \n",
      "  inflating: Collection5/15_01_13a.ann  \n",
      "  inflating: Collection5/15_01_13a.txt  \n",
      "  inflating: Collection5/15_01_13b.ann  \n",
      "  inflating: Collection5/15_01_13b.txt  \n",
      "  inflating: Collection5/15_01_13e.ann  \n",
      "  inflating: Collection5/15_01_13e.txt  \n",
      "  inflating: Collection5/15_01_13f.ann  \n",
      "  inflating: Collection5/15_01_13f.txt  \n",
      "  inflating: Collection5/160.ann     \n",
      "  inflating: Collection5/160.txt     \n",
      "  inflating: Collection5/161.ann     \n",
      "  inflating: Collection5/161.txt     \n",
      "  inflating: Collection5/162.ann     \n",
      "  inflating: Collection5/162.txt     \n",
      "  inflating: Collection5/163.ann     \n",
      "  inflating: Collection5/163.txt     \n",
      "  inflating: Collection5/164.ann     \n",
      "  inflating: Collection5/164.txt     \n",
      "  inflating: Collection5/165.ann     \n",
      "  inflating: Collection5/165.txt     \n",
      "  inflating: Collection5/166.ann     \n",
      "  inflating: Collection5/166.txt     \n",
      "  inflating: Collection5/167.ann     \n",
      "  inflating: Collection5/167.txt     \n",
      "  inflating: Collection5/168.ann     \n",
      "  inflating: Collection5/168.txt     \n",
      "  inflating: Collection5/169.ann     \n",
      "  inflating: Collection5/169.txt     \n",
      "  inflating: Collection5/170.ann     \n",
      "  inflating: Collection5/170.txt     \n",
      "  inflating: Collection5/171.ann     \n",
      "  inflating: Collection5/171.txt     \n",
      "  inflating: Collection5/172.ann     \n",
      "  inflating: Collection5/172.txt     \n",
      "  inflating: Collection5/173.ann     \n",
      "  inflating: Collection5/173.txt     \n",
      "  inflating: Collection5/174.ann     \n",
      "  inflating: Collection5/174.txt     \n",
      "  inflating: Collection5/175.ann     \n",
      "  inflating: Collection5/175.txt     \n",
      "  inflating: Collection5/176.ann     \n",
      "  inflating: Collection5/176.txt     \n",
      "  inflating: Collection5/177.ann     \n",
      "  inflating: Collection5/177.txt     \n",
      "  inflating: Collection5/178.ann     \n",
      "  inflating: Collection5/178.txt     \n",
      "  inflating: Collection5/179.ann     \n",
      "  inflating: Collection5/179.txt     \n",
      "  inflating: Collection5/180.ann     \n",
      "  inflating: Collection5/180.txt     \n",
      "  inflating: Collection5/181.ann     \n",
      "  inflating: Collection5/181.txt     \n",
      "  inflating: Collection5/182.ann     \n",
      "  inflating: Collection5/182.txt     \n",
      "  inflating: Collection5/183.ann     \n",
      "  inflating: Collection5/183.txt     \n",
      "  inflating: Collection5/184.ann     \n",
      "  inflating: Collection5/184.txt     \n",
      "  inflating: Collection5/185.ann     \n",
      "  inflating: Collection5/185.txt     \n",
      "  inflating: Collection5/186.ann     \n",
      "  inflating: Collection5/186.txt     \n",
      "  inflating: Collection5/187.ann     \n",
      "  inflating: Collection5/187.txt     \n",
      "  inflating: Collection5/188.ann     \n",
      "  inflating: Collection5/188.txt     \n",
      "  inflating: Collection5/189.ann     \n",
      "  inflating: Collection5/189.txt     \n",
      "  inflating: Collection5/190.ann     \n",
      "  inflating: Collection5/190.txt     \n",
      "  inflating: Collection5/191.ann     \n",
      "  inflating: Collection5/191.txt     \n",
      "  inflating: Collection5/192.ann     \n",
      "  inflating: Collection5/192.txt     \n",
      "  inflating: Collection5/193.ann     \n",
      "  inflating: Collection5/193.txt     \n",
      "  inflating: Collection5/194.ann     \n",
      "  inflating: Collection5/194.txt     \n",
      "  inflating: Collection5/195.ann     \n",
      "  inflating: Collection5/195.txt     \n",
      "  inflating: Collection5/196.ann     \n",
      "  inflating: Collection5/196.txt     \n",
      "  inflating: Collection5/197.ann     \n",
      "  inflating: Collection5/197.txt     \n",
      "  inflating: Collection5/198.ann     \n",
      "  inflating: Collection5/198.txt     \n",
      "  inflating: Collection5/199.ann     \n",
      "  inflating: Collection5/199.txt     \n",
      "  inflating: Collection5/19_11_12d.ann  \n",
      "  inflating: Collection5/19_11_12d.txt  \n",
      "  inflating: Collection5/19_11_12h.ann  \n",
      "  inflating: Collection5/19_11_12h.txt  \n",
      "  inflating: Collection5/200.ann     \n",
      "  inflating: Collection5/200.txt     \n",
      "  inflating: Collection5/2001.ann    \n",
      "  inflating: Collection5/2001.txt    \n",
      "  inflating: Collection5/2002.ann    \n",
      "  inflating: Collection5/2002.txt    \n",
      "  inflating: Collection5/2003.ann    \n",
      "  inflating: Collection5/2003.txt    \n",
      "  inflating: Collection5/2004.ann    \n",
      "  inflating: Collection5/2004.txt    \n",
      "  inflating: Collection5/2005.ann    \n",
      "  inflating: Collection5/2005.txt    \n",
      "  inflating: Collection5/2006.ann    \n",
      "  inflating: Collection5/2006.txt    \n",
      "  inflating: Collection5/2007.ann    \n",
      "  inflating: Collection5/2007.txt    \n",
      "  inflating: Collection5/2008.ann    \n",
      "  inflating: Collection5/2008.txt    \n",
      "  inflating: Collection5/2009.ann    \n",
      "  inflating: Collection5/2009.txt    \n",
      "  inflating: Collection5/201.ann     \n",
      "  inflating: Collection5/201.txt     \n",
      "  inflating: Collection5/2010.ann    \n",
      "  inflating: Collection5/2010.txt    \n",
      "  inflating: Collection5/2011.ann    \n",
      "  inflating: Collection5/2011.txt    \n",
      "  inflating: Collection5/2012.ann    \n",
      "  inflating: Collection5/2012.txt    \n",
      "  inflating: Collection5/2013.ann    \n",
      "  inflating: Collection5/2013.txt    \n",
      "  inflating: Collection5/2014.ann    \n",
      "  inflating: Collection5/2014.txt    \n",
      "  inflating: Collection5/2015.ann    \n",
      "  inflating: Collection5/2015.txt    \n",
      "  inflating: Collection5/2016.ann    \n",
      "  inflating: Collection5/2016.txt    \n",
      "  inflating: Collection5/2017.ann    \n",
      "  inflating: Collection5/2017.txt    \n",
      "  inflating: Collection5/2018.ann    \n",
      "  inflating: Collection5/2018.txt    \n",
      "  inflating: Collection5/2019.ann    \r\n",
      "  inflating: Collection5/2019.txt    \r\n",
      "  inflating: Collection5/202.ann     \r\n",
      "  inflating: Collection5/202.txt     \r\n",
      "  inflating: Collection5/2020.ann    \r\n",
      "  inflating: Collection5/2020.txt    \r\n",
      "  inflating: Collection5/2021.ann    \r\n",
      "  inflating: Collection5/2021.txt    \r\n",
      "  inflating: Collection5/2022.ann    \r\n",
      "  inflating: Collection5/2022.txt    \r\n",
      "  inflating: Collection5/2023.ann    \r\n",
      "  inflating: Collection5/2023.txt    \r\n",
      "  inflating: Collection5/2024.ann    \r\n",
      "  inflating: Collection5/2024.txt    \r\n",
      "  inflating: Collection5/2025.ann    \r\n",
      "  inflating: Collection5/2025.txt    \r\n",
      "  inflating: Collection5/2026.ann    \r\n",
      "  inflating: Collection5/2026.txt    \r\n",
      "  inflating: Collection5/2027.ann    \r\n",
      "  inflating: Collection5/2027.txt    \r\n",
      "  inflating: Collection5/2028.ann    \r\n",
      "  inflating: Collection5/2028.txt    \r\n",
      "  inflating: Collection5/2029.ann    \r\n",
      "  inflating: Collection5/2029.txt    \r\n",
      "  inflating: Collection5/203.ann     \r\n",
      "  inflating: Collection5/203.txt     \r\n",
      "  inflating: Collection5/2030.ann    \r\n",
      "  inflating: Collection5/2030.txt    \r\n",
      "  inflating: Collection5/2031.ann    \r\n",
      "  inflating: Collection5/2031.txt    \r\n",
      "  inflating: Collection5/2032.ann    \r\n",
      "  inflating: Collection5/2032.txt    \r\n",
      "  inflating: Collection5/2034.ann    \r\n",
      "  inflating: Collection5/2034.txt    \r\n",
      "  inflating: Collection5/2035.ann    \r\n",
      "  inflating: Collection5/2035.txt    \r\n",
      "  inflating: Collection5/2036.ann    \r\n",
      "  inflating: Collection5/2036.txt    \r\n",
      "  inflating: Collection5/2037.ann    \r\n",
      "  inflating: Collection5/2037.txt    \r\n",
      "  inflating: Collection5/2038.ann    \r\n",
      "  inflating: Collection5/2038.txt    \r\n",
      "  inflating: Collection5/2039.ann    \r\n",
      "  inflating: Collection5/2039.txt    \r\n",
      "  inflating: Collection5/204.ann     \r\n",
      "  inflating: Collection5/204.txt     \r\n",
      "  inflating: Collection5/2040.ann    \r\n",
      "  inflating: Collection5/2040.txt    \r\n",
      "  inflating: Collection5/2041.ann    \r\n",
      "  inflating: Collection5/2041.txt    \r\n",
      "  inflating: Collection5/2042.ann    \r\n",
      "  inflating: Collection5/2042.txt    \r\n",
      "  inflating: Collection5/2043.ann    \r\n",
      "  inflating: Collection5/2043.txt    \r\n",
      "  inflating: Collection5/2044.ann    \r\n",
      "  inflating: Collection5/2044.txt    \r\n",
      "  inflating: Collection5/2045.ann    \r\n",
      "  inflating: Collection5/2045.txt    \r\n",
      "  inflating: Collection5/2046.ann    \r\n",
      "  inflating: Collection5/2046.txt    \r\n",
      "  inflating: Collection5/2047.ann    \r\n",
      "  inflating: Collection5/2047.txt    \r\n",
      "  inflating: Collection5/2048.ann    \r\n",
      "  inflating: Collection5/2048.txt    \r\n",
      "  inflating: Collection5/2049.ann    \r\n",
      "  inflating: Collection5/2049.txt    \r\n",
      "  inflating: Collection5/205.ann     \r\n",
      "  inflating: Collection5/205.txt     \r\n",
      "  inflating: Collection5/2050.ann    \r\n",
      "  inflating: Collection5/2050.txt    \r\n",
      "  inflating: Collection5/206.ann     \r\n",
      "  inflating: Collection5/206.txt     \r\n",
      "  inflating: Collection5/207.ann     \r\n",
      "  inflating: Collection5/207.txt     \r\n",
      "  inflating: Collection5/208.ann     \r\n",
      "  inflating: Collection5/208.txt     \r\n",
      "  inflating: Collection5/209.ann     \r\n",
      "  inflating: Collection5/209.txt     \r\n",
      "  inflating: Collection5/20_11_12a.ann  \r\n",
      "  inflating: Collection5/20_11_12a.txt  \r\n",
      "  inflating: Collection5/20_11_12b.ann  \r\n",
      "  inflating: Collection5/20_11_12b.txt  \r\n",
      "  inflating: Collection5/20_11_12c.ann  \r\n",
      "  inflating: Collection5/20_11_12c.txt  \r\n",
      "  inflating: Collection5/20_11_12d.ann  \r\n",
      "  inflating: Collection5/20_11_12d.txt  \r\n",
      "  inflating: Collection5/20_11_12i.ann  \r\n",
      "  inflating: Collection5/20_11_12i.txt  \r\n",
      "  inflating: Collection5/210.ann     \r\n",
      "  inflating: Collection5/210.txt     \r\n",
      "  inflating: Collection5/211.ann     \r\n",
      "  inflating: Collection5/211.txt     \r\n",
      "  inflating: Collection5/212.ann     \r\n",
      "  inflating: Collection5/212.txt     \r\n",
      "  inflating: Collection5/213.ann     \r\n",
      "  inflating: Collection5/213.txt     \r\n",
      "  inflating: Collection5/214.ann     \r\n",
      "  inflating: Collection5/214.txt     \r\n",
      "  inflating: Collection5/215.ann     \r\n",
      "  inflating: Collection5/215.txt     \r\n",
      "  inflating: Collection5/216.ann     \r\n",
      "  inflating: Collection5/216.txt     \r\n",
      "  inflating: Collection5/217.ann     \r\n",
      "  inflating: Collection5/217.txt     \r\n",
      "  inflating: Collection5/218.ann     \r\n",
      "  inflating: Collection5/218.txt     \r\n",
      "  inflating: Collection5/219.ann     \r\n",
      "  inflating: Collection5/219.txt     \r\n",
      "  inflating: Collection5/21_11_12c.ann  \r\n",
      "  inflating: Collection5/21_11_12c.txt  \r\n",
      "  inflating: Collection5/21_11_12h.ann  \r\n",
      "  inflating: Collection5/21_11_12h.txt  \r\n",
      "  inflating: Collection5/21_11_12i.ann  \r\n",
      "  inflating: Collection5/21_11_12i.txt  \r\n",
      "  inflating: Collection5/21_11_12j.ann  \r\n",
      "  inflating: Collection5/21_11_12j.txt  \r\n",
      "  inflating: Collection5/220.ann     \r\n",
      "  inflating: Collection5/220.txt     \r\n",
      "  inflating: Collection5/221.ann     \r\n",
      "  inflating: Collection5/221.txt     \r\n",
      "  inflating: Collection5/222.ann     \r\n",
      "  inflating: Collection5/222.txt     \r\n",
      "  inflating: Collection5/223.ann     \r\n",
      "  inflating: Collection5/223.txt     \r\n",
      "  inflating: Collection5/224.ann     \r\n",
      "  inflating: Collection5/224.txt     \r\n",
      "  inflating: Collection5/225.ann     \r\n",
      "  inflating: Collection5/225.txt     \r\n",
      "  inflating: Collection5/226.ann     \r\n",
      "  inflating: Collection5/226.txt     \r\n",
      "  inflating: Collection5/227.ann     \r\n",
      "  inflating: Collection5/227.txt     \r\n",
      "  inflating: Collection5/228.ann     \r\n",
      "  inflating: Collection5/228.txt     \r\n",
      "  inflating: Collection5/229.ann     \r\n",
      "  inflating: Collection5/229.txt     \r\n",
      "  inflating: Collection5/22_11_12a.ann  \r\n",
      "  inflating: Collection5/22_11_12a.txt  \r\n",
      "  inflating: Collection5/22_11_12c.ann  \r\n",
      "  inflating: Collection5/22_11_12c.txt  \r\n",
      "  inflating: Collection5/22_11_12d.ann  \r\n",
      "  inflating: Collection5/22_11_12d.txt  \r\n",
      "  inflating: Collection5/22_11_12g.ann  \r\n",
      "  inflating: Collection5/22_11_12g.txt  \r\n",
      "  inflating: Collection5/22_11_12h.ann  \r\n",
      "  inflating: Collection5/22_11_12h.txt  \r\n",
      "  inflating: Collection5/22_11_12i.ann  \r\n",
      "  inflating: Collection5/22_11_12i.txt  \r\n",
      "  inflating: Collection5/22_11_12j.ann  \r\n",
      "  inflating: Collection5/22_11_12j.txt  \r\n",
      "  inflating: Collection5/230.ann     \r\n",
      "  inflating: Collection5/230.txt     \r\n",
      "  inflating: Collection5/231.ann     \r\n",
      "  inflating: Collection5/231.txt     \r\n",
      "  inflating: Collection5/232.ann     \r\n",
      "  inflating: Collection5/232.txt     \r\n",
      "  inflating: Collection5/233.ann     \r\n",
      "  inflating: Collection5/233.txt     \r\n",
      "  inflating: Collection5/234.ann     \r\n",
      "  inflating: Collection5/234.txt     \r\n",
      "  inflating: Collection5/235.ann     \r\n",
      "  inflating: Collection5/235.txt     \r\n",
      "  inflating: Collection5/236.ann     \r\n",
      "  inflating: Collection5/236.txt     \r\n",
      "  inflating: Collection5/237.ann     \r\n",
      "  inflating: Collection5/237.txt     \r\n",
      "  inflating: Collection5/238.ann     \r\n",
      "  inflating: Collection5/238.txt     \r\n",
      "  inflating: Collection5/239.ann     \r\n",
      "  inflating: Collection5/239.txt     \r\n",
      "  inflating: Collection5/23_11_12a.ann  \r\n",
      "  inflating: Collection5/23_11_12a.txt  \r\n",
      "  inflating: Collection5/23_11_12b.ann  \r\n",
      "  inflating: Collection5/23_11_12b.txt  \r\n",
      "  inflating: Collection5/23_11_12c.ann  \r\n",
      "  inflating: Collection5/23_11_12c.txt  \r\n",
      "  inflating: Collection5/23_11_12d.ann  \r\n",
      "  inflating: Collection5/23_11_12d.txt  \r\n",
      "  inflating: Collection5/23_11_12e.ann  \r\n",
      "  inflating: Collection5/23_11_12e.txt  \r\n",
      "  inflating: Collection5/23_11_12f.ann  \r\n",
      "  inflating: Collection5/23_11_12f.txt  \r\n",
      "  inflating: Collection5/240.ann     \r\n",
      "  inflating: Collection5/240.txt     \r\n",
      "  inflating: Collection5/241.ann     \r\n",
      "  inflating: Collection5/241.txt     \r\n",
      "  inflating: Collection5/242.ann     \r\n",
      "  inflating: Collection5/242.txt     \r\n",
      "  inflating: Collection5/243.ann     \r\n",
      "  inflating: Collection5/243.txt     \r\n",
      "  inflating: Collection5/244.ann     \r\n",
      "  inflating: Collection5/244.txt     \r\n",
      "  inflating: Collection5/245.ann     \r\n",
      "  inflating: Collection5/245.txt     \r\n",
      "  inflating: Collection5/246.ann     \r\n",
      "  inflating: Collection5/246.txt     \r\n",
      "  inflating: Collection5/247.ann     \r\n",
      "  inflating: Collection5/247.txt     \r\n",
      "  inflating: Collection5/248.ann     \r\n",
      "  inflating: Collection5/248.txt     \r\n",
      "  inflating: Collection5/249.ann     \r\n",
      "  inflating: Collection5/249.txt     \r\n",
      "  inflating: Collection5/250.ann     \r\n",
      "  inflating: Collection5/250.txt     \r\n",
      "  inflating: Collection5/251.ann     \r\n",
      "  inflating: Collection5/251.txt     \r\n",
      "  inflating: Collection5/252.ann     \r\n",
      "  inflating: Collection5/252.txt     \r\n",
      "  inflating: Collection5/253.ann     \r\n",
      "  inflating: Collection5/253.txt     \r\n",
      "  inflating: Collection5/254.ann     \r\n",
      "  inflating: Collection5/254.txt     \r\n",
      "  inflating: Collection5/255.ann     \r\n",
      "  inflating: Collection5/255.txt     \r\n",
      "  inflating: Collection5/256.ann     \r\n",
      "  inflating: Collection5/256.txt     \r\n",
      "  inflating: Collection5/257.ann     \r\n",
      "  inflating: Collection5/257.txt     \r\n",
      "  inflating: Collection5/258.ann     \r\n",
      "  inflating: Collection5/258.txt     \r\n",
      "  inflating: Collection5/259.ann     \r\n",
      "  inflating: Collection5/259.txt     \r\n",
      "  inflating: Collection5/25_12_12a.ann  \r\n",
      "  inflating: Collection5/25_12_12a.txt  \r\n",
      "  inflating: Collection5/25_12_12c.ann  \r\n",
      "  inflating: Collection5/25_12_12c.txt  \r\n",
      "  inflating: Collection5/25_12_12d.ann  \r\n",
      "  inflating: Collection5/25_12_12d.txt  \r\n",
      "  inflating: Collection5/25_12_12e.ann  \r\n",
      "  inflating: Collection5/25_12_12e.txt  \r\n",
      "  inflating: Collection5/260.ann     \r\n",
      "  inflating: Collection5/260.txt     \r\n",
      "  inflating: Collection5/261.ann     \r\n",
      "  inflating: Collection5/261.txt     \r\n",
      "  inflating: Collection5/262.ann     \r\n",
      "  inflating: Collection5/262.txt     \r\n",
      "  inflating: Collection5/263.ann     \r\n",
      "  inflating: Collection5/263.txt     \r\n",
      "  inflating: Collection5/264.ann     \r\n",
      "  inflating: Collection5/264.txt     \r\n",
      "  inflating: Collection5/265.ann     \r\n",
      "  inflating: Collection5/265.txt     \r\n",
      "  inflating: Collection5/266.ann     \r\n",
      "  inflating: Collection5/266.txt     \r\n",
      "  inflating: Collection5/267.ann     \r\n",
      "  inflating: Collection5/267.txt     \r\n",
      "  inflating: Collection5/268.ann     \r\n",
      "  inflating: Collection5/268.txt     \r\n",
      "  inflating: Collection5/269.ann     \r\n",
      "  inflating: Collection5/269.txt     \r\n",
      "  inflating: Collection5/26_11_12b.ann  \r\n",
      "  inflating: Collection5/26_11_12b.txt  \r\n",
      "  inflating: Collection5/26_11_12c.ann  \r\n",
      "  inflating: Collection5/26_11_12c.txt  \r\n",
      "  inflating: Collection5/26_11_12e.ann  \r\n",
      "  inflating: Collection5/26_11_12e.txt  \r\n",
      "  inflating: Collection5/26_11_12f.ann  \r\n",
      "  inflating: Collection5/26_11_12f.txt  \r\n",
      "  inflating: Collection5/270.ann     \r\n",
      "  inflating: Collection5/270.txt     \r\n",
      "  inflating: Collection5/271.ann     \r\n",
      "  inflating: Collection5/271.txt     \r\n",
      "  inflating: Collection5/272.ann     \r\n",
      "  inflating: Collection5/272.txt     \r\n",
      "  inflating: Collection5/273.ann     \r\n",
      "  inflating: Collection5/273.txt     \r\n",
      "  inflating: Collection5/274.ann     \r\n",
      "  inflating: Collection5/274.txt     \r\n",
      "  inflating: Collection5/275.ann     \r\n",
      "  inflating: Collection5/275.txt     \r\n",
      "  inflating: Collection5/276.ann     \r\n",
      "  inflating: Collection5/276.txt     \r\n",
      "  inflating: Collection5/277.ann     \r\n",
      "  inflating: Collection5/277.txt     \r\n",
      "  inflating: Collection5/278.ann     \r\n",
      "  inflating: Collection5/278.txt     \r\n",
      "  inflating: Collection5/279.ann     \r\n",
      "  inflating: Collection5/279.txt     \r\n",
      "  inflating: Collection5/27_11_12a.ann  \r\n",
      "  inflating: Collection5/27_11_12a.txt  \r\n",
      "  inflating: Collection5/27_11_12c.ann  \r\n",
      "  inflating: Collection5/27_11_12c.txt  \r\n",
      "  inflating: Collection5/27_11_12d.ann  \r\n",
      "  inflating: Collection5/27_11_12d.txt  \r\n",
      "  inflating: Collection5/27_11_12e.ann  \r\n",
      "  inflating: Collection5/27_11_12e.txt  \r\n",
      "  inflating: Collection5/27_11_12j.ann  \r\n",
      "  inflating: Collection5/27_11_12j.txt  \r\n",
      "  inflating: Collection5/280.ann     \r\n",
      "  inflating: Collection5/280.txt     \r\n",
      "  inflating: Collection5/281.ann     \r\n",
      "  inflating: Collection5/281.txt     \r\n",
      "  inflating: Collection5/282.ann     \r\n",
      "  inflating: Collection5/282.txt     \r\n",
      "  inflating: Collection5/283.ann     \r\n",
      "  inflating: Collection5/283.txt     \r\n",
      "  inflating: Collection5/284.ann     \r\n",
      "  inflating: Collection5/284.txt     \r\n",
      "  inflating: Collection5/285.ann     \r\n",
      "  inflating: Collection5/285.txt     \r\n",
      "  inflating: Collection5/286.ann     \r\n",
      "  inflating: Collection5/286.txt     \r\n",
      "  inflating: Collection5/287.ann     \r\n",
      "  inflating: Collection5/287.txt     \r\n",
      "  inflating: Collection5/288.ann     \r\n",
      "  inflating: Collection5/288.txt     \r\n",
      "  inflating: Collection5/289.ann     \r\n",
      "  inflating: Collection5/289.txt     \r\n",
      "  inflating: Collection5/28_11_12a.ann  \r\n",
      "  inflating: Collection5/28_11_12a.txt  \r\n",
      "  inflating: Collection5/28_11_12f.ann  \r\n",
      "  inflating: Collection5/28_11_12f.txt  \r\n",
      "  inflating: Collection5/28_11_12g.ann  \r\n",
      "  inflating: Collection5/28_11_12g.txt  \r\n",
      "  inflating: Collection5/28_11_12h.ann  \r\n",
      "  inflating: Collection5/28_11_12h.txt  \r\n",
      "  inflating: Collection5/28_11_12i.ann  \r\n",
      "  inflating: Collection5/28_11_12i.txt  \r\n",
      "  inflating: Collection5/28_11_12j.ann  \r\n",
      "  inflating: Collection5/28_11_12j.txt  \r\n",
      "  inflating: Collection5/290.ann     \r\n",
      "  inflating: Collection5/290.txt     \r\n",
      "  inflating: Collection5/291.ann     \r\n",
      "  inflating: Collection5/291.txt     \r\n",
      "  inflating: Collection5/292.ann     \r\n",
      "  inflating: Collection5/292.txt     \r\n",
      "  inflating: Collection5/293.ann     \r\n",
      "  inflating: Collection5/293.txt     \r\n",
      "  inflating: Collection5/294.ann     \r\n",
      "  inflating: Collection5/294.txt     \r\n",
      "  inflating: Collection5/295.ann     \r\n",
      "  inflating: Collection5/295.txt     \r\n",
      "  inflating: Collection5/296.ann     \r\n",
      "  inflating: Collection5/296.txt     \r\n",
      "  inflating: Collection5/297.ann     \r\n",
      "  inflating: Collection5/297.txt     \r\n",
      "  inflating: Collection5/298.ann     \r\n",
      "  inflating: Collection5/298.txt     \r\n",
      "  inflating: Collection5/299.ann     \r\n",
      "  inflating: Collection5/299.txt     \r\n",
      "  inflating: Collection5/29_11_12a.ann  \r\n",
      "  inflating: Collection5/29_11_12a.txt  \r\n",
      "  inflating: Collection5/29_11_12b.ann  \r\n",
      "  inflating: Collection5/29_11_12b.txt  \r\n",
      "  inflating: Collection5/300.ann     \r\n",
      "  inflating: Collection5/300.txt     \r\n",
      "  inflating: Collection5/301.ann     \r\n",
      "  inflating: Collection5/301.txt     \r\n",
      "  inflating: Collection5/302.ann     \r\n",
      "  inflating: Collection5/302.txt     \r\n",
      "  inflating: Collection5/303.ann     \r\n",
      "  inflating: Collection5/303.txt     \r\n",
      "  inflating: Collection5/304.ann     \r\n",
      "  inflating: Collection5/304.txt     \r\n",
      "  inflating: Collection5/305.ann     \r\n",
      "  inflating: Collection5/305.txt     \r\n",
      "  inflating: Collection5/306.ann     \r\n",
      "  inflating: Collection5/306.txt     \r\n",
      "  inflating: Collection5/307.ann     \r\n",
      "  inflating: Collection5/307.txt     \r\n",
      "  inflating: Collection5/308.ann     \r\n",
      "  inflating: Collection5/308.txt     \r\n",
      "  inflating: Collection5/309.ann     \r\n",
      "  inflating: Collection5/309.txt     \r\n",
      "  inflating: Collection5/30_11_12b.ann  \r\n",
      "  inflating: Collection5/30_11_12b.txt  \r\n",
      "  inflating: Collection5/30_11_12h.ann  \r\n",
      "  inflating: Collection5/30_11_12h.txt  \r\n",
      "  inflating: Collection5/30_11_12i.ann  \r\n",
      "  inflating: Collection5/30_11_12i.txt  \r\n",
      "  inflating: Collection5/310.ann     \r\n",
      "  inflating: Collection5/310.txt     \r\n",
      "  inflating: Collection5/311.ann     \r\n",
      "  inflating: Collection5/311.txt     \r\n",
      "  inflating: Collection5/312.ann     \r\n",
      "  inflating: Collection5/312.txt     \r\n",
      "  inflating: Collection5/313.ann     \r\n",
      "  inflating: Collection5/313.txt     \r\n",
      "  inflating: Collection5/314.ann     \r\n",
      "  inflating: Collection5/314.txt     \r\n",
      "  inflating: Collection5/315.ann     \r\n",
      "  inflating: Collection5/315.txt     \r\n",
      "  inflating: Collection5/316.ann     \r\n",
      "  inflating: Collection5/316.txt     \r\n",
      "  inflating: Collection5/317.ann     \r\n",
      "  inflating: Collection5/317.txt     \r\n",
      "  inflating: Collection5/318.ann     \r\n",
      "  inflating: Collection5/318.txt     \r\n",
      "  inflating: Collection5/319.ann     \r\n",
      "  inflating: Collection5/319.txt     \r\n",
      "  inflating: Collection5/320.ann     \r\n",
      "  inflating: Collection5/320.txt     \r\n",
      "  inflating: Collection5/321.ann     \r\n",
      "  inflating: Collection5/321.txt     \r\n",
      "  inflating: Collection5/322.ann     \r\n",
      "  inflating: Collection5/322.txt     \r\n",
      "  inflating: Collection5/323.ann     \r\n",
      "  inflating: Collection5/323.txt     \r\n",
      "  inflating: Collection5/324.ann     \r\n",
      "  inflating: Collection5/324.txt     \r\n",
      "  inflating: Collection5/325.ann     \r\n",
      "  inflating: Collection5/325.txt     \r\n",
      "  inflating: Collection5/326.ann     \r\n",
      "  inflating: Collection5/326.txt     \r\n",
      "  inflating: Collection5/327.ann     \r\n",
      "  inflating: Collection5/327.txt     \r\n",
      "  inflating: Collection5/328.ann     \r\n",
      "  inflating: Collection5/328.txt     \r\n",
      "  inflating: Collection5/329.ann     \r\n",
      "  inflating: Collection5/329.txt     \r\n",
      "  inflating: Collection5/330.ann     \r\n",
      "  inflating: Collection5/330.txt     \r\n",
      "  inflating: Collection5/331.ann     \r\n",
      "  inflating: Collection5/331.txt     \r\n",
      "  inflating: Collection5/332.ann     \r\n",
      "  inflating: Collection5/332.txt     \r\n",
      "  inflating: Collection5/333.ann     \r\n",
      "  inflating: Collection5/333.txt     \r\n",
      "  inflating: Collection5/334.ann     \r\n",
      "  inflating: Collection5/334.txt     \r\n",
      "  inflating: Collection5/335.ann     \r\n",
      "  inflating: Collection5/335.txt     \r\n",
      "  inflating: Collection5/336.ann     \r\n",
      "  inflating: Collection5/336.txt     \r\n",
      "  inflating: Collection5/337.ann     \r\n",
      "  inflating: Collection5/337.txt     \r\n",
      "  inflating: Collection5/338.ann     \r\n",
      "  inflating: Collection5/338.txt     \r\n",
      "  inflating: Collection5/339.ann     \r\n",
      "  inflating: Collection5/339.txt     \r\n",
      "  inflating: Collection5/340.ann     \r\n",
      "  inflating: Collection5/340.txt     \r\n",
      "  inflating: Collection5/341.ann     \r\n",
      "  inflating: Collection5/341.txt     \r\n",
      "  inflating: Collection5/342.ann     \r\n",
      "  inflating: Collection5/342.txt     \r\n",
      "  inflating: Collection5/343.ann     \r\n",
      "  inflating: Collection5/343.txt     \r\n",
      "  inflating: Collection5/344.ann     \r\n",
      "  inflating: Collection5/344.txt     \r\n",
      "  inflating: Collection5/345.ann     \r\n",
      "  inflating: Collection5/345.txt     \r\n",
      "  inflating: Collection5/346.ann     \r\n",
      "  inflating: Collection5/346.txt     \r\n",
      "  inflating: Collection5/347.ann     \r\n",
      "  inflating: Collection5/347.txt     \r\n",
      "  inflating: Collection5/348.ann     \r\n",
      "  inflating: Collection5/348.txt     \r\n",
      "  inflating: Collection5/349.ann     \r\n",
      "  inflating: Collection5/349.txt     \r\n",
      "  inflating: Collection5/350.ann     \r\n",
      "  inflating: Collection5/350.txt     \r\n",
      "  inflating: Collection5/351.ann     \r\n",
      "  inflating: Collection5/351.txt     \r\n",
      "  inflating: Collection5/352.ann     \r\n",
      "  inflating: Collection5/352.txt     \r\n",
      "  inflating: Collection5/353.ann     \r\n",
      "  inflating: Collection5/353.txt     \r\n",
      "  inflating: Collection5/354.ann     \r\n",
      "  inflating: Collection5/354.txt     \r\n",
      "  inflating: Collection5/355.ann     \r\n",
      "  inflating: Collection5/355.txt     \r\n",
      "  inflating: Collection5/356.ann     \r\n",
      "  inflating: Collection5/356.txt     \r\n",
      "  inflating: Collection5/357.ann     \r\n",
      "  inflating: Collection5/357.txt     \r\n",
      "  inflating: Collection5/358.ann     \r\n",
      "  inflating: Collection5/358.txt     \r\n",
      "  inflating: Collection5/359.ann     \r\n",
      "  inflating: Collection5/359.txt     \r\n",
      "  inflating: Collection5/360.ann     \r\n",
      "  inflating: Collection5/360.txt     \r\n",
      "  inflating: Collection5/361.ann     \r\n",
      "  inflating: Collection5/361.txt     \r\n",
      "  inflating: Collection5/362.ann     \r\n",
      "  inflating: Collection5/362.txt     \r\n",
      "  inflating: Collection5/363.ann     \r\n",
      "  inflating: Collection5/363.txt     \r\n",
      "  inflating: Collection5/364.ann     \r\n",
      "  inflating: Collection5/364.txt     \r\n",
      "  inflating: Collection5/365.ann     \r\n",
      "  inflating: Collection5/365.txt     \r\n",
      "  inflating: Collection5/366.ann     \r\n",
      "  inflating: Collection5/366.txt     \r\n",
      "  inflating: Collection5/367.ann     \r\n",
      "  inflating: Collection5/367.txt     \r\n",
      "  inflating: Collection5/368.ann     \r\n",
      "  inflating: Collection5/368.txt     \r\n",
      "  inflating: Collection5/369.ann     \r\n",
      "  inflating: Collection5/369.txt     \r\n",
      "  inflating: Collection5/370.ann     \r\n",
      "  inflating: Collection5/370.txt     \r\n",
      "  inflating: Collection5/371.ann     \r\n",
      "  inflating: Collection5/371.txt     \r\n",
      "  inflating: Collection5/372.ann     \r\n",
      "  inflating: Collection5/372.txt     \r\n",
      "  inflating: Collection5/373.ann     \r\n",
      "  inflating: Collection5/373.txt     \r\n",
      "  inflating: Collection5/374.ann     \r\n",
      "  inflating: Collection5/374.txt     \r\n",
      "  inflating: Collection5/375.ann     \r\n",
      "  inflating: Collection5/375.txt     \r\n",
      "  inflating: Collection5/376.ann     \r\n",
      "  inflating: Collection5/376.txt     \r\n",
      "  inflating: Collection5/377.ann     \r\n",
      "  inflating: Collection5/377.txt     \r\n",
      "  inflating: Collection5/378.ann     \r\n",
      "  inflating: Collection5/378.txt     \r\n",
      "  inflating: Collection5/379.ann     \r\n",
      "  inflating: Collection5/379.txt     \r\n",
      "  inflating: Collection5/380.ann     \r\n",
      "  inflating: Collection5/380.txt     \r\n",
      "  inflating: Collection5/381.ann     \r\n",
      "  inflating: Collection5/381.txt     \r\n",
      "  inflating: Collection5/382.ann     \r\n",
      "  inflating: Collection5/382.txt     \r\n",
      "  inflating: Collection5/383.ann     \r\n",
      "  inflating: Collection5/383.txt     \r\n",
      "  inflating: Collection5/384.ann     \r\n",
      "  inflating: Collection5/384.txt     \r\n",
      "  inflating: Collection5/385.ann     \r\n",
      "  inflating: Collection5/385.txt     \r\n",
      "  inflating: Collection5/386.ann     \r\n",
      "  inflating: Collection5/386.txt     \r\n",
      "  inflating: Collection5/387.ann     \r\n",
      "  inflating: Collection5/387.txt     \r\n",
      "  inflating: Collection5/388.ann     \r\n",
      "  inflating: Collection5/388.txt     \r\n",
      "  inflating: Collection5/389.ann     \r\n",
      "  inflating: Collection5/389.txt     \r\n",
      "  inflating: Collection5/390.ann     \r\n",
      "  inflating: Collection5/390.txt     \r\n",
      "  inflating: Collection5/391.ann     \r\n",
      "  inflating: Collection5/391.txt     \r\n",
      "  inflating: Collection5/392.ann     \r\n",
      "  inflating: Collection5/392.txt     \r\n",
      "  inflating: Collection5/393.ann     \r\n",
      "  inflating: Collection5/393.txt     \r\n",
      "  inflating: Collection5/394.ann     \r\n",
      "  inflating: Collection5/394.txt     \r\n",
      "  inflating: Collection5/395.ann     \r\n",
      "  inflating: Collection5/395.txt     \r\n",
      "  inflating: Collection5/396.ann     \r\n",
      "  inflating: Collection5/396.txt     \r\n",
      "  inflating: Collection5/397.ann     \r\n",
      "  inflating: Collection5/397.txt     \r\n",
      "  inflating: Collection5/398.ann     \r\n",
      "  inflating: Collection5/398.txt     \r\n",
      "  inflating: Collection5/399.ann     \r\n",
      "  inflating: Collection5/399.txt     \r\n",
      "  inflating: Collection5/400.ann     \r\n",
      "  inflating: Collection5/400.txt     \r\n",
      "  inflating: Collection5/401.ann     \r\n",
      "  inflating: Collection5/401.txt     \r\n",
      "  inflating: Collection5/402.ann     \r\n",
      "  inflating: Collection5/402.txt     \r\n",
      "  inflating: Collection5/403.ann     \r\n",
      "  inflating: Collection5/403.txt     \r\n",
      "  inflating: Collection5/404.ann     \r\n",
      "  inflating: Collection5/404.txt     \r\n",
      "  inflating: Collection5/405.ann     \r\n",
      "  inflating: Collection5/405.txt     \r\n",
      "  inflating: Collection5/406.ann     \r\n",
      "  inflating: Collection5/406.txt     \r\n",
      "  inflating: Collection5/407.ann     \r\n",
      "  inflating: Collection5/407.txt     \r\n",
      "  inflating: Collection5/408.ann     \r\n",
      "  inflating: Collection5/408.txt     \r\n",
      "  inflating: Collection5/409.ann     \r\n",
      "  inflating: Collection5/409.txt     \r\n",
      "  inflating: Collection5/410.ann     \r\n",
      "  inflating: Collection5/410.txt     \r\n",
      "  inflating: Collection5/411.ann     \r\n",
      "  inflating: Collection5/411.txt     \r\n",
      "  inflating: Collection5/412.ann     \r\n",
      "  inflating: Collection5/412.txt     \r\n",
      "  inflating: Collection5/413.ann     \r\n",
      "  inflating: Collection5/413.txt     \r\n",
      "  inflating: Collection5/414.ann     \r\n",
      "  inflating: Collection5/414.txt     \r\n",
      "  inflating: Collection5/415.ann     \r\n",
      "  inflating: Collection5/415.txt     \r\n",
      "  inflating: Collection5/416.ann     \r\n",
      "  inflating: Collection5/416.txt     \r\n",
      "  inflating: Collection5/417.ann     \r\n",
      "  inflating: Collection5/417.txt     \r\n",
      "  inflating: Collection5/418.ann     \r\n",
      "  inflating: Collection5/418.txt     \r\n",
      "  inflating: Collection5/419.ann     \r\n",
      "  inflating: Collection5/419.txt     \r\n",
      "  inflating: Collection5/420.ann     \r\n",
      "  inflating: Collection5/420.txt     \r\n",
      "  inflating: Collection5/421.ann     \r\n",
      "  inflating: Collection5/421.txt     \r\n",
      "  inflating: Collection5/422.ann     \r\n",
      "  inflating: Collection5/422.txt     \r\n",
      "  inflating: Collection5/423.ann     \r\n",
      "  inflating: Collection5/423.txt     \r\n",
      "  inflating: Collection5/424.ann     \r\n",
      "  inflating: Collection5/424.txt     \r\n",
      "  inflating: Collection5/425.ann     \r\n",
      "  inflating: Collection5/425.txt     \r\n",
      "  inflating: Collection5/426.ann     \r\n",
      "  inflating: Collection5/426.txt     \r\n",
      "  inflating: Collection5/427.ann     \r\n",
      "  inflating: Collection5/427.txt     \r\n",
      "  inflating: Collection5/428.ann     \r\n",
      "  inflating: Collection5/428.txt     \r\n",
      "  inflating: Collection5/429.ann     \r\n",
      "  inflating: Collection5/429.txt     \r\n",
      "  inflating: Collection5/430.ann     \r\n",
      "  inflating: Collection5/430.txt     \r\n",
      "  inflating: Collection5/431.ann     \r\n",
      "  inflating: Collection5/431.txt     \r\n",
      "  inflating: Collection5/432.ann     \r\n",
      "  inflating: Collection5/432.txt     \r\n",
      "  inflating: Collection5/433.ann     \r\n",
      "  inflating: Collection5/433.txt     \r\n",
      "  inflating: Collection5/434.ann     \r\n",
      "  inflating: Collection5/434.txt     \r\n",
      "  inflating: Collection5/435.ann     \r\n",
      "  inflating: Collection5/435.txt     \r\n",
      "  inflating: Collection5/436.ann     \r\n",
      "  inflating: Collection5/436.txt     \r\n",
      "  inflating: Collection5/437.ann     \r\n",
      "  inflating: Collection5/437.txt     \r\n",
      "  inflating: Collection5/438.ann     \r\n",
      "  inflating: Collection5/438.txt     \r\n",
      "  inflating: Collection5/439.ann     \r\n",
      "  inflating: Collection5/439.txt     \r\n",
      "  inflating: Collection5/440.ann     \r\n",
      "  inflating: Collection5/440.txt     \r\n",
      "  inflating: Collection5/441.ann     \r\n",
      "  inflating: Collection5/441.txt     \r\n",
      "  inflating: Collection5/442.ann     \r\n",
      "  inflating: Collection5/442.txt     \r\n",
      "  inflating: Collection5/443.ann     \r\n",
      "  inflating: Collection5/443.txt     \r\n",
      "  inflating: Collection5/444.ann     \r\n",
      "  inflating: Collection5/444.txt     \r\n",
      "  inflating: Collection5/445.ann     \r\n",
      "  inflating: Collection5/445.txt     \r\n",
      "  inflating: Collection5/446.ann     \r\n",
      "  inflating: Collection5/446.txt     \r\n",
      "  inflating: Collection5/447.ann     \r\n",
      "  inflating: Collection5/447.txt     \r\n",
      "  inflating: Collection5/448.ann     \r\n",
      "  inflating: Collection5/448.txt     \r\n",
      "  inflating: Collection5/449.ann     \r\n",
      "  inflating: Collection5/449.txt     \r\n",
      "  inflating: Collection5/450.ann     \r\n",
      "  inflating: Collection5/450.txt     \r\n",
      "  inflating: Collection5/451.ann     \r\n",
      "  inflating: Collection5/451.txt     \r\n",
      "  inflating: Collection5/452.ann     \r\n",
      "  inflating: Collection5/452.txt     \r\n",
      "  inflating: Collection5/453.ann     \r\n",
      "  inflating: Collection5/453.txt     \r\n",
      "  inflating: Collection5/454.ann     \r\n",
      "  inflating: Collection5/454.txt     \r\n",
      "  inflating: Collection5/455.ann     \r\n",
      "  inflating: Collection5/455.txt     \r\n",
      "  inflating: Collection5/457.ann     \r\n",
      "  inflating: Collection5/457.txt     \r\n",
      "  inflating: Collection5/458.ann     \r\n",
      "  inflating: Collection5/458.txt     \r\n",
      "  inflating: Collection5/459.ann     \r\n",
      "  inflating: Collection5/459.txt     \r\n",
      "  inflating: Collection5/460.ann     \r\n",
      "  inflating: Collection5/460.txt     \r\n",
      "  inflating: Collection5/461.ann     \r\n",
      "  inflating: Collection5/461.txt     \r\n",
      "  inflating: Collection5/462.ann     \r\n",
      "  inflating: Collection5/462.txt     \r\n",
      "  inflating: Collection5/463.ann     \r\n",
      "  inflating: Collection5/463.txt     \r\n",
      "  inflating: Collection5/464.ann     \r\n",
      "  inflating: Collection5/464.txt     \r\n",
      "  inflating: Collection5/465.ann     \r\n",
      "  inflating: Collection5/465.txt     \r\n",
      "  inflating: Collection5/466.ann     \r\n",
      "  inflating: Collection5/466.txt     \r\n",
      "  inflating: Collection5/467.ann     \r\n",
      "  inflating: Collection5/467.txt     \r\n",
      "  inflating: Collection5/468.ann     \r\n",
      "  inflating: Collection5/468.txt     \r\n",
      "  inflating: Collection5/469.ann     \r\n",
      "  inflating: Collection5/469.txt     \r\n",
      "  inflating: Collection5/470.ann     \r\n",
      "  inflating: Collection5/470.txt     \r\n",
      "  inflating: Collection5/471.ann     \r\n",
      "  inflating: Collection5/471.txt     \r\n",
      "  inflating: Collection5/472.ann     \r\n",
      "  inflating: Collection5/472.txt     \r\n",
      "  inflating: Collection5/473.ann     \r\n",
      "  inflating: Collection5/473.txt     \r\n",
      "  inflating: Collection5/474.ann     \r\n",
      "  inflating: Collection5/474.txt     \r\n",
      "  inflating: Collection5/475.ann     \r\n",
      "  inflating: Collection5/475.txt     \r\n",
      "  inflating: Collection5/476.ann     \r\n",
      "  inflating: Collection5/476.txt     \r\n",
      "  inflating: Collection5/477.ann     \r\n",
      "  inflating: Collection5/477.txt     \r\n",
      "  inflating: Collection5/478.ann     \r\n",
      "  inflating: Collection5/478.txt     \r\n",
      "  inflating: Collection5/479.ann     \r\n",
      "  inflating: Collection5/479.txt     \r\n",
      "  inflating: Collection5/480.ann     \r\n",
      "  inflating: Collection5/480.txt     \r\n",
      "  inflating: Collection5/481.ann     \r\n",
      "  inflating: Collection5/481.txt     \r\n",
      "  inflating: Collection5/482.ann     \r\n",
      "  inflating: Collection5/482.txt     \r\n",
      "  inflating: Collection5/483.ann     \r\n",
      "  inflating: Collection5/483.txt     \r\n",
      "  inflating: Collection5/484.ann     \r\n",
      "  inflating: Collection5/484.txt     \r\n",
      "  inflating: Collection5/485.ann     \r\n",
      "  inflating: Collection5/485.txt     \r\n",
      "  inflating: Collection5/486.ann     \r\n",
      "  inflating: Collection5/486.txt     \r\n",
      "  inflating: Collection5/487.ann     \r\n",
      "  inflating: Collection5/487.txt     \r\n",
      "  inflating: Collection5/488.ann     \r\n",
      "  inflating: Collection5/488.txt     \r\n",
      "  inflating: Collection5/489.ann     \r\n",
      "  inflating: Collection5/489.txt     \r\n",
      "  inflating: Collection5/490.ann     \r\n",
      "  inflating: Collection5/490.txt     \r\n",
      "  inflating: Collection5/491.ann     \r\n",
      "  inflating: Collection5/491.txt     \r\n",
      "  inflating: Collection5/492.ann     \r\n",
      "  inflating: Collection5/492.txt     \r\n",
      "  inflating: Collection5/493.ann     \r\n",
      "  inflating: Collection5/493.txt     \r\n",
      "  inflating: Collection5/494.ann     \r\n",
      "  inflating: Collection5/494.txt     \r\n",
      "  inflating: Collection5/495.ann     \r\n",
      "  inflating: Collection5/495.txt     \r\n",
      "  inflating: Collection5/496.ann     \r\n",
      "  inflating: Collection5/496.txt     \r\n",
      "  inflating: Collection5/497.ann     \r\n",
      "  inflating: Collection5/497.txt     \r\n",
      "  inflating: Collection5/498.ann     \r\n",
      "  inflating: Collection5/498.txt     \r\n",
      "  inflating: Collection5/499.ann     \r\n",
      "  inflating: Collection5/499.txt     \r\n",
      "  inflating: Collection5/500.ann     \r\n",
      "  inflating: Collection5/500.txt     \r\n",
      "  inflating: Collection5/501.ann     \r\n",
      "  inflating: Collection5/501.txt     \r\n",
      "  inflating: Collection5/502.ann     \r\n",
      "  inflating: Collection5/502.txt     \r\n",
      "  inflating: Collection5/503.ann     \r\n",
      "  inflating: Collection5/503.txt     \r\n",
      "  inflating: Collection5/504.ann     \r\n",
      "  inflating: Collection5/504.txt     \r\n",
      "  inflating: Collection5/505.ann     \r\n",
      "  inflating: Collection5/505.txt     \r\n",
      "  inflating: Collection5/506.ann     \r\n",
      "  inflating: Collection5/506.txt     \r\n",
      "  inflating: Collection5/507.ann     \r\n",
      "  inflating: Collection5/507.txt     \r\n",
      "  inflating: Collection5/508.ann     \r\n",
      "  inflating: Collection5/508.txt     \r\n",
      "  inflating: Collection5/509.ann     \r\n",
      "  inflating: Collection5/509.txt     \r\n",
      "  inflating: Collection5/510.ann     \r\n",
      "  inflating: Collection5/510.txt     \r\n",
      "  inflating: Collection5/511.ann     \r\n",
      "  inflating: Collection5/511.txt     \r\n",
      "  inflating: Collection5/512.ann     \r\n",
      "  inflating: Collection5/512.txt     \r\n",
      "  inflating: Collection5/513.ann     \r\n",
      "  inflating: Collection5/513.txt     \r\n",
      "  inflating: Collection5/514.ann     \r\n",
      "  inflating: Collection5/514.txt     \r\n",
      "  inflating: Collection5/515.ann     \r\n",
      "  inflating: Collection5/515.txt     \r\n",
      "  inflating: Collection5/516.ann     \r\n",
      "  inflating: Collection5/516.txt     \r\n",
      "  inflating: Collection5/517.ann     \r\n",
      "  inflating: Collection5/517.txt     \r\n",
      "  inflating: Collection5/518.ann     \r\n",
      "  inflating: Collection5/518.txt     \r\n",
      "  inflating: Collection5/519.ann     \r\n",
      "  inflating: Collection5/519.txt     \r\n",
      "  inflating: Collection5/520.ann     \r\n",
      "  inflating: Collection5/520.txt     \r\n",
      "  inflating: Collection5/521.ann     \r\n",
      "  inflating: Collection5/521.txt     \r\n",
      "  inflating: Collection5/522.ann     \r\n",
      "  inflating: Collection5/522.txt     \r\n",
      "  inflating: Collection5/523.ann     \r\n",
      "  inflating: Collection5/523.txt     \r\n",
      "  inflating: Collection5/524.ann     \r\n",
      "  inflating: Collection5/524.txt     \r\n",
      "  inflating: Collection5/525.ann     \r\n",
      "  inflating: Collection5/525.txt     \r\n",
      "  inflating: Collection5/526.ann     \r\n",
      "  inflating: Collection5/526.txt     \r\n",
      "  inflating: Collection5/527.ann     \r\n",
      "  inflating: Collection5/527.txt     \r\n",
      "  inflating: Collection5/528.ann     \r\n",
      "  inflating: Collection5/528.txt     \r\n",
      "  inflating: Collection5/529.ann     \r\n",
      "  inflating: Collection5/529.txt     \r\n",
      "  inflating: Collection5/530.ann     \r\n",
      "  inflating: Collection5/530.txt     \r\n",
      "  inflating: Collection5/531.ann     \r\n",
      "  inflating: Collection5/531.txt     \r\n",
      "  inflating: Collection5/532.ann     \r\n",
      "  inflating: Collection5/532.txt     \r\n",
      "  inflating: Collection5/533 (!).ann  \r\n",
      "  inflating: Collection5/533 (!).txt  \r\n",
      "  inflating: Collection5/534.ann     \r\n",
      "  inflating: Collection5/534.txt     \r\n",
      "  inflating: Collection5/535.ann     \r\n",
      "  inflating: Collection5/535.txt     \r\n",
      "  inflating: Collection5/536.ann     \r\n",
      "  inflating: Collection5/536.txt     \r\n",
      "  inflating: Collection5/537.ann     \r\n",
      "  inflating: Collection5/537.txt     \r\n",
      "  inflating: Collection5/538.ann     \r\n",
      "  inflating: Collection5/538.txt     \r\n",
      "  inflating: Collection5/539.ann     \r\n",
      "  inflating: Collection5/539.txt     \r\n",
      "  inflating: Collection5/540.ann     \r\n",
      "  inflating: Collection5/540.txt     \r\n",
      "  inflating: Collection5/541.ann     \r\n",
      "  inflating: Collection5/541.txt     \r\n",
      "  inflating: Collection5/542.ann     \r\n",
      "  inflating: Collection5/542.txt     \r\n",
      "  inflating: Collection5/543.ann     \r\n",
      "  inflating: Collection5/543.txt     \r\n",
      "  inflating: Collection5/544.ann     \r\n",
      "  inflating: Collection5/544.txt     \r\n",
      "  inflating: Collection5/545.ann     \r\n",
      "  inflating: Collection5/545.txt     \r\n",
      "  inflating: Collection5/546.ann     \r\n",
      "  inflating: Collection5/546.txt     \r\n",
      "  inflating: Collection5/547.ann     \r\n",
      "  inflating: Collection5/547.txt     \r\n",
      "  inflating: Collection5/548.ann     \r\n",
      "  inflating: Collection5/548.txt     \r\n",
      "  inflating: Collection5/549.ann     \r\n",
      "  inflating: Collection5/549.txt     \r\n",
      "  inflating: Collection5/550.ann     \r\n",
      "  inflating: Collection5/550.txt     \r\n",
      "  inflating: Collection5/551.ann     \r\n",
      "  inflating: Collection5/551.txt     \r\n",
      "  inflating: Collection5/552.ann     \r\n",
      "  inflating: Collection5/552.txt     \r\n",
      "  inflating: Collection5/553.ann     \r\n",
      "  inflating: Collection5/553.txt     \r\n",
      "  inflating: Collection5/554.ann     \r\n",
      "  inflating: Collection5/554.txt     \r\n",
      "  inflating: Collection5/555 (!).ann  \r\n",
      "  inflating: Collection5/555 (!).txt  \r\n",
      "  inflating: Collection5/556.ann     \r\n",
      "  inflating: Collection5/556.txt     \r\n",
      "  inflating: Collection5/557.ann     \r\n",
      "  inflating: Collection5/557.txt     \r\n",
      "  inflating: Collection5/558.ann     \r\n",
      "  inflating: Collection5/558.txt     \r\n",
      "  inflating: Collection5/559.ann     \r\n",
      "  inflating: Collection5/559.txt     \r\n",
      "  inflating: Collection5/560.ann     \r\n",
      "  inflating: Collection5/560.txt     \r\n",
      "  inflating: Collection5/561.ann     \r\n",
      "  inflating: Collection5/561.txt     \r\n",
      "  inflating: Collection5/562.ann     \r\n",
      "  inflating: Collection5/562.txt     \r\n",
      "  inflating: Collection5/563.ann     \r\n",
      "  inflating: Collection5/563.txt     \r\n",
      "  inflating: Collection5/564.ann     \r\n",
      "  inflating: Collection5/564.txt     \r\n",
      "  inflating: Collection5/565.ann     \r\n",
      "  inflating: Collection5/565.txt     \r\n",
      "  inflating: Collection5/567.ann     \r\n",
      "  inflating: Collection5/567.txt     \r\n",
      "  inflating: Collection5/568.ann     \r\n",
      "  inflating: Collection5/568.txt     \r\n",
      "  inflating: Collection5/569.ann     \r\n",
      "  inflating: Collection5/569.txt     \r\n",
      "  inflating: Collection5/570.ann     \r\n",
      "  inflating: Collection5/570.txt     \r\n",
      "  inflating: Collection5/571.ann     \r\n",
      "  inflating: Collection5/571.txt     \r\n",
      "  inflating: Collection5/572.ann     \r\n",
      "  inflating: Collection5/572.txt     \r\n",
      "  inflating: Collection5/574.ann     \r\n",
      "  inflating: Collection5/574.txt     \r\n",
      "  inflating: Collection5/575.ann     \r\n",
      "  inflating: Collection5/575.txt     \r\n",
      "  inflating: Collection5/576.ann     \r\n",
      "  inflating: Collection5/576.txt     \r\n",
      "  inflating: Collection5/577.ann     \r\n",
      "  inflating: Collection5/577.txt     \r\n",
      "  inflating: Collection5/578.ann     \r\n",
      "  inflating: Collection5/578.txt     \r\n",
      "  inflating: Collection5/579.ann     \r\n",
      "  inflating: Collection5/579.txt     \r\n",
      "  inflating: Collection5/581.ann     \r\n",
      "  inflating: Collection5/581.txt     \r\n",
      "  inflating: Collection5/582.ann     \r\n",
      "  inflating: Collection5/582.txt     \r\n",
      "  inflating: Collection5/583.ann     \r\n",
      "  inflating: Collection5/583.txt     \r\n",
      "  inflating: Collection5/584 (!).ann  \r\n",
      "  inflating: Collection5/584 (!).txt  \r\n",
      "  inflating: Collection5/585.ann     \r\n",
      "  inflating: Collection5/585.txt     \r\n",
      "  inflating: Collection5/586.ann     \r\n",
      "  inflating: Collection5/586.txt     \r\n",
      "  inflating: Collection5/587.ann     \r\n",
      "  inflating: Collection5/587.txt     \r\n",
      "  inflating: Collection5/588.ann     \r\n",
      "  inflating: Collection5/588.txt     \r\n",
      "  inflating: Collection5/589.ann     \r\n",
      "  inflating: Collection5/589.txt     \r\n",
      "  inflating: Collection5/590.ann     \r\n",
      "  inflating: Collection5/590.txt     \r\n",
      "  inflating: Collection5/591.ann     \r\n",
      "  inflating: Collection5/591.txt     \r\n",
      "  inflating: Collection5/592.ann     \r\n",
      "  inflating: Collection5/592.txt     \r\n",
      "  inflating: Collection5/593.ann     \r\n",
      "  inflating: Collection5/593.txt     \r\n",
      "  inflating: Collection5/594.ann     \r\n",
      "  inflating: Collection5/594.txt     \r\n",
      "  inflating: Collection5/595.ann     \r\n",
      "  inflating: Collection5/595.txt     \r\n",
      "  inflating: Collection5/596.ann     \r\n",
      "  inflating: Collection5/596.txt     \r\n",
      "  inflating: Collection5/597.ann     \r\n",
      "  inflating: Collection5/597.txt     \r\n",
      "  inflating: Collection5/598 (!).ann  \r\n",
      "  inflating: Collection5/598 (!).txt  \r\n",
      "  inflating: Collection5/599.ann     \r\n",
      "  inflating: Collection5/599.txt     \r\n",
      "  inflating: Collection5/600.ann     \r\n",
      "  inflating: Collection5/600.txt     \r\n",
      "  inflating: Collection5/601.ann     \r\n",
      "  inflating: Collection5/601.txt     \r\n",
      "  inflating: Collection5/602.ann     \r\n",
      "  inflating: Collection5/602.txt     \r\n",
      "  inflating: Collection5/610.ann     \r\n",
      "  inflating: Collection5/610.txt     \r\n",
      "  inflating: Collection5/611.ann     \r\n",
      "  inflating: Collection5/611.txt     \r\n",
      "  inflating: Collection5/612.ann     \r\n",
      "  inflating: Collection5/612.txt     \r\n",
      "  inflating: Collection5/613.ann     \r\n",
      "  inflating: Collection5/613.txt     \r\n",
      "  inflating: Collection5/614.ann     \r\n",
      "  inflating: Collection5/614.txt     \r\n",
      "  inflating: Collection5/615.ann     \r\n",
      "  inflating: Collection5/615.txt     \r\n",
      "  inflating: Collection5/616.ann     \r\n",
      "  inflating: Collection5/616.txt     \r\n",
      "  inflating: Collection5/617.ann     \r\n",
      "  inflating: Collection5/617.txt     \r\n",
      "  inflating: Collection5/618.ann     \r\n",
      "  inflating: Collection5/618.txt     \r\n",
      "  inflating: Collection5/619.ann     \r\n",
      "  inflating: Collection5/619.txt     \r\n",
      "  inflating: Collection5/620.ann     \r\n",
      "  inflating: Collection5/620.txt     \r\n",
      "  inflating: Collection5/621.ann     \r\n",
      "  inflating: Collection5/621.txt     \r\n",
      "  inflating: Collection5/622.ann     \r\n",
      "  inflating: Collection5/622.txt     \r\n",
      "  inflating: Collection5/623.ann     \r\n",
      "  inflating: Collection5/623.txt     \r\n",
      "  inflating: Collection5/624.ann     \r\n",
      "  inflating: Collection5/624.txt     \r\n",
      "  inflating: Collection5/625.ann     \r\n",
      "  inflating: Collection5/625.txt     \r\n",
      "  inflating: Collection5/626.ann     \r\n",
      "  inflating: Collection5/626.txt     \r\n",
      "  inflating: Collection5/627.ann     \r\n",
      "  inflating: Collection5/627.txt     \r\n",
      "  inflating: Collection5/628.ann     \r\n",
      "  inflating: Collection5/628.txt     \r\n",
      "  inflating: Collection5/629.ann     \r\n",
      "  inflating: Collection5/629.txt     \r\n",
      "  inflating: Collection5/630.ann     \r\n",
      "  inflating: Collection5/630.txt     \r\n",
      "  inflating: Collection5/631.ann     \r\n",
      "  inflating: Collection5/631.txt     \r\n",
      "  inflating: Collection5/632.ann     \r\n",
      "  inflating: Collection5/632.txt     \r\n",
      "  inflating: Collection5/633.ann     \r\n",
      "  inflating: Collection5/633.txt     \r\n",
      "  inflating: Collection5/abdulatipov.ann  \r\n",
      "  inflating: Collection5/abdulatipov.txt  \r\n",
      "  inflating: Collection5/artjakov.ann  \r\n",
      "  inflating: Collection5/artjakov.txt  \r\n",
      "  inflating: Collection5/Avtovaz.ann  \r\n",
      "  inflating: Collection5/Avtovaz.txt  \r\n",
      "  inflating: Collection5/blokhin.ann  \r\n",
      "  inflating: Collection5/blokhin.txt  \r\n",
      "  inflating: Collection5/chaves.ann  \r\n",
      "  inflating: Collection5/chaves.txt  \r\n",
      "  inflating: Collection5/chirkunov.ann  \r\n",
      "  inflating: Collection5/chirkunov.txt  \r\n",
      "  inflating: Collection5/kamchatka.ann  \r\n",
      "  inflating: Collection5/kamchatka.txt  \r\n",
      "  inflating: Collection5/klinton.ann  \r\n",
      "  inflating: Collection5/klinton.txt  \r\n",
      "  inflating: Collection5/kuleshov.ann  \r\n",
      "  inflating: Collection5/kuleshov.txt  \r\n",
      "  inflating: Collection5/last_01.ann  \r\n",
      "  inflating: Collection5/last_01.txt  \r\n",
      "  inflating: Collection5/last_02.ann  \r\n",
      "  inflating: Collection5/last_02.txt  \r\n",
      "  inflating: Collection5/last_03.ann  \r\n",
      "  inflating: Collection5/last_03.txt  \r\n",
      "  inflating: Collection5/last_04.ann  \r\n",
      "  inflating: Collection5/last_04.txt  \r\n",
      "  inflating: Collection5/last_05.ann  \r\n",
      "  inflating: Collection5/last_05.txt  \r\n",
      "  inflating: Collection5/last_06.ann  \r\n",
      "  inflating: Collection5/last_06.txt  \r\n",
      "  inflating: Collection5/last_07_new.ann  \r\n",
      "  inflating: Collection5/last_07_new.txt  \r\n",
      "  inflating: Collection5/last_08.ann  \r\n",
      "  inflating: Collection5/last_08.txt  \r\n",
      "  inflating: Collection5/last_09.ann  \r\n",
      "  inflating: Collection5/last_09.txt  \r\n",
      "  inflating: Collection5/last_10.ann  \r\n",
      "  inflating: Collection5/last_10.txt  \r\n",
      "  inflating: Collection5/last_11.ann  \r\n",
      "  inflating: Collection5/last_11.txt  \r\n",
      "  inflating: Collection5/last_12.ann  \r\n",
      "  inflating: Collection5/last_12.txt  \r\n",
      "  inflating: Collection5/last_13.ann  \r\n",
      "  inflating: Collection5/last_13.txt  \r\n",
      "  inflating: Collection5/last_14.ann  \r\n",
      "  inflating: Collection5/last_14.txt  \r\n",
      "  inflating: Collection5/last_15.ann  \r\n",
      "  inflating: Collection5/last_15.txt  \r\n",
      "  inflating: Collection5/last_16.ann  \r\n",
      "  inflating: Collection5/last_16.txt  \r\n",
      "  inflating: Collection5/last_17.ann  \r\n",
      "  inflating: Collection5/last_17.txt  \r\n",
      "  inflating: Collection5/last_18.ann  \r\n",
      "  inflating: Collection5/last_18.txt  \r\n",
      "  inflating: Collection5/last_19.ann  \r\n",
      "  inflating: Collection5/last_19.txt  \r\n",
      "  inflating: Collection5/last_20.ann  \r\n",
      "  inflating: Collection5/last_20.txt  \r\n",
      "  inflating: Collection5/last_21.ann  \r\n",
      "  inflating: Collection5/last_21.txt  \r\n",
      "  inflating: Collection5/last_22.ann  \r\n",
      "  inflating: Collection5/last_22.txt  \r\n",
      "  inflating: Collection5/last_23.ann  \r\n",
      "  inflating: Collection5/last_23.txt  \r\n",
      "  inflating: Collection5/last_24.ann  \r\n",
      "  inflating: Collection5/last_24.txt  \r\n",
      "  inflating: Collection5/last_25.ann  \r\n",
      "  inflating: Collection5/last_25.txt  \r\n",
      "  inflating: Collection5/last_26.ann  \r\n",
      "  inflating: Collection5/last_26.txt  \r\n",
      "  inflating: Collection5/last_27.ann  \r\n",
      "  inflating: Collection5/last_27.txt  \r\n",
      "  inflating: Collection5/last_28.ann  \r\n",
      "  inflating: Collection5/last_28.txt  \r\n",
      "  inflating: Collection5/last_29.ann  \r\n",
      "  inflating: Collection5/last_29.txt  \r\n",
      "  inflating: Collection5/last_30_new.ann  \r\n",
      "  inflating: Collection5/last_30_new.txt  \r\n",
      "  inflating: Collection5/last_31.ann  \r\n",
      "  inflating: Collection5/last_31.txt  \r\n",
      "  inflating: Collection5/last_32.ann  \r\n",
      "  inflating: Collection5/last_32.txt  \r\n",
      "  inflating: Collection5/last_33.ann  \r\n",
      "  inflating: Collection5/last_33.txt  \r\n",
      "  inflating: Collection5/last_34.ann  \r\n",
      "  inflating: Collection5/last_34.txt  \r\n",
      "  inflating: Collection5/last_35.ann  \r\n",
      "  inflating: Collection5/last_35.txt  \r\n",
      "  inflating: Collection5/last_36.ann  \r\n",
      "  inflating: Collection5/last_36.txt  \r\n",
      "  inflating: Collection5/last_37.ann  \r\n",
      "  inflating: Collection5/last_37.txt  \r\n",
      "  inflating: Collection5/last_38.ann  \r\n",
      "  inflating: Collection5/last_38.txt  \r\n",
      "  inflating: Collection5/last_39.ann  \r\n",
      "  inflating: Collection5/last_39.txt  \r\n",
      "  inflating: Collection5/last_40.ann  \r\n",
      "  inflating: Collection5/last_40.txt  \r\n",
      "  inflating: Collection5/last_41.ann  \r\n",
      "  inflating: Collection5/last_41.txt  \r\n",
      "  inflating: Collection5/last_42.ann  \r\n",
      "  inflating: Collection5/last_42.txt  \r\n",
      "  inflating: Collection5/last_43.ann  \r\n",
      "  inflating: Collection5/last_43.txt  \r\n",
      "  inflating: Collection5/last_44.ann  \r\n",
      "  inflating: Collection5/last_44.txt  \r\n",
      "  inflating: Collection5/last_45.ann  \r\n",
      "  inflating: Collection5/last_45.txt  \r\n",
      "  inflating: Collection5/last_46.ann  \r\n",
      "  inflating: Collection5/last_46.txt  \r\n",
      "  inflating: Collection5/last_47.ann  \r\n",
      "  inflating: Collection5/last_47.txt  \r\n",
      "  inflating: Collection5/last_48.ann  \r\n",
      "  inflating: Collection5/last_48.txt  \r\n",
      "  inflating: Collection5/last_49.ann  \r\n",
      "  inflating: Collection5/last_49.txt  \r\n",
      "  inflating: Collection5/last_50.ann  \r\n",
      "  inflating: Collection5/last_50.txt  \r\n",
      "  inflating: Collection5/last_51.ann  \r\n",
      "  inflating: Collection5/last_51.txt  \r\n",
      "  inflating: Collection5/last_52.ann  \r\n",
      "  inflating: Collection5/last_52.txt  \r\n",
      "  inflating: Collection5/last_53.ann  \r\n",
      "  inflating: Collection5/last_53.txt  \r\n",
      "  inflating: Collection5/last_54.ann  \r\n",
      "  inflating: Collection5/last_54.txt  \r\n",
      "  inflating: Collection5/last_55.ann  \r\n",
      "  inflating: Collection5/last_55.txt  \r\n",
      "  inflating: Collection5/last_56.ann  \r\n",
      "  inflating: Collection5/last_56.txt  \r\n",
      "  inflating: Collection5/last_57.ann  \r\n",
      "  inflating: Collection5/last_57.txt  \r\n",
      "  inflating: Collection5/last_58.ann  \r\n",
      "  inflating: Collection5/last_58.txt  \r\n",
      "  inflating: Collection5/last_59.ann  \r\n",
      "  inflating: Collection5/last_59.txt  \r\n",
      "  inflating: Collection5/last_60.ann  \r\n",
      "  inflating: Collection5/last_60.txt  \r\n",
      "  inflating: Collection5/last_61.ann  \r\n",
      "  inflating: Collection5/last_61.txt  \r\n",
      "  inflating: Collection5/last_62.ann  \r\n",
      "  inflating: Collection5/last_62.txt  \r\n",
      "  inflating: Collection5/last_63.ann  \r\n",
      "  inflating: Collection5/last_63.txt  \r\n",
      "  inflating: Collection5/last_64.ann  \r\n",
      "  inflating: Collection5/last_64.txt  \r\n",
      "  inflating: Collection5/last_65.ann  \r\n",
      "  inflating: Collection5/last_65.txt  \r\n",
      "  inflating: Collection5/last_66.ann  \r\n",
      "  inflating: Collection5/last_66.txt  \r\n",
      "  inflating: Collection5/last_67.ann  \r\n",
      "  inflating: Collection5/last_67.txt  \r\n",
      "  inflating: Collection5/last_68.ann  \r\n",
      "  inflating: Collection5/last_68.txt  \r\n",
      "  inflating: Collection5/last_69.ann  \r\n",
      "  inflating: Collection5/last_69.txt  \r\n",
      "  inflating: Collection5/last_70.ann  \r\n",
      "  inflating: Collection5/last_70.txt  \r\n",
      "  inflating: Collection5/last_71.ann  \r\n",
      "  inflating: Collection5/last_71.txt  \r\n",
      "  inflating: Collection5/last_72.ann  \r\n",
      "  inflating: Collection5/last_72.txt  \r\n",
      "  inflating: Collection5/last_73.ann  \r\n",
      "  inflating: Collection5/last_73.txt  \r\n",
      "  inflating: Collection5/last_74.ann  \r\n",
      "  inflating: Collection5/last_74.txt  \r\n",
      "  inflating: Collection5/last_75.ann  \r\n",
      "  inflating: Collection5/last_75.txt  \r\n",
      "  inflating: Collection5/lenoblast.ann  \r\n",
      "  inflating: Collection5/lenoblast.txt  \r\n",
      "  inflating: Collection5/maykl dzhekson.ann  \r\n",
      "  inflating: Collection5/maykl dzhekson.txt  \r\n",
      "  inflating: Collection5/mvd.ann     \r\n",
      "  inflating: Collection5/mvd.txt     \r\n",
      "  inflating: Collection5/mvd2.ann    \r\n",
      "  inflating: Collection5/mvd2.txt    \r\n",
      "  inflating: Collection5/rosobrnadzor.ann  \r\n",
      "  inflating: Collection5/rosobrnadzor.txt  \r\n",
      "  inflating: Collection5/ryadovoy chelah.ann  \r\n",
      "  inflating: Collection5/ryadovoy chelah.txt  \r\n",
      "  inflating: Collection5/semenenko.ann  \r\n",
      "  inflating: Collection5/semenenko.txt  \r\n",
      "  inflating: Collection5/shojgu1.ann  \r\n",
      "  inflating: Collection5/shojgu1.txt  \r\n",
      "  inflating: Collection5/shojgu3.ann  \r\n",
      "  inflating: Collection5/shojgu3.txt  \r\n",
      "  inflating: Collection5/shojgu4.ann  \r\n",
      "  inflating: Collection5/shojgu4.txt  \r\n",
      "  inflating: Collection5/shojgu6.ann  \r\n",
      "  inflating: Collection5/shojgu6.txt  \r\n",
      "  inflating: Collection5/si_tzjanpin.ann  \r\n",
      "  inflating: Collection5/si_tzjanpin.txt  \r\n",
      "  inflating: Collection5/sobjanin2.ann  \r\n",
      "  inflating: Collection5/sobjanin2.txt  \r\n",
      "  inflating: Collection5/turkmenija.ann  \r\n",
      "  inflating: Collection5/turkmenija.txt  \r\n",
      "  inflating: Collection5/uchitel.ann  \r\n",
      "  inflating: Collection5/uchitel.txt  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b7b10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_ne5('Collection5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ed34d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Жириновский предлагает обменять с США Сноудена на Бута Лидер ЛДПР Владимир Жириновский предложил обменять бывшего сотрудника ЦРУ США Эдварда Сноудена, который прибыл в Москву, на осужденного в Америке бизнесмена Виктора Бута. \"Сноудена ни в коем случае не высылать в США, а обменять на Виктора Бута и Константина Ярошенко. В идеале — добавить генерала Олега Калугина\", — написал он в своем микроблоге в Twitter. Сноуден, работавший на компанию Booz Allen Hamilton — подрядчика Центрального разведывательного управления США, в начале июня распространил секретный ордер суда, по которому спецслужбы получили доступ ко всем звонкам крупнейшего сотового оператора Verizon, а также данные о сверхсекретной программе агентства национальной безопасности PRISM, позволяющей отслеживать электронные коммуникации на крупнейших сайтах. В воскресенье стало известно, что Сноуден прибыл из Гонконга в Москву и запросил убежища в Эквадоре. Что ждет Эдварда Сноудена Эдвард Сноуден, наверное, не знал только одного: что отныне от него ничего уже не будет зависеть. Москва-Гавана-Каракас – в новой траектории жизни. В плотном кольце новых друзей, которым нужно быстро вытащить из тебя то, что еще не сказал. А может, говорить больше и нечего. Когда и они в этом убедятся, как раз объявят посадку в Каракасе. Добро пожаловать в третий мир. Потому что если тебе нет место в первом, не станет с тобой надолго связываться и второй. Подробнее >>  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = next(records)\n",
    "text = document.text\n",
    "# text\n",
    "text = re.sub('\\r\\n\\r\\n',' ',text)\n",
    "text = re.sub('\\r\\n',' ',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b54137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d56f538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Жириновский', 'JJ'),\n",
       " ('предлагает', 'NNP'),\n",
       " ('обменять', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('Сноудена', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Бута', 'NNP'),\n",
       " ('Лидер', 'NNP'),\n",
       " ('ЛДПР', 'NNP'),\n",
       " ('Владимир', 'NNP'),\n",
       " ('Жириновский', 'NNP'),\n",
       " ('предложил', 'NNP'),\n",
       " ('обменять', 'NNP'),\n",
       " ('бывшего', 'NNP'),\n",
       " ('сотрудника', 'NNP'),\n",
       " ('ЦРУ', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('Эдварда', 'NNP'),\n",
       " ('Сноудена', 'NNP'),\n",
       " (',', ','),\n",
       " ('который', 'NNP'),\n",
       " ('прибыл', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Москву', 'NNP'),\n",
       " (',', ','),\n",
       " ('на', 'NNP'),\n",
       " ('осужденного', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Америке', 'NNP'),\n",
       " ('бизнесмена', 'NNP'),\n",
       " ('Виктора', 'NNP'),\n",
       " ('Бута', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('Сноудена', 'JJ'),\n",
       " ('ни', 'NN'),\n",
       " ('в', 'NNP'),\n",
       " ('коем', 'NNP'),\n",
       " ('случае', 'NNP'),\n",
       " ('не', 'NNP'),\n",
       " ('высылать', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " (',', ','),\n",
       " ('а', 'NNP'),\n",
       " ('обменять', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Виктора', 'NNP'),\n",
       " ('Бута', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('Константина', 'NNP'),\n",
       " ('Ярошенко', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('В', 'VB'),\n",
       " ('идеале', 'JJ'),\n",
       " ('—', 'NNP'),\n",
       " ('добавить', 'NNP'),\n",
       " ('генерала', 'NNP'),\n",
       " ('Олега', 'NNP'),\n",
       " ('Калугина', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('—', 'NNP'),\n",
       " ('написал', 'NNP'),\n",
       " ('он', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('своем', 'NNP'),\n",
       " ('микроблоге', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Twitter', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Сноуден', 'NN'),\n",
       " (',', ','),\n",
       " ('работавший', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('компанию', 'NNP'),\n",
       " ('Booz', 'NNP'),\n",
       " ('Allen', 'NNP'),\n",
       " ('Hamilton', 'NNP'),\n",
       " ('—', 'NNP'),\n",
       " ('подрядчика', 'NNP'),\n",
       " ('Центрального', 'NNP'),\n",
       " ('разведывательного', 'NNP'),\n",
       " ('управления', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " (',', ','),\n",
       " ('в', 'NNP'),\n",
       " ('начале', 'NNP'),\n",
       " ('июня', 'NNP'),\n",
       " ('распространил', 'NNP'),\n",
       " ('секретный', 'NNP'),\n",
       " ('ордер', 'NNP'),\n",
       " ('суда', 'NNP'),\n",
       " (',', ','),\n",
       " ('по', 'NNP'),\n",
       " ('которому', 'NNP'),\n",
       " ('спецслужбы', 'NNP'),\n",
       " ('получили', 'NNP'),\n",
       " ('доступ', 'NNP'),\n",
       " ('ко', 'NNP'),\n",
       " ('всем', 'NNP'),\n",
       " ('звонкам', 'NNP'),\n",
       " ('крупнейшего', 'NNP'),\n",
       " ('сотового', 'NNP'),\n",
       " ('оператора', 'NNP'),\n",
       " ('Verizon', 'NNP'),\n",
       " (',', ','),\n",
       " ('а', 'NNP'),\n",
       " ('также', 'NNP'),\n",
       " ('данные', 'NNP'),\n",
       " ('о', 'NNP'),\n",
       " ('сверхсекретной', 'NNP'),\n",
       " ('программе', 'NNP'),\n",
       " ('агентства', 'NNP'),\n",
       " ('национальной', 'NNP'),\n",
       " ('безопасности', 'NNP'),\n",
       " ('PRISM', 'NNP'),\n",
       " (',', ','),\n",
       " ('позволяющей', 'NNP'),\n",
       " ('отслеживать', 'NNP'),\n",
       " ('электронные', 'NNP'),\n",
       " ('коммуникации', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('крупнейших', 'NNP'),\n",
       " ('сайтах', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('В', 'VB'),\n",
       " ('воскресенье', 'JJ'),\n",
       " ('стало', 'NNP'),\n",
       " ('известно', 'NNP'),\n",
       " (',', ','),\n",
       " ('что', 'NNP'),\n",
       " ('Сноуден', 'NNP'),\n",
       " ('прибыл', 'NNP'),\n",
       " ('из', 'NNP'),\n",
       " ('Гонконга', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Москву', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('запросил', 'NNP'),\n",
       " ('убежища', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Эквадоре', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Что', 'VB'),\n",
       " ('ждет', 'JJ'),\n",
       " ('Эдварда', 'NNP'),\n",
       " ('Сноудена', 'NNP'),\n",
       " ('Эдвард', 'NNP'),\n",
       " ('Сноуден', 'NNP'),\n",
       " (',', ','),\n",
       " ('наверное', 'NNP'),\n",
       " (',', ','),\n",
       " ('не', 'NNP'),\n",
       " ('знал', 'NNP'),\n",
       " ('только', 'NNP'),\n",
       " ('одного', 'NN'),\n",
       " (':', ':'),\n",
       " ('что', 'JJ'),\n",
       " ('отныне', 'NNP'),\n",
       " ('от', 'NNP'),\n",
       " ('него', 'NNP'),\n",
       " ('ничего', 'NNP'),\n",
       " ('уже', 'NNP'),\n",
       " ('не', 'NNP'),\n",
       " ('будет', 'NNP'),\n",
       " ('зависеть', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Москва-Гавана-Каракас', 'JJ'),\n",
       " ('–', 'JJ'),\n",
       " ('в', 'NN'),\n",
       " ('новой', 'NNP'),\n",
       " ('траектории', 'NNP'),\n",
       " ('жизни', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('В', 'VB'),\n",
       " ('плотном', 'JJ'),\n",
       " ('кольце', 'NNP'),\n",
       " ('новых', 'NNP'),\n",
       " ('друзей', 'NNP'),\n",
       " (',', ','),\n",
       " ('которым', 'NNP'),\n",
       " ('нужно', 'NNP'),\n",
       " ('быстро', 'NNP'),\n",
       " ('вытащить', 'NNP'),\n",
       " ('из', 'NNP'),\n",
       " ('тебя', 'NNP'),\n",
       " ('то', 'NNP'),\n",
       " (',', ','),\n",
       " ('что', 'NNP'),\n",
       " ('еще', 'NNP'),\n",
       " ('не', 'NNP'),\n",
       " ('сказал', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('А', 'NN'),\n",
       " ('может', 'NN'),\n",
       " (',', ','),\n",
       " ('говорить', 'NNP'),\n",
       " ('больше', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('нечего', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Когда', 'VB'),\n",
       " ('и', 'JJ'),\n",
       " ('они', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('этом', 'NNP'),\n",
       " ('убедятся', 'NNP'),\n",
       " (',', ','),\n",
       " ('как', 'NNP'),\n",
       " ('раз', 'NNP'),\n",
       " ('объявят', 'NNP'),\n",
       " ('посадку', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Каракасе', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Добро', 'VB'),\n",
       " ('пожаловать', 'JJ'),\n",
       " ('в', 'NNP'),\n",
       " ('третий', 'NNP'),\n",
       " ('мир', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Потому', 'VB'),\n",
       " ('что', 'JJ'),\n",
       " ('если', 'NNP'),\n",
       " ('тебе', 'NNP'),\n",
       " ('нет', 'NNP'),\n",
       " ('место', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('первом', 'NNP'),\n",
       " (',', ','),\n",
       " ('не', 'NNP'),\n",
       " ('станет', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('тобой', 'NNP'),\n",
       " ('надолго', 'NNP'),\n",
       " ('связываться', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('второй', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Подробнее', 'VB'),\n",
       " ('>', 'JJ'),\n",
       " ('>', 'NN')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(text)) # Как предварительно очистить все статьи в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8f85b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Hamilton', 'PERSON'),\n",
       " ('Америке', 'PERSON'),\n",
       " ('Виктора Бута', 'PERSON'),\n",
       " ('Москву', 'PERSON'),\n",
       " ('Сноуден', 'PERSON'),\n",
       " ('Эдварда Сноудена', 'PERSON'),\n",
       " ('Эдварда Сноудена Эдвард Сноуден', 'PERSON')}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60eb27",
   "metadata": {},
   "source": [
    "**В целом - работает. Но ошибки встречаются ('Америке', 'PERSON').**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ae94b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ne5Span(\n",
       "     index='T1',\n",
       "     type='PER',\n",
       "     start=0,\n",
       "     stop=11,\n",
       "     text='Жириновский'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T2',\n",
       "     type='GEOPOLIT',\n",
       "     start=34,\n",
       "     stop=37,\n",
       "     text='США'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T3',\n",
       "     type='PER',\n",
       "     start=38,\n",
       "     stop=46,\n",
       "     text='Сноудена'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T4',\n",
       "     type='PER',\n",
       "     start=50,\n",
       "     stop=54,\n",
       "     text='Бута'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T5',\n",
       "     type='ORG',\n",
       "     start=64,\n",
       "     stop=68,\n",
       "     text='ЛДПР'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T6',\n",
       "     type='PER',\n",
       "     start=69,\n",
       "     stop=89,\n",
       "     text='Владимир Жириновский'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T7',\n",
       "     type='ORG',\n",
       "     start=128,\n",
       "     stop=131,\n",
       "     text='ЦРУ'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T8',\n",
       "     type='GEOPOLIT',\n",
       "     start=132,\n",
       "     stop=135,\n",
       "     text='США'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T9',\n",
       "     type='PER',\n",
       "     start=136,\n",
       "     stop=152,\n",
       "     text='Эдварда Сноудена'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T10',\n",
       "     type='LOC',\n",
       "     start=171,\n",
       "     stop=177,\n",
       "     text='Москву'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T11',\n",
       "     type='GEOPOLIT',\n",
       "     start=196,\n",
       "     stop=203,\n",
       "     text='Америке'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T12',\n",
       "     type='PER',\n",
       "     start=215,\n",
       "     stop=227,\n",
       "     text='Виктора Бута'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T13',\n",
       "     type='PER',\n",
       "     start=233,\n",
       "     stop=241,\n",
       "     text='Сноудена'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T14',\n",
       "     type='GEOPOLIT',\n",
       "     start=273,\n",
       "     stop=276,\n",
       "     text='США'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T15',\n",
       "     type='PER',\n",
       "     start=292,\n",
       "     stop=304,\n",
       "     text='Виктора Бута'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T16',\n",
       "     type='PER',\n",
       "     start=307,\n",
       "     stop=327,\n",
       "     text='Константина Ярошенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T17',\n",
       "     type='PER',\n",
       "     start=358,\n",
       "     stop=372,\n",
       "     text='Олега Калугина'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T18',\n",
       "     type='MEDIA',\n",
       "     start=409,\n",
       "     stop=416,\n",
       "     text='Twitter'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T19',\n",
       "     type='PER',\n",
       "     start=421,\n",
       "     stop=428,\n",
       "     text='Сноуден'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T20',\n",
       "     type='ORG',\n",
       "     start=453,\n",
       "     stop=472,\n",
       "     text='Booz Allen Hamilton'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T21',\n",
       "     type='ORG',\n",
       "     start=486,\n",
       "     stop=527,\n",
       "     text='Центрального разведывательного управления'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T22',\n",
       "     type='GEOPOLIT',\n",
       "     start=528,\n",
       "     stop=531,\n",
       "     text='США'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T23',\n",
       "     type='ORG',\n",
       "     start=669,\n",
       "     stop=676,\n",
       "     text='Verizon'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T24',\n",
       "     type='ORG',\n",
       "     start=756,\n",
       "     stop=761,\n",
       "     text='PRISM'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T25',\n",
       "     type='PER',\n",
       "     start=868,\n",
       "     stop=875,\n",
       "     text='Сноуден'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T26',\n",
       "     type='GEOPOLIT',\n",
       "     start=886,\n",
       "     stop=894,\n",
       "     text='Гонконга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T27',\n",
       "     type='LOC',\n",
       "     start=897,\n",
       "     stop=903,\n",
       "     text='Москву'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T28',\n",
       "     type='GEOPOLIT',\n",
       "     start=925,\n",
       "     stop=933,\n",
       "     text='Эквадоре'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T29',\n",
       "     type='PER',\n",
       "     start=947,\n",
       "     stop=963,\n",
       "     text='Эдварда Сноудена'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T30',\n",
       "     type='PER',\n",
       "     start=967,\n",
       "     stop=981,\n",
       "     text='Эдвард Сноуден'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T31',\n",
       "     type='LOC',\n",
       "     start=1065,\n",
       "     stop=1086,\n",
       "     text='Москва-Гавана-Каракас'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T32',\n",
       "     type='LOC',\n",
       "     start=1297,\n",
       "     stop=1305,\n",
       "     text='Каракасе'\n",
       " )]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea661f3",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0c57a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 16:26:02.950553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Collecting ru-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.4.0/ru_core_news_sm-3.4.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ru-core-news-sm==3.4.0) (3.4.2)\n",
      "Requirement already satisfied: pymorphy2>=0.9 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ru-core-news-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (0.7.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (63.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.23.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: ru-core-news-sm\n",
      "Successfully installed ru-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c40ac25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import ru_core_news_sm\n",
    "from spacy.lang.ru.examples import sentences \n",
    "from spacy.lang.ru import Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f1dd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "684e49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_bb = text\n",
    "article = nlp(ny_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d78c8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Жириновский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " предлагает обменять с \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " на \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Бута Лидер\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ЛДПР\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Владимир Жириновский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " предложил обменять бывшего сотрудника \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ЦРУ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдварда Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", который прибыл в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москву\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", на осужденного в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Америке\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " бизнесмена \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Виктора Бута\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". &quot;\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " ни в коем случае не высылать в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", а обменять на \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Виктора Бута\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " и \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Константина Ярошенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". В идеале — добавить генерала \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Олега Калугина\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "&quot;, — написал он в своем микроблоге в \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", работавший на компанию \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Booz Allen Hamilton\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " — подрядчика \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Центрального разведывательного управления\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", в начале июня распространил секретный ордер суда, по которому спецслужбы получили доступ ко всем звонкам крупнейшего сотового оператора \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Verizon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", а также данные о сверхсекретной программе агентства национальной безопасности PRISM, позволяющей отслеживать электронные коммуникации на крупнейших сайтах. В воскресенье стало известно, что \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " прибыл из \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Гонконга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москву\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " и запросил убежища в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эквадоре\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Что ждет \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдварда Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдвард Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", наверное, не знал только одного: что отныне от него ничего уже не будет зависеть. \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москва-Гавана-Каракас\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " – в новой траектории жизни. В плотном кольце новых друзей, которым нужно быстро вытащить из тебя то, что еще не сказал. А может, говорить больше и нечего. Когда и они в этом убедятся, как раз объявят посадку в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Каракасе\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Добро пожаловать в третий мир. Потому что если тебе нет место в первом, не станет с тобой надолго связываться и второй. Подробнее &gt;&gt;  </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19963f3a",
   "metadata": {},
   "source": [
    "**На этом тексте библиотека SpaCy без ошибок.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb93a50",
   "metadata": {},
   "source": [
    "**Посмотрим на список токенов, частей речи и сущностей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "953d3a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жириновский PROPN nsubj\n",
      "предлагает VERB ROOT\n",
      "обменять VERB xcomp\n",
      "с ADP case\n",
      "США PROPN obl\n",
      "Сноудена PROPN obj\n",
      "на ADP case\n",
      "Бута PROPN obl\n",
      "Лидер NOUN nsubj\n",
      "ЛДПР PROPN nmod\n",
      "Владимир PROPN appos\n",
      "Жириновский PROPN flat:name\n",
      "предложил VERB conj\n",
      "обменять VERB xcomp\n",
      "бывшего ADJ amod\n",
      "сотрудника NOUN obj\n",
      "ЦРУ PROPN nmod\n",
      "США PROPN nmod\n",
      "Эдварда PROPN appos\n",
      "Сноудена PROPN flat:name\n",
      ", PUNCT punct\n",
      "который PRON nsubj\n",
      "прибыл VERB acl:relcl\n",
      "в ADP case\n",
      "Москву PROPN obl\n",
      ", PUNCT punct\n",
      "на ADP case\n",
      "осужденного NOUN acl\n",
      "в ADP case\n",
      "Америке PROPN obl\n",
      "бизнесмена NOUN appos\n",
      "Виктора PROPN appos\n",
      "Бута PROPN flat:name\n",
      ". PUNCT punct\n",
      "\" PUNCT punct\n",
      "Сноудена NOUN obj\n",
      "ни PART advmod\n",
      "в ADP fixed\n",
      "коем DET fixed\n",
      "случае NOUN fixed\n",
      "не PART advmod\n",
      "высылать VERB ROOT\n",
      "в ADP case\n",
      "США PROPN obl\n",
      ", PUNCT punct\n",
      "а CCONJ cc\n",
      "обменять VERB conj\n",
      "на ADP case\n",
      "Виктора PROPN obl\n",
      "Бута PROPN flat:name\n",
      "и CCONJ cc\n",
      "Константина PROPN conj\n",
      "Ярошенко PROPN flat:name\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "идеале NOUN ROOT\n",
      "— PUNCT punct\n",
      "добавить VERB parataxis\n",
      "генерала NOUN obj\n",
      "Олега PROPN appos\n",
      "Калугина PROPN flat:name\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "— PUNCT punct\n",
      "написал VERB parataxis\n",
      "он PRON nsubj\n",
      "в ADP case\n",
      "своем DET det\n",
      "микроблоге NOUN obl\n",
      "в ADP case\n",
      "Twitter PROPN nmod\n",
      ". PUNCT punct\n",
      "Сноуден PROPN nsubj\n",
      ", PUNCT punct\n",
      "работавший VERB acl\n",
      "на ADP case\n",
      "компанию NOUN obl\n",
      "Booz X appos\n",
      "Allen X flat:foreign\n",
      "Hamilton X flat:foreign\n",
      "— PUNCT punct\n",
      "подрядчика NOUN appos\n",
      "Центрального ADJ amod\n",
      "разведывательного ADJ amod\n",
      "управления NOUN nmod\n",
      "США PROPN nmod\n",
      ", PUNCT punct\n",
      "в ADP case\n",
      "начале NOUN obl\n",
      "июня NOUN nmod\n",
      "распространил VERB ROOT\n",
      "секретный ADJ amod\n",
      "ордер NOUN obj\n",
      "суда NOUN nmod\n",
      ", PUNCT punct\n",
      "по ADP case\n",
      "которому PRON obl\n",
      "спецслужбы NOUN nsubj\n",
      "получили VERB acl:relcl\n",
      "доступ NOUN obj\n",
      "ко ADP case\n",
      "всем DET det\n",
      "звонкам NOUN nmod\n",
      "крупнейшего ADJ amod\n",
      "сотового ADJ amod\n",
      "оператора NOUN nmod\n",
      "Verizon PROPN appos\n",
      ", PUNCT punct\n",
      "а CCONJ cc\n",
      "также ADV fixed\n",
      "данные NOUN conj\n",
      "о ADP case\n",
      "сверхсекретной ADJ amod\n",
      "программе NOUN nmod\n",
      "агентства NOUN nmod\n",
      "национальной ADJ amod\n",
      "безопасности NOUN nmod\n",
      "PRISM PROPN appos\n",
      ", PUNCT punct\n",
      "позволяющей VERB acl\n",
      "отслеживать VERB xcomp\n",
      "электронные ADJ amod\n",
      "коммуникации NOUN obj\n",
      "на ADP case\n",
      "крупнейших ADJ amod\n",
      "сайтах NOUN obl\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "воскресенье NOUN obl\n",
      "стало VERB ROOT\n",
      "известно ADJ xcomp\n",
      ", PUNCT punct\n",
      "что SCONJ mark\n",
      "Сноуден PROPN nsubj\n",
      "прибыл VERB ccomp\n",
      "из ADP case\n",
      "Гонконга PROPN obl\n",
      "в ADP case\n",
      "Москву PROPN obl\n",
      "и CCONJ cc\n",
      "запросил VERB conj\n",
      "убежища NOUN obj\n",
      "в ADP case\n",
      "Эквадоре PROPN obl\n",
      ". PUNCT punct\n",
      "Что PRON obj\n",
      "ждет VERB ccomp\n",
      "Эдварда PROPN nsubj\n",
      "Сноудена PROPN flat:name\n",
      "Эдвард PROPN nsubj\n",
      "Сноуден PROPN flat:name\n",
      ", PUNCT punct\n",
      "наверное ADV parataxis\n",
      ", PUNCT punct\n",
      "не PART advmod\n",
      "знал VERB ROOT\n",
      "только PART advmod\n",
      "одного NUM obj\n",
      ": PUNCT punct\n",
      "что PRON nsubj\n",
      "отныне ADV advmod\n",
      "от ADP case\n",
      "него PRON obl\n",
      "ничего PRON nsubj\n",
      "уже ADV advmod\n",
      "не PART advmod\n",
      "будет AUX aux\n",
      "зависеть VERB parataxis\n",
      ". PUNCT punct\n",
      "Москва PROPN nsubj\n",
      "- PROPN nsubj\n",
      "Гавана PROPN nsubj\n",
      "- PROPN nsubj\n",
      "Каракас PROPN nsubj\n",
      "– PUNCT punct\n",
      "в ADP case\n",
      "новой ADJ amod\n",
      "траектории NOUN ROOT\n",
      "жизни NOUN nmod\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "плотном ADJ amod\n",
      "кольце NOUN ROOT\n",
      "новых ADJ amod\n",
      "друзей NOUN nmod\n",
      ", PUNCT punct\n",
      "которым PRON iobj\n",
      "нужно ADJ acl:relcl\n",
      "быстро ADV advmod\n",
      "вытащить VERB csubj\n",
      "из ADP case\n",
      "тебя PRON obl\n",
      "то PRON obj\n",
      ", PUNCT punct\n",
      "что SCONJ nsubj\n",
      "еще ADV advmod\n",
      "не PART advmod\n",
      "сказал VERB acl\n",
      ". PUNCT punct\n",
      "А CCONJ cc\n",
      "может VERB parataxis\n",
      ", PUNCT punct\n",
      "говорить VERB csubj\n",
      "больше ADV advmod\n",
      "и CCONJ cc\n",
      "нечего VERB ROOT\n",
      ". PUNCT punct\n",
      "Когда SCONJ mark\n",
      "и PART advmod\n",
      "они PRON nsubj\n",
      "в ADP case\n",
      "этом PRON obl\n",
      "убедятся VERB ROOT\n",
      ", PUNCT punct\n",
      "как ADV advmod\n",
      "раз NOUN fixed\n",
      "объявят VERB advcl\n",
      "посадку NOUN obj\n",
      "в ADP case\n",
      "Каракасе PROPN obl\n",
      ". PUNCT punct\n",
      "Добро NOUN ROOT\n",
      "пожаловать VERB nmod\n",
      "в ADP case\n",
      "третий ADJ amod\n",
      "мир NOUN obl\n",
      ". PUNCT punct\n",
      "Потому ADV mark\n",
      "что SCONJ fixed\n",
      "если SCONJ mark\n",
      "тебе PRON iobj\n",
      "нет VERB advcl\n",
      "место NOUN nsubj\n",
      "в ADP case\n",
      "первом ADJ nmod\n",
      ", PUNCT punct\n",
      "не PART advmod\n",
      "станет VERB ROOT\n",
      "с ADP case\n",
      "тобой PRON obl\n",
      "надолго ADV advmod\n",
      "связываться VERB xcomp\n",
      "и CCONJ cc\n",
      "второй ADJ conj\n",
      ". PUNCT punct\n",
      "Подробнее ADV ROOT\n",
      "> PUNCT punct\n",
      "> PUNCT punct\n",
      "  SPACE dep\n"
     ]
    }
   ],
   "source": [
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df676bc",
   "metadata": {},
   "source": [
    "### DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b168514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2==0.9\n",
      "  Downloading pymorphy2-0.9-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m426.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.9) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.9) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.9) (0.7.2)\n",
      "Installing collected packages: pymorphy2\n",
      "  Attempting uninstall: pymorphy2\n",
      "    Found existing installation: pymorphy2 0.8\n",
      "    Uninstalling pymorphy2-0.8:\n",
      "      Successfully uninstalled pymorphy2-0.8\n",
      "Successfully installed pymorphy2-0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05a4faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.18.0)\n",
      "Requirement already satisfied: scipy in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: librosa in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: unidecode in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.3.6)\n",
      "Requirement already satisfied: inflect in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (6.0.2)\n",
      "Requirement already satisfied: transformers in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.23.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (0.4.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (0.56.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (0.11.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from librosa) (1.2.0)\n",
      "Collecting pydantic>=1.9.1\n",
      "  Using cached pydantic-1.10.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: filelock in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (5.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (63.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata->numba>=0.45.1->librosa) (3.8.0)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.3\n",
      "    Uninstalling pydantic-1.3:\n",
      "      Successfully uninstalled pydantic-1.3\n",
      "Successfully installed pydantic-1.10.2\n",
      "Collecting deeppavlov\n",
      "  Using cached deeppavlov-0.17.6-py3-none-any.whl (878 kB)\n",
      "Requirement already satisfied: uvicorn==0.11.7 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.11.7)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: prometheus-client==0.7.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.7.1)\n",
      "Requirement already satisfied: pytz==2019.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.25.3)\n",
      "Collecting scikit-learn==0.21.2\n",
      "  Using cached scikit-learn-0.21.2.tar.gz (12.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pymorphy2==0.8\n",
      "  Using cached pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: filelock==3.0.12 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (3.0.12)\n",
      "Requirement already satisfied: Cython==0.29.14 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.29.14)\n",
      "Collecting pytelegrambotapi==3.6.7\n",
      "  Using cached pyTelegramBotAPI-3.6.7-py3-none-any.whl\n",
      "Requirement already satisfied: overrides==2.7.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: click==7.1.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: pyopenssl==22.0.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (22.0.0)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: scipy==1.4.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: tqdm==4.62.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (4.62.0)\n",
      "Requirement already satisfied: h5py==2.10.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (2.10.0)\n",
      "Collecting pydantic==1.3\n",
      "  Using cached pydantic-1.3-py36.py37.py38-none-any.whl (85 kB)\n",
      "Collecting aio-pika==6.4.1\n",
      "  Using cached aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: uvloop==0.14.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: requests==2.22.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (2.4.417127.4579844)\n",
      "Requirement already satisfied: numpy==1.18.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (1.18.0)\n",
      "Requirement already satisfied: protobuf<4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (3.19.6)\n",
      "Requirement already satisfied: nltk==3.4.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: yarl in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (1.8.1)\n",
      "Collecting aiormq<4,>=3.2.0\n",
      "  Using cached aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: six in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from h5py==2.10.0->deeppavlov) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: cryptography>=35.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyopenssl==22.0.0->deeppavlov) (37.0.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (1.25.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2022.9.14)\n",
      "Requirement already satisfied: joblib in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sacremoses==0.0.35->deeppavlov) (1.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: websockets==8.* in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
      "Requirement already satisfied: httptools==0.1.* in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.1)\n",
      "Requirement already satisfied: multidict>=4.0 in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n",
      "Requirement already satisfied: pycparser in /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n",
      "Building wheels for collected packages: scikit-learn\n",
      "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[66 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Partial import of sklearn during the build process.\n",
      "  \u001b[31m   \u001b[0m blas_opt_info:\n",
      "  \u001b[31m   \u001b[0m blas_mkl_info:\n",
      "  \u001b[31m   \u001b[0m customize UnixCCompiler\n",
      "  \u001b[31m   \u001b[0m   FOUND:\n",
      "  \u001b[31m   \u001b[0m     libraries = ['mkl_rt', 'pthread']\n",
      "  \u001b[31m   \u001b[0m     library_dirs = ['/Users/mac/miniconda3/envs/pytorch_p38/lib']\n",
      "  \u001b[31m   \u001b[0m     define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m     include_dirs = ['/usr/local/include', '/Users/mac/miniconda3/envs/pytorch_p38/include']\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   FOUND:\n",
      "  \u001b[31m   \u001b[0m     libraries = ['mkl_rt', 'pthread']\n",
      "  \u001b[31m   \u001b[0m     library_dirs = ['/Users/mac/miniconda3/envs/pytorch_p38/lib']\n",
      "  \u001b[31m   \u001b[0m     define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m     include_dirs = ['/usr/local/include', '/Users/mac/miniconda3/envs/pytorch_p38/include']\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/mac/miniconda3/envs/pytorch_p38/include -arch x86_64 -I/Users/mac/miniconda3/envs/pytorch_p38/include -arch x86_64\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m compile options: '-c'\n",
      "  \u001b[31m   \u001b[0m extra options: '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m gcc: test_openmp.c\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 290, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup_package()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 286, in setup_package\n",
      "  \u001b[31m   \u001b[0m     setup(**metadata)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/core.py\", line 137, in setup\n",
      "  \u001b[31m   \u001b[0m     config = configuration()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 174, in configuration\n",
      "  \u001b[31m   \u001b[0m     config.add_subpackage('sklearn')\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 1033, in add_subpackage\n",
      "  \u001b[31m   \u001b[0m     config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 999, in get_subpackage\n",
      "  \u001b[31m   \u001b[0m     config = self._get_configuration_from_setup_py(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 941, in _get_configuration_from_setup_py\n",
      "  \u001b[31m   \u001b[0m     config = setup_module.configuration(*args)\n",
      "  \u001b[31m   \u001b[0m   File \"sklearn/setup.py\", line 80, in configuration\n",
      "  \u001b[31m   \u001b[0m     maybe_cythonize_extensions(top_path, config)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/sklearn/_build_utils/__init__.py\", line 68, in maybe_cythonize_extensions\n",
      "  \u001b[31m   \u001b[0m     with_openmp = check_openmp_support()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/sklearn/_build_utils/openmp_helpers.py\", line 140, in check_openmp_support\n",
      "  \u001b[31m   \u001b[0m     raise CompileError(err_message)\n",
      "  \u001b[31m   \u001b[0m distutils.errors.CompileError:\n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m It seems that scikit-learn cannot be built with OpenMP support.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - Make sure you have followed the installation instructions:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://scikit-learn.org/dev/developers/advanced_installation.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If your compiler supports OpenMP but the build still fails, please\n",
      "  \u001b[31m   \u001b[0m   submit a bug report at:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://github.com/scikit-learn/scikit-learn/issues\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If you want to build scikit-learn without OpenMP support, you can set\n",
      "  \u001b[31m   \u001b[0m   the environment variable SKLEARN_NO_OPENMP and rerun the build\n",
      "  \u001b[31m   \u001b[0m   command. Note however that some estimators will run in sequential\n",
      "  \u001b[31m   \u001b[0m   mode and their `n_jobs` parameter will have no effect anymore.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
      "Failed to build scikit-learn\n",
      "Installing collected packages: pymorphy2, pydantic, scikit-learn, pytelegrambotapi, aiormq, aio-pika, deeppavlov\n",
      "  Attempting uninstall: pymorphy2\n",
      "    Found existing installation: pymorphy2 0.9\n",
      "    Uninstalling pymorphy2-0.9:\n",
      "      Successfully uninstalled pymorphy2-0.9\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.2\n",
      "    Uninstalling scikit-learn-1.1.2:\n",
      "      Successfully uninstalled scikit-learn-1.1.2\n",
      "  Running setup.py install for scikit-learn ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for scikit-learn\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[66 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Partial import of sklearn during the build process.\n",
      "  \u001b[31m   \u001b[0m blas_opt_info:\n",
      "  \u001b[31m   \u001b[0m blas_mkl_info:\n",
      "  \u001b[31m   \u001b[0m customize UnixCCompiler\n",
      "  \u001b[31m   \u001b[0m   FOUND:\n",
      "  \u001b[31m   \u001b[0m     libraries = ['mkl_rt', 'pthread']\n",
      "  \u001b[31m   \u001b[0m     library_dirs = ['/Users/mac/miniconda3/envs/pytorch_p38/lib']\n",
      "  \u001b[31m   \u001b[0m     define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m     include_dirs = ['/usr/local/include', '/Users/mac/miniconda3/envs/pytorch_p38/include']\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   FOUND:\n",
      "  \u001b[31m   \u001b[0m     libraries = ['mkl_rt', 'pthread']\n",
      "  \u001b[31m   \u001b[0m     library_dirs = ['/Users/mac/miniconda3/envs/pytorch_p38/lib']\n",
      "  \u001b[31m   \u001b[0m     define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m     include_dirs = ['/usr/local/include', '/Users/mac/miniconda3/envs/pytorch_p38/include']\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m C compiler: gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/mac/miniconda3/envs/pytorch_p38/include -arch x86_64 -I/Users/mac/miniconda3/envs/pytorch_p38/include -arch x86_64\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m compile options: '-c'\n",
      "  \u001b[31m   \u001b[0m extra options: '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m gcc: test_openmp.c\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 290, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup_package()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 286, in setup_package\n",
      "  \u001b[31m   \u001b[0m     setup(**metadata)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/core.py\", line 137, in setup\n",
      "  \u001b[31m   \u001b[0m     config = configuration()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/setup.py\", line 174, in configuration\n",
      "  \u001b[31m   \u001b[0m     config.add_subpackage('sklearn')\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 1033, in add_subpackage\n",
      "  \u001b[31m   \u001b[0m     config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 999, in get_subpackage\n",
      "  \u001b[31m   \u001b[0m     config = self._get_configuration_from_setup_py(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/distutils/misc_util.py\", line 941, in _get_configuration_from_setup_py\n",
      "  \u001b[31m   \u001b[0m     config = setup_module.configuration(*args)\n",
      "  \u001b[31m   \u001b[0m   File \"sklearn/setup.py\", line 80, in configuration\n",
      "  \u001b[31m   \u001b[0m     maybe_cythonize_extensions(top_path, config)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/sklearn/_build_utils/__init__.py\", line 68, in maybe_cythonize_extensions\n",
      "  \u001b[31m   \u001b[0m     with_openmp = check_openmp_support()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/pip-install-om6o6fs8/scikit-learn_8cd522258bb1482f930a386152720f51/sklearn/_build_utils/openmp_helpers.py\", line 140, in check_openmp_support\n",
      "  \u001b[31m   \u001b[0m     raise CompileError(err_message)\n",
      "  \u001b[31m   \u001b[0m distutils.errors.CompileError:\n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m It seems that scikit-learn cannot be built with OpenMP support.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - Make sure you have followed the installation instructions:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://scikit-learn.org/dev/developers/advanced_installation.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If your compiler supports OpenMP but the build still fails, please\n",
      "  \u001b[31m   \u001b[0m   submit a bug report at:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     https://github.com/scikit-learn/scikit-learn/issues\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m - If you want to build scikit-learn without OpenMP support, you can set\n",
      "  \u001b[31m   \u001b[0m   the environment variable SKLEARN_NO_OPENMP and rerun the build\n",
      "  \u001b[31m   \u001b[0m   command. Note however that some estimators will run in sequential\n",
      "  \u001b[31m   \u001b[0m   mode and their `n_jobs` parameter will have no effect anymore.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m                     ***\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h  Rolling back uninstall of scikit-learn\n",
      "  Moving to /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/scikit_learn-1.1.2.dist-info/\n",
      "   from /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/~cikit_learn-1.1.2.dist-info\n",
      "  Moving to /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/sklearn/\n",
      "   from /Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/~klearn\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m scikit-learn\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y tensorflow tensorflow-gpu\n",
    "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7f79b",
   "metadata": {},
   "source": [
    "**В этот блокнот не удалось установить библиотеку deeppalov из-за конфликта зависимостей.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db417380",
   "metadata": {},
   "source": [
    "**Написать свой NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fab90252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from razdel import tokenize\n",
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "415bd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test_true, y_test_pred):\n",
    "    print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f736e89",
   "metadata": {},
   "source": [
    "Воспользуемся размеченным корпусом текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce1caafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='1047',\n",
       "    text='Жириновский предлагает обменять с США Сноудена на Бута\\r\\n\\r\\nЛидер ЛДПР Владимир Жириновский предложил обменять бывшего сотрудника ЦРУ США Эдварда Сноудена, который прибыл в Москву, на осужденного в Америке бизнесмена Виктора Бута.\\r\\n\\r\\n\"Сноудена ни в коем случае не высылать в США, а обменять на Виктора Бута и Константина Ярошенко. В идеале — добавить генерала Олега Калугина\", — написал он в своем микроблоге в Twitter.\\r\\n\\r\\nСноуден, работавший на компанию Booz Allen Hamilton — подрядчика Центрального разведывательного управления США, в начале июня распространил секретный ордер суда, по которому спецслужбы получили доступ ко всем звонкам крупнейшего сотового оператора Verizon, а также данные о сверхсекретной программе агентства национальной безопасности PRISM, позволяющей отслеживать электронные коммуникации на крупнейших сайтах. В воскресенье стало известно, что Сноуден прибыл из Гонконга в Москву и запросил убежища в Эквадоре.\\r\\n\\r\\nЧто ждет Эдварда Сноудена\\r\\n\\r\\nЭдвард Сноуден, наверное, не знал только одного: что отныне от него ничего уже не будет зависеть. Москва-Гавана-Каракас – в новой траектории жизни. В плотном кольце новых друзей, которым нужно быстро вытащить из тебя то, что еще не сказал. А может, говорить больше и нечего. Когда и они в этом убедятся, как раз объявят посадку в Каракасе. Добро пожаловать в третий мир. Потому что если тебе нет место в первом, не станет с тобой надолго связываться и второй. Подробнее >>\\r\\n\\r\\n\\r\\n',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='PER',\n",
       "         start=0,\n",
       "         stop=11,\n",
       "         text='Жириновский'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='GEOPOLIT',\n",
       "         start=34,\n",
       "         stop=37,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='PER',\n",
       "         start=38,\n",
       "         stop=46,\n",
       "         text='Сноудена'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='PER',\n",
       "         start=50,\n",
       "         stop=54,\n",
       "         text='Бута'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='ORG',\n",
       "         start=64,\n",
       "         stop=68,\n",
       "         text='ЛДПР'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='PER',\n",
       "         start=69,\n",
       "         stop=89,\n",
       "         text='Владимир Жириновский'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='ORG',\n",
       "         start=128,\n",
       "         stop=131,\n",
       "         text='ЦРУ'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='GEOPOLIT',\n",
       "         start=132,\n",
       "         stop=135,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='PER',\n",
       "         start=136,\n",
       "         stop=152,\n",
       "         text='Эдварда Сноудена'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='LOC',\n",
       "         start=171,\n",
       "         stop=177,\n",
       "         text='Москву'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='GEOPOLIT',\n",
       "         start=196,\n",
       "         stop=203,\n",
       "         text='Америке'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='PER',\n",
       "         start=215,\n",
       "         stop=227,\n",
       "         text='Виктора Бута'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='PER',\n",
       "         start=233,\n",
       "         stop=241,\n",
       "         text='Сноудена'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='GEOPOLIT',\n",
       "         start=273,\n",
       "         stop=276,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='PER',\n",
       "         start=292,\n",
       "         stop=304,\n",
       "         text='Виктора Бута'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='PER',\n",
       "         start=307,\n",
       "         stop=327,\n",
       "         text='Константина Ярошенко'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='PER',\n",
       "         start=358,\n",
       "         stop=372,\n",
       "         text='Олега Калугина'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T18',\n",
       "         type='MEDIA',\n",
       "         start=409,\n",
       "         stop=416,\n",
       "         text='Twitter'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T19',\n",
       "         type='PER',\n",
       "         start=421,\n",
       "         stop=428,\n",
       "         text='Сноуден'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T20',\n",
       "         type='ORG',\n",
       "         start=453,\n",
       "         stop=472,\n",
       "         text='Booz Allen Hamilton'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T21',\n",
       "         type='ORG',\n",
       "         start=486,\n",
       "         stop=527,\n",
       "         text='Центрального разведывательного управления'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T22',\n",
       "         type='GEOPOLIT',\n",
       "         start=528,\n",
       "         stop=531,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T23',\n",
       "         type='ORG',\n",
       "         start=669,\n",
       "         stop=676,\n",
       "         text='Verizon'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T24',\n",
       "         type='ORG',\n",
       "         start=756,\n",
       "         stop=761,\n",
       "         text='PRISM'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T25',\n",
       "         type='PER',\n",
       "         start=868,\n",
       "         stop=875,\n",
       "         text='Сноуден'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T26',\n",
       "         type='GEOPOLIT',\n",
       "         start=886,\n",
       "         stop=894,\n",
       "         text='Гонконга'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T27',\n",
       "         type='LOC',\n",
       "         start=897,\n",
       "         stop=903,\n",
       "         text='Москву'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T28',\n",
       "         type='GEOPOLIT',\n",
       "         start=925,\n",
       "         stop=933,\n",
       "         text='Эквадоре'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T29',\n",
       "         type='PER',\n",
       "         start=947,\n",
       "         stop=963,\n",
       "         text='Эдварда Сноудена'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T30',\n",
       "         type='PER',\n",
       "         start=967,\n",
       "         stop=981,\n",
       "         text='Эдвард Сноуден'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T31',\n",
       "         type='LOC',\n",
       "         start=1065,\n",
       "         stop=1086,\n",
       "         text='Москва-Гавана-Каракас'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T32',\n",
       "         type='LOC',\n",
       "         start=1297,\n",
       "         stop=1305,\n",
       "         text='Каракасе'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = load_ne5('Collection5/')\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d37687b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a673c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b10753c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219014\n",
       "PER          21178\n",
       "ORG          13641\n",
       "LOC           4564\n",
       "GEOPOLIT      4349\n",
       "MEDIA         2481\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "73629a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Д</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Медведев</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>назначил</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ряд</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  tag\n",
       "0         Д  PER\n",
       "1         .  PER\n",
       "2  Медведев  PER\n",
       "3  назначил  OUT\n",
       "4       ряд  OUT"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ba950f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce0fc0",
   "metadata": {},
   "source": [
    "**Закодируем целевую переменную**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6f1c0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e95132",
   "metadata": {},
   "source": [
    "Посмотрим на классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b02d77ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "442df467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.apply(len).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "db388c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234997        прошедшие\n",
       "70628          назначен\n",
       "47866                 в\n",
       "191428          Василий\n",
       "255655      возвращения\n",
       "              ...      \n",
       "184632                ,\n",
       "94564       организаций\n",
       "122979    парламентария\n",
       "13008                на\n",
       "163487          желанию\n",
       "Name: word, Length: 66307, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d48a289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 21:18:31.866202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e300a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a283f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Здесь может быть предобработка текста\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(  \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            #ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "63599c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29842"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3dd0b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax') # [OUT, PER, ORG, LOC, GEOPOLIT, MEDIA]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c67528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "126922ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bd5ce01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12433/12433 [==============================] - 243s 20ms/step - loss: 0.1452 - accuracy: 0.9571 - val_loss: 0.2059 - val_accuracy: 0.9395\n",
      "Epoch 2/3\n",
      "12433/12433 [==============================] - 255s 20ms/step - loss: 0.1124 - accuracy: 0.9653 - val_loss: 0.4621 - val_accuracy: 0.8940\n",
      "Epoch 3/3\n",
      "12433/12433 [==============================] - 281s 23ms/step - loss: 0.1055 - accuracy: 0.9666 - val_loss: 0.3598 - val_accuracy: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8064499ee0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a2c848ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "46e410a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010966691883908"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cfa441d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      1053\n",
      "           1       0.85      0.78      0.81      1118\n",
      "           2       0.95      0.77      0.85       631\n",
      "           3       0.89      0.57      0.70      3424\n",
      "           4       0.97      0.92      0.94     54777\n",
      "           5       0.50      0.87      0.63      5304\n",
      "\n",
      "    accuracy                           0.89     66307\n",
      "   macro avg       0.84      0.80      0.80     66307\n",
      "weighted avg       0.92      0.89      0.90     66307\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/ipykernel_2717/4235935365.py\", line 3, in <cell line: 3>\n",
      "    get_classification_report(valid_y, y_pred_classes)\n",
      "  File \"/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/ipykernel_2717/1749492431.py\", line 5, in get_classification_report\n",
      "    print(pd.crosstab(y_test_true, y_test_pred))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 679, in crosstab\n",
      "    if len(names) != len(arrs):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/frame.py\", line 8721, in pivot_table\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 96, in pivot_table\n",
      "    agged = grouped.agg(aggfunc)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 207, in __internal_pivot_table\n",
      "    for level in table.columns.names[1:]:\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/frame.py\", line 9100, in unstack\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/reshape.py\", line 25, in <module>\n",
      "    from pandas.core.arrays.categorical import _factorize_from_iterable\n",
      "ImportError: cannot import name '_factorize_from_iterable' from 'pandas.core.arrays.categorical' (/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/arrays/categorical.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3a3a1",
   "metadata": {},
   "source": [
    "#### Обучим нейронную сеть на биграммах и триграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0028eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Здесь может быть предобработка текста.\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization( \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0e8bf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ee8780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fb7b51bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12433/12433 [==============================] - 262s 21ms/step - loss: 0.2946 - accuracy: 0.9144 - val_loss: 0.2097 - val_accuracy: 0.9378\n",
      "Epoch 2/3\n",
      "12433/12433 [==============================] - 484s 39ms/step - loss: 0.1258 - accuracy: 0.9624 - val_loss: 0.2078 - val_accuracy: 0.9406\n",
      "Epoch 3/3\n",
      "12433/12433 [==============================] - 354s 28ms/step - loss: 0.1091 - accuracy: 0.9659 - val_loss: 0.2096 - val_accuracy: 0.9408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8077b74d30>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c54693eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eb0d7f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363363752557727"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ac196045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\r\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89      1053\n",
      "           1       0.86      0.76      0.81      1118\n",
      "           2       0.95      0.77      0.85       631\n",
      "           3       0.88      0.57      0.69      3424\n",
      "           4       0.94      0.99      0.97     54777\n",
      "           5       0.98      0.70      0.82      5304\n",
      "\n",
      "    accuracy                           0.94     66307\n",
      "   macro avg       0.91      0.78      0.84     66307\n",
      "weighted avg       0.94      0.94      0.94     66307\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/ipykernel_2717/4235935365.py\", line 3, in <cell line: 3>\n",
      "    get_classification_report(valid_y, y_pred_classes)\n",
      "  File \"/var/folders/r4/tj59nbc57_3bl6zqxrkxfk140000gn/T/ipykernel_2717/1749492431.py\", line 5, in get_classification_report\n",
      "    print(pd.crosstab(y_test_true, y_test_pred))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 679, in crosstab\n",
      "    if len(names) != len(arrs):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/frame.py\", line 8721, in pivot_table\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 96, in pivot_table\n",
      "    agged = grouped.agg(aggfunc)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\", line 207, in __internal_pivot_table\n",
      "    for level in table.columns.names[1:]:\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/frame.py\", line 9100, in unstack\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/reshape/reshape.py\", line 25, in <module>\n",
      "    from pandas.core.arrays.categorical import _factorize_from_iterable\n",
      "ImportError: cannot import name '_factorize_from_iterable' from 'pandas.core.arrays.categorical' (/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/core/arrays/categorical.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/mac/miniconda3/envs/pytorch_p38/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0704d0",
   "metadata": {},
   "source": [
    "### Результьат выше у сети, которая обучалась на N-граммах."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
