{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-f8TnGpE_ex"
   },
   "source": [
    "Основная идея, лежащая в основе модели Transformer, - это самовнимание - способность отслеживать различные позиции входной последовательности для вычисления представления этой последовательности. Transformer создает стопки слоев самовнимания, что объясняется ниже в разделах « Масштабируемое скалярное произведение» и « Многоголовое внимание» .\n",
    "\n",
    "Модель преобразователя обрабатывает входные данные переменного размера, используя стопки слоев самовнимания вместо RNN или CNN . Эта общая архитектура имеет ряд преимуществ:\n",
    "\n",
    "* Он не делает никаких предположений о временных / пространственных отношениях между данными. Это идеально подходит для обработки набора объектов.\n",
    "* Выходы уровня можно вычислять параллельно, а не последовательно, как RNN.\n",
    "* Отдаленные элементы могут влиять на вывод друг друга, не проходя через множество RNN-шагов или слоев свертки (например, см. Преобразователь памяти сцены ).\n",
    "* Он может изучать дальнодействующие зависимости. Это проблема для многих задач последовательности.\n",
    "\n",
    "Минусы этой архитектуры:\n",
    "\n",
    "* Для временного ряда выход для временного шага рассчитывается из всей истории, а не только из входных данных и текущего скрытого состояния. Это может быть менее эффективно.\n",
    "* Если вход действительно имеет временное / пространственное соотношение, как текст, должно быть добавлено некоторое позиционное кодирование или модель будет эффективно увидеть мешок слов.\n",
    "\n",
    "После обучения модели вы сможете ввести предложение на русском и вернуть перевод на английском.\n",
    "\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" width=\"800\" alt=\"Attention heatmap\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFG0NDRu5mYQ",
    "outputId": "6aad58a9-652e-4655-ef75-77df0da72789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.2) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.2) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.2) (1.21.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.2) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tfds-nightly\n",
    "\n",
    "# Pin matplotlib version to 3.2.2 since in the latest version\n",
    "# transformer.ipynb fails with the following error:\n",
    "# https://stackoverflow.com/questions/62953704/valueerror-the-number-of-fixedlocator-locations-5-usually-from-a-call-to-set\n",
    "!pip install matplotlib==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4_Qt8W1hJE_"
   },
   "source": [
    "Use [TFDS](https://www.tensorflow.org/datasets) to load the Ru-English translation dataset from the [TED Talks Open Translation Project](https://www.ted.com/participate/translate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q9t4FmN96eN"
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCEKotqosGfq"
   },
   "source": [
    "Создайте собственный токенизатор подслов из training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for ru, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_ru = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (ru.numpy() for ru, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DYWukNFkGQN",
    "outputId": "fed96b25-e2b7-4dc0-82dc-4d2b410f77b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [8073, 1034, 8104, 5774, 13, 3531, 8035]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9KJWJjrsZ4Y"
   },
   "source": [
    "Токенизатор кодирует строку, разбивая ее на подслова, если слово отсутствует в его словаре.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf2ntBxjkqK6",
    "outputId": "3d66bc75-261c-4a8b-b1fb-e630878756ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8073 ----> T\n",
      "1034 ----> ran\n",
      "8104 ----> s\n",
      "5774 ----> former \n",
      "13 ----> is \n",
      "3531 ----> awesome\n",
      "8035 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Добавьте начальный и конечный токены к входу и цели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "# two tokenizers (tokenizer_ru and tokenizer_en) are used to encode two languages (lang1 and lang2). \n",
    "# The encode method is then called on each language, which adds the beginning and end of sequence tokens \n",
    "# (tokenizer_ru.vocabsize and tokenizer_en.vocabsize+1) to the encoded text. Finally, the encoded text for both \n",
    "# languages is returned as a tuple.\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_ru.vocab_size] + tokenizer_ru.encode(\n",
    "      lang1.numpy()) + [tokenizer_ru.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx1sFbR-9fRs"
   },
   "source": [
    "Вы хотите использовать Dataset.map чтобы применить эту функцию к каждому элементу набора данных. Dataset.map работает в графическом режиме.\n",
    "\n",
    "Тензоры графов не имеют значения.\n",
    "В графическом режиме вы можете использовать только операции и функции TensorFlow.\n",
    "Таким образом, вы не можете напрямую .map эту функцию: вам нужно обернуть ее в tf.py_function . tf.py_function будет передавать обычные тензоры (со значением и .numpy() для доступа к нему) в .numpy() функцию python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mah1cS-P70Iz"
   },
   "outputs": [],
   "source": [
    "def tf_encode(ru, en):\n",
    "\n",
    "# tf.pyfunction method is used to call a custom function (encode) with two arguments (ru and en). \n",
    "# The two arguments are then mapped to two data types (tf.int64, tf.int64) which will be returned as a tuple containing two Tensors.\n",
    "  result_ru, result_en = tf.py_function(encode, [ru, en], [tf.int64, tf.int64])\n",
    "\n",
    "#set_shape method is used to set the shape of each Tensor to [None], which means that it will be able to accept variable-length inputs.\n",
    "  result_ru.set_shape([None]) \n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_ru, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QEgbjntk6Yf"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myo7GJnKcVe9"
   },
   "outputs": [],
   "source": [
    "# filter out examples that have a length greater than the specified maxlength. \n",
    "# This is done by checking the length of both x and y using the tf.size method and then using the tf.logicaland method to return\n",
    "#  a boolean value indicating if both lengths are less than the specified max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mk9AZdZ5bcS"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE) # create batches of data in which the examples are padded with zeros to match the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fXvfYVfQr2n",
    "outputId": "9ac5f89a-a1fe-4bca-e8bf-ff1b993d2833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8179,   57,   86, ...,    0,    0,    0],\n",
       "        [8179,    3,   38, ...,    0,    0,    0],\n",
       "        [8179,   57,  135, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8179,    3,    7, ...,    0,    0,    0],\n",
       "        [8179,  138,  250, ...,    0,    0,    0],\n",
       "        [8179,   19,    7, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8245,   90,  101, ...,    0,    0,    0],\n",
       "        [8245,   70,   25, ...,    0,    0,    0],\n",
       "        [8245,   90,  153, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8245,    4,   18, ...,    0,    0,    0],\n",
       "        [8245,   19,   59, ...,    0,    0,    0],\n",
       "        [8245,   24,   18, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Поскольку эта модель не содержит повторений или сверток, добавляется позиционное кодирование, чтобы дать модели некоторую информацию об относительном положении слов в предложении.\n",
    "\n",
    "Вектор позиционного кодирования добавляется к вектору внедрения. Вложения представляют собой токен в d-мерном пространстве, где токены с одинаковым значением будут ближе друг к другу. Но вложения не кодируют относительное положение слов в предложении. Таким образом, после добавления позиционной кодировки слова будут ближе друг к другу на основе сходства их значения и их положения в предложении в d-мерном пространстве.\n",
    "\n",
    "Формула для расчета позиционного кодирования выглядит следующим образом:\n",
    "\n",
    "\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  print(pos * angle_rates)\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis], # стобец\n",
    "                          np.arange(d_model)[np.newaxis, :], # строка\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "1kLCla68EloE",
    "outputId": "62aba047-1474-498d-b471-420e3379ebd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 9.64661620e-01 ... 1.07460783e-04\n",
      "  1.03663293e-04 1.03663293e-04]\n",
      " [2.00000000e+00 2.00000000e+00 1.92932324e+00 ... 2.14921566e-04\n",
      "  2.07326586e-04 2.07326586e-04]\n",
      " ...\n",
      " [4.70000000e+01 4.70000000e+01 4.53390961e+01 ... 5.05065679e-03\n",
      "  4.87217476e-03 4.87217476e-03]\n",
      " [4.80000000e+01 4.80000000e+01 4.63037578e+01 ... 5.15811758e-03\n",
      "  4.97583806e-03 4.97583806e-03]\n",
      " [4.90000000e+01 4.90000000e+01 4.72684194e+01 ... 5.26557836e-03\n",
      "  5.07950135e-03 5.07950135e-03]]\n",
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Получаем 50 строк по 512 значений\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Замаскируйте все маркеры площадок в пакете последовательности. Это гарантирует, что модель не обрабатывает отступы как входные данные. Маска указывает, где присутствует значение пэда 0 : она выводит 1 в этих местах и 0 противном случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # 1 if element is null, otherwise - 0\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7BYeBCNvi7n",
    "outputId": "307ce7a7-6c68-47e4-9b3c-2fc250ccb998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "Маска упреждающего просмотра используется для маскировки будущих токенов в последовательности. Другими словами, маска указывает, какие записи не следует использовать.\n",
    "\n",
    "Это означает, что для предсказания третьего слова будут использоваться только первое и второе слово. Аналогично для предсказания четвертого слова будут использоваться только первое, второе и третье слово и так далее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) # Copy a tensor setting everything outside a central band in each innermost matrix to zero.\n",
    "  return mask  # (seq_len, seq_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxKGuXxaBeeE",
    "outputId": "fca952e3-e1d5-44f8-ed67-7b2fcae70cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8062663  0.6184777  0.62404716]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "print(x)\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "Функция внимания, используемая преобразователем, принимает три входа: Q (запрос), K (ключ), V (значение). Уравнение, используемое для расчета весов внимания:\n",
    "\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "Внимание скалярного произведения масштабируется с коэффициентом квадратного корня из глубины. Это сделано потому, что для больших значений глубины скалярное произведение увеличивается по величине, подталкивая функцию softmax, где у него есть небольшие градиенты, что приводит к очень жесткому softmax.\n",
    "\n",
    "Например, предположим, что Q и K имеют среднее значение 0 и дисперсию 1. Их матричное умножение будет иметь среднее значение 0 и дисперсию dk . Следовательно, для масштабирования используется квадратный корень из dk (а не какое-либо другое число), потому что матрица Q и K должна иметь среднее значение 0 и дисперсию 1, и вы получите более мягкий softmax.\n",
    "\n",
    "Маска умножается на -1e9 (близко к отрицательной бесконечности). Это сделано потому, что маска суммируется с умножением масштабированной матрицы Q и K и применяется непосредственно перед softmax. Цель состоит в том, чтобы обнулить эти ячейки, и большие отрицательные входные данные для softmax близки к нулю на выходе.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "Поскольку нормализация softmax выполняется для K, его значения определяют степень важности, придаваемой Q.\n",
    "\n",
    "Выходные данные представляют собой умножение весов внимания и вектора V (значения). Это гарантирует, что слова, на которых вы хотите сосредоточиться, останутся как есть, а нерелевантные слова будут удалены.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAzUAf2DPlNt",
    "outputId": "5b999a56-9ed6-45c7-dcfc-e4de094343d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg6k-fGhgXra",
    "outputId": "3b2c0fda-408c-4e05-c849-af860d3ff325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAq3YOzUgXhb",
    "outputId": "5153e8d2-ddd5-41fd-f491-3bb39e8197c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dlU8Tm-hYrF",
    "outputId": "901a60f7-f463-425c-8550-78962e376d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.     5.5 ]\n",
      " [ 10.     0.  ]\n",
      " [277.75   2.75]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 10]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Многоголовое внимание состоит из четырех частей:\n",
    "\n",
    "* Слои линейные и разбиваются на головы.\n",
    "* Повышенное внимание к скалярному продукту\n",
    "* Конкатенация голов.\n",
    "* Финальный линейный слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Каждый блок внимания с несколькими головами получает три входа; Q (запрос), K (ключ), V (значение). Они проходят через линейные (плотные) слои и разбиваются на несколько головок.\n",
    "\n",
    "scaled_dot_product_attention определенный выше, применяется к каждой голове (транслируется для эффективности). На этапе внимания необходимо использовать соответствующую маску. Затем вывод внимания для каждой головы объединяется (с использованием tf.transpose и tf.reshape ) и пропускается через последний слой Dense .\n",
    "\n",
    "Вместо одной единственной головы внимания Q, K и V разделены на несколько голов, потому что это позволяет модели совместно обращать внимание на информацию в разных положениях из разных пространств представления. После разделения каждая голова имеет уменьшенную размерность, поэтому общая стоимость вычислений такая же, как и внимание одной головы с полной размерностью.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    # If one component of shape is the special value -1, the size of that dimension is computed so that \n",
    "    # the total size remains constant. In particular, a shape of [-1] flattens into 1-D. \n",
    "    # At most one component of shape can be -1.\n",
    "    \n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Создайте слой MultiHeadAttention чтобы попробовать. В каждом месте в последовательности y MultiHeadAttention запускает все 8 головок внимания по всем другим местам в последовательности, возвращая новый вектор той же длины в каждом месте.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hu94p-_-2_BX",
    "outputId": "ca3d9e25-eb65-4d2d-806b-27bf07fac42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tf.Tensor(\n",
      "[[[-8.3446294e-02 -1.1372101e+00  1.7035490e-01  8.1131881e-01\n",
      "    1.1187821e-01 -1.3373218e+00  3.5845959e-01  2.4289222e-01\n",
      "    1.4814006e+00 -1.2713318e-01]\n",
      "  [-4.4430542e-01 -1.2427014e+00  2.4207979e-01  6.1922336e-01\n",
      "   -7.2514206e-02 -6.2541550e-01 -7.4294431e-04  2.4274252e-01\n",
      "    8.6139941e-01 -2.3329727e-01]\n",
      "  [ 7.6528476e-03 -8.0667156e-01  3.6338526e-01  7.9842746e-01\n",
      "    6.4593959e-01 -8.5639685e-01  6.7565972e-01  6.3925183e-01\n",
      "    1.8766478e-01 -5.4372984e-01]]\n",
      "\n",
      " [[ 4.3565303e-01 -5.1512593e-01  1.6545182e-02  1.0204152e+00\n",
      "    1.1188320e-01 -6.9260621e-01  3.5381642e-01  4.6537798e-02\n",
      "    9.2674851e-01 -2.6420832e-01]\n",
      "  [ 5.2235380e-02 -1.2927507e+00  5.9865481e-01  3.7441763e-01\n",
      "    1.3769303e-02 -9.7013462e-01 -2.2618605e-01  1.9565576e-01\n",
      "    1.0930191e+00 -4.4620234e-01]\n",
      "  [ 3.5221142e-01 -8.4387583e-01  5.5822504e-01  3.8493225e-01\n",
      "    5.1525193e-01 -1.1740443e+00  6.2471372e-01  4.2284104e-01\n",
      "    9.1830796e-01 -6.8576860e-01]]], shape=(2, 3, 10), dtype=float32)\n",
      "reshaped X: \n",
      " tf.Tensor(\n",
      "[[[[-8.3446294e-02 -1.1372101e+00  1.7035490e-01  8.1131881e-01\n",
      "     1.1187821e-01]\n",
      "   [-1.3373218e+00  3.5845959e-01  2.4289222e-01  1.4814006e+00\n",
      "    -1.2713318e-01]]\n",
      "\n",
      "  [[-4.4430542e-01 -1.2427014e+00  2.4207979e-01  6.1922336e-01\n",
      "    -7.2514206e-02]\n",
      "   [-6.2541550e-01 -7.4294431e-04  2.4274252e-01  8.6139941e-01\n",
      "    -2.3329727e-01]]\n",
      "\n",
      "  [[ 7.6528476e-03 -8.0667156e-01  3.6338526e-01  7.9842746e-01\n",
      "     6.4593959e-01]\n",
      "   [-8.5639685e-01  6.7565972e-01  6.3925183e-01  1.8766478e-01\n",
      "    -5.4372984e-01]]]\n",
      "\n",
      "\n",
      " [[[ 4.3565303e-01 -5.1512593e-01  1.6545182e-02  1.0204152e+00\n",
      "     1.1188320e-01]\n",
      "   [-6.9260621e-01  3.5381642e-01  4.6537798e-02  9.2674851e-01\n",
      "    -2.6420832e-01]]\n",
      "\n",
      "  [[ 5.2235380e-02 -1.2927507e+00  5.9865481e-01  3.7441763e-01\n",
      "     1.3769303e-02]\n",
      "   [-9.7013462e-01 -2.2618605e-01  1.9565576e-01  1.0930191e+00\n",
      "    -4.4620234e-01]]\n",
      "\n",
      "  [[ 3.5221142e-01 -8.4387583e-01  5.5822504e-01  3.8493225e-01\n",
      "     5.1525193e-01]\n",
      "   [-1.1740443e+00  6.2471372e-01  4.2284104e-01  9.1830796e-01\n",
      "    -6.8576860e-01]]]], shape=(2, 3, 2, 5), dtype=float32)\n",
      "perm_ X: \n",
      " tf.Tensor(\n",
      "[[[[-8.3446294e-02 -1.1372101e+00  1.7035490e-01  8.1131881e-01\n",
      "     1.1187821e-01]\n",
      "   [-4.4430542e-01 -1.2427014e+00  2.4207979e-01  6.1922336e-01\n",
      "    -7.2514206e-02]\n",
      "   [ 7.6528476e-03 -8.0667156e-01  3.6338526e-01  7.9842746e-01\n",
      "     6.4593959e-01]]\n",
      "\n",
      "  [[-1.3373218e+00  3.5845959e-01  2.4289222e-01  1.4814006e+00\n",
      "    -1.2713318e-01]\n",
      "   [-6.2541550e-01 -7.4294431e-04  2.4274252e-01  8.6139941e-01\n",
      "    -2.3329727e-01]\n",
      "   [-8.5639685e-01  6.7565972e-01  6.3925183e-01  1.8766478e-01\n",
      "    -5.4372984e-01]]]\n",
      "\n",
      "\n",
      " [[[ 4.3565303e-01 -5.1512593e-01  1.6545182e-02  1.0204152e+00\n",
      "     1.1188320e-01]\n",
      "   [ 5.2235380e-02 -1.2927507e+00  5.9865481e-01  3.7441763e-01\n",
      "     1.3769303e-02]\n",
      "   [ 3.5221142e-01 -8.4387583e-01  5.5822504e-01  3.8493225e-01\n",
      "     5.1525193e-01]]\n",
      "\n",
      "  [[-6.9260621e-01  3.5381642e-01  4.6537798e-02  9.2674851e-01\n",
      "    -2.6420832e-01]\n",
      "   [-9.7013462e-01 -2.2618605e-01  1.9565576e-01  1.0930191e+00\n",
      "    -4.4620234e-01]\n",
      "   [-1.1740443e+00  6.2471372e-01  4.2284104e-01  9.1830796e-01\n",
      "    -6.8576860e-01]]]], shape=(2, 2, 3, 5), dtype=float32)\n",
      "X: \n",
      " tf.Tensor(\n",
      "[[[ 0.33870602  1.0729351   0.94293696  0.5275038   1.019324\n",
      "    0.17348997  1.4797248   0.47269538 -0.06535351  0.2082527 ]\n",
      "  [-0.1303517   0.3829045   0.766804    0.54427886  0.68947816\n",
      "   -0.10124903  0.89336723  0.44716042  0.28024915 -0.16242735]\n",
      "  [ 0.6741154  -0.02980796  0.29890275  1.0225616   0.9136029\n",
      "    0.6356264   0.45067656  0.13352324 -0.3065802   0.12661287]]\n",
      "\n",
      " [[ 0.06348701  1.1072477   0.86958444  0.37293047  0.6292525\n",
      "    0.10239348  0.94674456  0.38796932  0.46444258  0.03634572]\n",
      "  [ 0.4855854   0.77977926  1.0258355   0.51980203  0.58185816\n",
      "    0.35359532  1.0879914   0.05496812 -0.45142028  0.37370086]\n",
      "  [ 0.38678062  0.5865835   0.87939525  0.680182    1.1571026\n",
      "    0.14357625  1.0746311  -0.04058586 -0.58704096  0.45046002]]], shape=(2, 3, 10), dtype=float32)\n",
      "reshaped X: \n",
      " tf.Tensor(\n",
      "[[[[ 0.33870602  1.0729351   0.94293696  0.5275038   1.019324  ]\n",
      "   [ 0.17348997  1.4797248   0.47269538 -0.06535351  0.2082527 ]]\n",
      "\n",
      "  [[-0.1303517   0.3829045   0.766804    0.54427886  0.68947816]\n",
      "   [-0.10124903  0.89336723  0.44716042  0.28024915 -0.16242735]]\n",
      "\n",
      "  [[ 0.6741154  -0.02980796  0.29890275  1.0225616   0.9136029 ]\n",
      "   [ 0.6356264   0.45067656  0.13352324 -0.3065802   0.12661287]]]\n",
      "\n",
      "\n",
      " [[[ 0.06348701  1.1072477   0.86958444  0.37293047  0.6292525 ]\n",
      "   [ 0.10239348  0.94674456  0.38796932  0.46444258  0.03634572]]\n",
      "\n",
      "  [[ 0.4855854   0.77977926  1.0258355   0.51980203  0.58185816]\n",
      "   [ 0.35359532  1.0879914   0.05496812 -0.45142028  0.37370086]]\n",
      "\n",
      "  [[ 0.38678062  0.5865835   0.87939525  0.680182    1.1571026 ]\n",
      "   [ 0.14357625  1.0746311  -0.04058586 -0.58704096  0.45046002]]]], shape=(2, 3, 2, 5), dtype=float32)\n",
      "perm_ X: \n",
      " tf.Tensor(\n",
      "[[[[ 0.33870602  1.0729351   0.94293696  0.5275038   1.019324  ]\n",
      "   [-0.1303517   0.3829045   0.766804    0.54427886  0.68947816]\n",
      "   [ 0.6741154  -0.02980796  0.29890275  1.0225616   0.9136029 ]]\n",
      "\n",
      "  [[ 0.17348997  1.4797248   0.47269538 -0.06535351  0.2082527 ]\n",
      "   [-0.10124903  0.89336723  0.44716042  0.28024915 -0.16242735]\n",
      "   [ 0.6356264   0.45067656  0.13352324 -0.3065802   0.12661287]]]\n",
      "\n",
      "\n",
      " [[[ 0.06348701  1.1072477   0.86958444  0.37293047  0.6292525 ]\n",
      "   [ 0.4855854   0.77977926  1.0258355   0.51980203  0.58185816]\n",
      "   [ 0.38678062  0.5865835   0.87939525  0.680182    1.1571026 ]]\n",
      "\n",
      "  [[ 0.10239348  0.94674456  0.38796932  0.46444258  0.03634572]\n",
      "   [ 0.35359532  1.0879914   0.05496812 -0.45142028  0.37370086]\n",
      "   [ 0.14357625  1.0746311  -0.04058586 -0.58704096  0.45046002]]]], shape=(2, 2, 3, 5), dtype=float32)\n",
      "X: \n",
      " tf.Tensor(\n",
      "[[[-0.60694134  0.2636442   1.2285306   1.3271891  -1.707418\n",
      "    0.8107741  -0.07385263 -1.0178176   0.04189809 -1.129388  ]\n",
      "  [-0.56661457  0.88365734  0.67982167  0.9119026  -1.2272061\n",
      "    0.34935698 -0.14697182 -0.7651222   0.28721404 -0.35205516]\n",
      "  [-0.57157594  0.58094877  0.4904019   0.2818901  -1.4420487\n",
      "    0.34980777 -0.18991046 -0.87728    -0.3634448  -0.3288951 ]]\n",
      "\n",
      " [[-0.31851298  0.48702684  1.0087583   0.9821595  -1.5356898\n",
      "    0.401346    0.25309834 -0.5617332   0.33773142 -0.559269  ]\n",
      "  [-0.4770825   0.6971857   0.5047903   1.3193121  -1.2989708\n",
      "    0.19379602 -0.15803994 -1.1971049  -0.28108662 -0.7439306 ]\n",
      "  [-0.13853116  0.42131346  0.704127    1.2057905  -1.4032288\n",
      "    0.44717196 -0.32810682 -0.50724196  0.13172747 -0.9269542 ]]], shape=(2, 3, 10), dtype=float32)\n",
      "reshaped X: \n",
      " tf.Tensor(\n",
      "[[[[-0.60694134  0.2636442   1.2285306   1.3271891  -1.707418  ]\n",
      "   [ 0.8107741  -0.07385263 -1.0178176   0.04189809 -1.129388  ]]\n",
      "\n",
      "  [[-0.56661457  0.88365734  0.67982167  0.9119026  -1.2272061 ]\n",
      "   [ 0.34935698 -0.14697182 -0.7651222   0.28721404 -0.35205516]]\n",
      "\n",
      "  [[-0.57157594  0.58094877  0.4904019   0.2818901  -1.4420487 ]\n",
      "   [ 0.34980777 -0.18991046 -0.87728    -0.3634448  -0.3288951 ]]]\n",
      "\n",
      "\n",
      " [[[-0.31851298  0.48702684  1.0087583   0.9821595  -1.5356898 ]\n",
      "   [ 0.401346    0.25309834 -0.5617332   0.33773142 -0.559269  ]]\n",
      "\n",
      "  [[-0.4770825   0.6971857   0.5047903   1.3193121  -1.2989708 ]\n",
      "   [ 0.19379602 -0.15803994 -1.1971049  -0.28108662 -0.7439306 ]]\n",
      "\n",
      "  [[-0.13853116  0.42131346  0.704127    1.2057905  -1.4032288 ]\n",
      "   [ 0.44717196 -0.32810682 -0.50724196  0.13172747 -0.9269542 ]]]], shape=(2, 3, 2, 5), dtype=float32)\n",
      "perm_ X: \n",
      " tf.Tensor(\n",
      "[[[[-0.60694134  0.2636442   1.2285306   1.3271891  -1.707418  ]\n",
      "   [-0.56661457  0.88365734  0.67982167  0.9119026  -1.2272061 ]\n",
      "   [-0.57157594  0.58094877  0.4904019   0.2818901  -1.4420487 ]]\n",
      "\n",
      "  [[ 0.8107741  -0.07385263 -1.0178176   0.04189809 -1.129388  ]\n",
      "   [ 0.34935698 -0.14697182 -0.7651222   0.28721404 -0.35205516]\n",
      "   [ 0.34980777 -0.18991046 -0.87728    -0.3634448  -0.3288951 ]]]\n",
      "\n",
      "\n",
      " [[[-0.31851298  0.48702684  1.0087583   0.9821595  -1.5356898 ]\n",
      "   [-0.4770825   0.6971857   0.5047903   1.3193121  -1.2989708 ]\n",
      "   [-0.13853116  0.42131346  0.704127    1.2057905  -1.4032288 ]]\n",
      "\n",
      "  [[ 0.401346    0.25309834 -0.5617332   0.33773142 -0.559269  ]\n",
      "   [ 0.19379602 -0.15803994 -1.1971049  -0.28108662 -0.7439306 ]\n",
      "   [ 0.44717196 -0.32810682 -0.50724196  0.13172747 -0.9269542 ]]]], shape=(2, 2, 3, 5), dtype=float32)\n",
      "Выходные размерности:  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3, 10]), TensorShape([2, 2, 3, 3]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=10, num_heads=2)\n",
    "y = tf.random.uniform((2, 3, 10))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "print('Выходные размерности:  \\n')\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Сеть с точечной прямой связью состоит из двух полностью связанных слоев с активацией ReLU между ними.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mytb1lPyOHLB",
    "outputId": "8e3f59e7-e8ac-4bd5-98ad-3000fa3acf86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "Модель трансформера следует той же общей схеме, что и стандартная последовательность действий с моделью внимания .\n",
    "\n",
    "Входное предложение проходит через N уровней кодировщика, которые генерируют выходные данные для каждого слова / токена в последовательности.\n",
    "Декодер отслеживает вывод кодировщика и свой собственный ввод (самовнимание), чтобы предсказать следующее слово.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Каждый уровень кодировщика состоит из подслоев:\n",
    "\n",
    "Многоголовое внимание (с дополнительной маской)\n",
    "Точечные сети прямого распространения.\n",
    "Каждый из этих подуровней имеет остаточную связь вокруг себя, за которой следует нормализация уровня. Остаточные соединения помогают избежать проблемы исчезающего градиента в глубоких сетях.\n",
    "\n",
    "Результатом каждого подслоя является LayerNorm(x + Sublayer(x)) . Нормализация выполняется по d_model (последняя). В трансформаторе N слоев кодировщика.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzZRXdO0mI48",
    "outputId": "991be10a-43d9-4377-f51e-af2fc46ebdfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Каждый слой декодера состоит из подслоев:\n",
    "\n",
    "Замаскированное внимание с несколькими головами (с опережающей маской и дополнительной маской)\n",
    "Многоголовое внимание (с дополнительной маской). V (значение) и K (ключ) получают выходной сигнал энкодера в качестве входных данных. Q (запрос) получает выходные данные от подуровня замаскированного многоголового внимания.\n",
    "Точечные сети прямого распространения\n",
    "Каждый из этих подуровней имеет остаточную связь вокруг себя, за которой следует нормализация уровня. Результатом каждого подслоя является LayerNorm(x + Sublayer(x)) . Нормализация выполняется по d_model (последняя).\n",
    "\n",
    "В трансформаторе N слоев декодера.\n",
    "\n",
    "Поскольку Q принимает выходные данные от первого блока внимания декодера, а K принимает выходные данные кодировщика, веса внимания представляют важность, придаваемую входу декодера на основе выходных данных кодера. Другими словами, декодер предсказывает следующее слово, глядя на выходные данные кодировщика и самостоятельно присматриваясь к своим собственным выходным данным. См. Демонстрацию выше в разделе «Внимание к скалярному произведению».\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ne2Bqx8k71l0",
    "outputId": "4a7a2a3c-9715-434e-9838-9ae6d19fcd16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1] #40\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QG9nueFQKXx",
    "outputId": "42eaafa2-9ee9-427a-d4b4-5bc6e16b148b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.96466162 ...    0.00010746\n",
      "     0.00010366    0.00010366]\n",
      " [   2.            2.            1.92932324 ...    0.00021492\n",
      "     0.00020733    0.00020733]\n",
      " ...\n",
      " [9997.         9997.         9643.72221425 ...    1.07428545\n",
      "     1.03632194    1.03632194]\n",
      " [9998.         9998.         9644.68687587 ...    1.07439291\n",
      "     1.0364256     1.0364256 ]\n",
      " [9999.         9999.         9645.65153749 ...    1.07450037\n",
      "     1.03652927    1.03652927]]\n",
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1jXoAMRZyvu",
    "outputId": "16133540-c206-4111-a354-06cb462e48f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.96466162 ...    0.00010746\n",
      "     0.00010366    0.00010366]\n",
      " [   2.            2.            1.92932324 ...    0.00021492\n",
      "     0.00020733    0.00020733]\n",
      " ...\n",
      " [4997.         4997.         4820.4141147  ...    0.53698153\n",
      "     0.51800547    0.51800547]\n",
      " [4998.         4998.         4821.37877632 ...    0.53708899\n",
      "     0.51810914    0.51810914]\n",
      " [4999.         4999.         4822.34343794 ...    0.53719645\n",
      "     0.5182128     0.5182128 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ4fbQcIkHW1",
    "outputId": "ea8dd197-4210-4ce2-d171-76877b8850dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.96466162 ...    0.00010746\n",
      "     0.00010366    0.00010366]\n",
      " [   2.            2.            1.92932324 ...    0.00021492\n",
      "     0.00020733    0.00020733]\n",
      " ...\n",
      " [9997.         9997.         9643.72221425 ...    1.07428545\n",
      "     1.03632194    1.03632194]\n",
      " [9998.         9998.         9644.68687587 ...    1.07439291\n",
      "     1.0364256     1.0364256 ]\n",
      " [9999.         9999.         9645.65153749 ...    1.07450037\n",
      "     1.03652927    1.03652927]]\n",
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.96466162 ...    0.00010746\n",
      "     0.00010366    0.00010366]\n",
      " [   2.            2.            1.92932324 ...    0.00021492\n",
      "     0.00020733    0.00020733]\n",
      " ...\n",
      " [5997.         5997.         5785.07573461 ...    0.64444231\n",
      "     0.62166877    0.62166877]\n",
      " [5998.         5998.         5786.04039623 ...    0.64454978\n",
      "     0.62177243    0.62177243]\n",
      " [5999.         5999.         5787.00505785 ...    0.64465724\n",
      "     0.62187609    0.62187609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_ru.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, np.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkvtTdKawj4p"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omzm9c9wyRJM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "f33ZCgvHpPdG",
    "outputId": "1960c355-1052-4e0a-a112-95c6e92fb2d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "  \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiysUa--4tOU",
    "outputId": "0b2b8073-4493-4b52-b1c6-014c2ad0e3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.86596432 ...    0.00013335\n",
      "     0.00011548    0.00011548]\n",
      " [   2.            2.            1.73192865 ...    0.0002667\n",
      "     0.00023096    0.00023096]\n",
      " ...\n",
      " [8178.         8178.         7081.85623644 ...    1.09055383\n",
      "     0.94438071    0.94438071]\n",
      " [8179.         8179.         7082.72220076 ...    1.09068718\n",
      "     0.94449619    0.94449619]\n",
      " [8180.         8180.         7083.58816509 ...    1.09082053\n",
      "     0.94461166    0.94461166]]\n",
      "[[   0.            0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [   1.            1.            0.86596432 ...    0.00013335\n",
      "     0.00011548    0.00011548]\n",
      " [   2.            2.            1.73192865 ...    0.0002667\n",
      "     0.00023096    0.00023096]\n",
      " ...\n",
      " [8244.         8244.         7139.00988178 ...    1.09935507\n",
      "     0.95200227    0.95200227]\n",
      " [8245.         8245.         7139.8758461  ...    1.09948842\n",
      "     0.95211775    0.95211775]\n",
      " [8246.         8246.         7140.74181043 ...    1.09962177\n",
      "     0.95223322    0.95223322]]\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNhuYfllndLZ"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "Цель делится на tar_inp и tar_real. tar_inp передается декодеру в качестве входных данных. tar_real - это тот же самый ввод, сдвинутый на 1: в каждом месте tar_input tar_real содержит следующий токен, который должен быть предсказан.\n",
    "\n",
    "Например, sentence = \"SOS Лев в джунглях спит EOS\"\n",
    "\n",
    "tar_inp = \"SOS tar_inp лев в джунглях\"\n",
    "\n",
    "tar_real = \"Лев в джунглях спит EOS\"\n",
    "\n",
    "Преобразователь - это авторегрессивная модель: он делает прогнозы по частям и использует свои выходные данные, чтобы решить, что делать дальше.\n",
    "\n",
    "Во время обучения в этом примере используется принуждение учителя (как в учебнике по созданию текста ). Принуждение учителя передает истинный результат следующему временному шагу независимо от того, что модель предсказывает на текущем временном шаге.\n",
    "\n",
    "Поскольку преобразователь предсказывает каждое слово, самовнимание позволяет ему смотреть на предыдущие слова во входной последовательности, чтобы лучше предсказать следующее слово.\n",
    "\n",
    "Чтобы модель не просматривала ожидаемый результат, в модели используется маска просмотра вперед.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Ru is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbvmaKNiznHZ",
    "outputId": "3500e84d-24c1-42b9-93a2-3e76109d3da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0325 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.9803 Accuracy 0.0007\n",
      "Epoch 1 Batch 100 Loss 8.8909 Accuracy 0.0209\n",
      "Epoch 1 Batch 150 Loss 8.7923 Accuracy 0.0327\n",
      "Epoch 1 Batch 200 Loss 8.6688 Accuracy 0.0432\n",
      "Epoch 1 Batch 250 Loss 8.5136 Accuracy 0.0522\n",
      "Epoch 1 Batch 300 Loss 8.3356 Accuracy 0.0615\n",
      "Epoch 1 Batch 350 Loss 8.1465 Accuracy 0.0698\n",
      "Epoch 1 Batch 400 Loss 7.9639 Accuracy 0.0770\n",
      "Epoch 1 Batch 450 Loss 7.7970 Accuracy 0.0824\n",
      "Epoch 1 Batch 500 Loss 7.6496 Accuracy 0.0879\n",
      "Epoch 1 Batch 550 Loss 7.5130 Accuracy 0.0947\n",
      "Epoch 1 Batch 600 Loss 7.3825 Accuracy 0.1022\n",
      "Epoch 1 Batch 650 Loss 7.2584 Accuracy 0.1091\n",
      "Epoch 1 Batch 700 Loss 7.1415 Accuracy 0.1157\n",
      "Epoch 1 Batch 750 Loss 7.0298 Accuracy 0.1226\n",
      "Epoch 1 Batch 800 Loss 6.9265 Accuracy 0.1290\n",
      "Epoch 1 Batch 850 Loss 6.8286 Accuracy 0.1351\n",
      "Epoch 1 Batch 900 Loss 6.7385 Accuracy 0.1408\n",
      "Epoch 1 Batch 950 Loss 6.6517 Accuracy 0.1464\n",
      "Epoch 1 Batch 1000 Loss 6.5730 Accuracy 0.1517\n",
      "Epoch 1 Batch 1050 Loss 6.4984 Accuracy 0.1568\n",
      "Epoch 1 Batch 1100 Loss 6.4275 Accuracy 0.1617\n",
      "Epoch 1 Batch 1150 Loss 6.3622 Accuracy 0.1663\n",
      "Epoch 1 Batch 1200 Loss 6.3002 Accuracy 0.1707\n",
      "Epoch 1 Batch 1250 Loss 6.2418 Accuracy 0.1749\n",
      "Epoch 1 Batch 1300 Loss 6.1872 Accuracy 0.1789\n",
      "Epoch 1 Batch 1350 Loss 6.1352 Accuracy 0.1826\n",
      "Epoch 1 Batch 1400 Loss 6.0863 Accuracy 0.1862\n",
      "Epoch 1 Batch 1450 Loss 6.0410 Accuracy 0.1894\n",
      "Epoch 1 Batch 1500 Loss 5.9961 Accuracy 0.1928\n",
      "Epoch 1 Batch 1550 Loss 5.9532 Accuracy 0.1959\n",
      "Epoch 1 Batch 1600 Loss 5.9124 Accuracy 0.1989\n",
      "Epoch 1 Batch 1650 Loss 5.8741 Accuracy 0.2017\n",
      "Epoch 1 Batch 1700 Loss 5.8360 Accuracy 0.2045\n",
      "Epoch 1 Batch 1750 Loss 5.8000 Accuracy 0.2073\n",
      "Epoch 1 Batch 1800 Loss 5.7657 Accuracy 0.2098\n",
      "Epoch 1 Batch 1850 Loss 5.7331 Accuracy 0.2123\n",
      "Epoch 1 Batch 1900 Loss 5.7010 Accuracy 0.2147\n",
      "Epoch 1 Batch 1950 Loss 5.6701 Accuracy 0.2169\n",
      "Epoch 1 Batch 2000 Loss 5.6406 Accuracy 0.2191\n",
      "Epoch 1 Batch 2050 Loss 5.6122 Accuracy 0.2212\n",
      "Epoch 1 Batch 2100 Loss 5.5849 Accuracy 0.2232\n",
      "Epoch 1 Batch 2150 Loss 5.5581 Accuracy 0.2253\n",
      "Epoch 1 Batch 2200 Loss 5.5320 Accuracy 0.2272\n",
      "Epoch 1 Batch 2250 Loss 5.5068 Accuracy 0.2291\n",
      "Epoch 1 Batch 2300 Loss 5.4820 Accuracy 0.2310\n",
      "Epoch 1 Batch 2350 Loss 5.4581 Accuracy 0.2328\n",
      "Epoch 1 Batch 2400 Loss 5.4347 Accuracy 0.2346\n",
      "Epoch 1 Batch 2450 Loss 5.4119 Accuracy 0.2363\n",
      "Epoch 1 Batch 2500 Loss 5.3901 Accuracy 0.2381\n",
      "Epoch 1 Batch 2550 Loss 5.3685 Accuracy 0.2397\n",
      "Epoch 1 Batch 2600 Loss 5.3476 Accuracy 0.2413\n",
      "Epoch 1 Loss 5.3374 Accuracy 0.2421\n",
      "Time taken for 1 epoch: 489.45995903015137 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.1967 Accuracy 0.3298\n",
      "Epoch 2 Batch 50 Loss 4.2127 Accuracy 0.3308\n",
      "Epoch 2 Batch 100 Loss 4.2281 Accuracy 0.3287\n",
      "Epoch 2 Batch 150 Loss 4.2151 Accuracy 0.3295\n",
      "Epoch 2 Batch 200 Loss 4.2031 Accuracy 0.3309\n",
      "Epoch 2 Batch 250 Loss 4.1943 Accuracy 0.3321\n",
      "Epoch 2 Batch 300 Loss 4.1840 Accuracy 0.3331\n",
      "Epoch 2 Batch 350 Loss 4.1738 Accuracy 0.3340\n",
      "Epoch 2 Batch 400 Loss 4.1672 Accuracy 0.3347\n",
      "Epoch 2 Batch 450 Loss 4.1581 Accuracy 0.3359\n",
      "Epoch 2 Batch 500 Loss 4.1510 Accuracy 0.3365\n",
      "Epoch 2 Batch 550 Loss 4.1418 Accuracy 0.3376\n",
      "Epoch 2 Batch 600 Loss 4.1337 Accuracy 0.3387\n",
      "Epoch 2 Batch 650 Loss 4.1268 Accuracy 0.3394\n",
      "Epoch 2 Batch 700 Loss 4.1189 Accuracy 0.3403\n",
      "Epoch 2 Batch 750 Loss 4.1099 Accuracy 0.3411\n",
      "Epoch 2 Batch 800 Loss 4.0998 Accuracy 0.3420\n",
      "Epoch 2 Batch 850 Loss 4.0884 Accuracy 0.3432\n",
      "Epoch 2 Batch 900 Loss 4.0795 Accuracy 0.3442\n",
      "Epoch 2 Batch 950 Loss 4.0711 Accuracy 0.3451\n",
      "Epoch 2 Batch 1000 Loss 4.0643 Accuracy 0.3458\n",
      "Epoch 2 Batch 1050 Loss 4.0551 Accuracy 0.3468\n",
      "Epoch 2 Batch 1100 Loss 4.0459 Accuracy 0.3477\n",
      "Epoch 2 Batch 1150 Loss 4.0378 Accuracy 0.3487\n",
      "Epoch 2 Batch 1200 Loss 4.0303 Accuracy 0.3495\n",
      "Epoch 2 Batch 1250 Loss 4.0229 Accuracy 0.3503\n",
      "Epoch 2 Batch 1300 Loss 4.0156 Accuracy 0.3510\n",
      "Epoch 2 Batch 1350 Loss 4.0077 Accuracy 0.3519\n",
      "Epoch 2 Batch 1400 Loss 4.0002 Accuracy 0.3527\n",
      "Epoch 2 Batch 1450 Loss 3.9929 Accuracy 0.3536\n",
      "Epoch 2 Batch 1500 Loss 3.9853 Accuracy 0.3544\n",
      "Epoch 2 Batch 1550 Loss 3.9771 Accuracy 0.3554\n",
      "Epoch 2 Batch 1600 Loss 3.9699 Accuracy 0.3561\n",
      "Epoch 2 Batch 1650 Loss 3.9622 Accuracy 0.3570\n",
      "Epoch 2 Batch 1700 Loss 3.9549 Accuracy 0.3578\n",
      "Epoch 2 Batch 1750 Loss 3.9477 Accuracy 0.3586\n",
      "Epoch 2 Batch 1800 Loss 3.9401 Accuracy 0.3595\n",
      "Epoch 2 Batch 1850 Loss 3.9336 Accuracy 0.3602\n",
      "Epoch 2 Batch 1900 Loss 3.9260 Accuracy 0.3610\n",
      "Epoch 2 Batch 1950 Loss 3.9189 Accuracy 0.3618\n",
      "Epoch 2 Batch 2000 Loss 3.9109 Accuracy 0.3627\n",
      "Epoch 2 Batch 2050 Loss 3.9028 Accuracy 0.3636\n",
      "Epoch 2 Batch 2100 Loss 3.8951 Accuracy 0.3644\n",
      "Epoch 2 Batch 2150 Loss 3.8880 Accuracy 0.3651\n",
      "Epoch 2 Batch 2200 Loss 3.8808 Accuracy 0.3659\n",
      "Epoch 2 Batch 2250 Loss 3.8732 Accuracy 0.3666\n",
      "Epoch 2 Batch 2300 Loss 3.8653 Accuracy 0.3674\n",
      "Epoch 2 Batch 2350 Loss 3.8583 Accuracy 0.3681\n",
      "Epoch 2 Batch 2400 Loss 3.8506 Accuracy 0.3689\n",
      "Epoch 2 Batch 2450 Loss 3.8428 Accuracy 0.3698\n",
      "Epoch 2 Batch 2500 Loss 3.8353 Accuracy 0.3706\n",
      "Epoch 2 Batch 2550 Loss 3.8280 Accuracy 0.3713\n",
      "Epoch 2 Batch 2600 Loss 3.8210 Accuracy 0.3721\n",
      "Epoch 2 Loss 3.8178 Accuracy 0.3724\n",
      "Time taken for 1 epoch: 180.14283180236816 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.3505 Accuracy 0.4245\n",
      "Epoch 3 Batch 50 Loss 3.3919 Accuracy 0.4201\n",
      "Epoch 3 Batch 100 Loss 3.3826 Accuracy 0.4202\n",
      "Epoch 3 Batch 150 Loss 3.3866 Accuracy 0.4186\n",
      "Epoch 3 Batch 200 Loss 3.3748 Accuracy 0.4196\n",
      "Epoch 3 Batch 250 Loss 3.3729 Accuracy 0.4196\n",
      "Epoch 3 Batch 300 Loss 3.3731 Accuracy 0.4196\n",
      "Epoch 3 Batch 350 Loss 3.3668 Accuracy 0.4202\n",
      "Epoch 3 Batch 400 Loss 3.3587 Accuracy 0.4211\n",
      "Epoch 3 Batch 450 Loss 3.3553 Accuracy 0.4215\n",
      "Epoch 3 Batch 500 Loss 3.3513 Accuracy 0.4219\n",
      "Epoch 3 Batch 550 Loss 3.3491 Accuracy 0.4220\n",
      "Epoch 3 Batch 600 Loss 3.3412 Accuracy 0.4226\n",
      "Epoch 3 Batch 650 Loss 3.3375 Accuracy 0.4231\n",
      "Epoch 3 Batch 700 Loss 3.3318 Accuracy 0.4238\n",
      "Epoch 3 Batch 750 Loss 3.3257 Accuracy 0.4244\n",
      "Epoch 3 Batch 800 Loss 3.3212 Accuracy 0.4250\n",
      "Epoch 3 Batch 850 Loss 3.3162 Accuracy 0.4255\n",
      "Epoch 3 Batch 900 Loss 3.3113 Accuracy 0.4260\n",
      "Epoch 3 Batch 950 Loss 3.3072 Accuracy 0.4265\n",
      "Epoch 3 Batch 1000 Loss 3.3007 Accuracy 0.4273\n",
      "Epoch 3 Batch 1050 Loss 3.2945 Accuracy 0.4280\n",
      "Epoch 3 Batch 1100 Loss 3.2895 Accuracy 0.4286\n",
      "Epoch 3 Batch 1150 Loss 3.2843 Accuracy 0.4291\n",
      "Epoch 3 Batch 1200 Loss 3.2794 Accuracy 0.4296\n",
      "Epoch 3 Batch 1250 Loss 3.2733 Accuracy 0.4303\n",
      "Epoch 3 Batch 1300 Loss 3.2680 Accuracy 0.4309\n",
      "Epoch 3 Batch 1350 Loss 3.2624 Accuracy 0.4315\n",
      "Epoch 3 Batch 1400 Loss 3.2573 Accuracy 0.4321\n",
      "Epoch 3 Batch 1450 Loss 3.2523 Accuracy 0.4327\n",
      "Epoch 3 Batch 1500 Loss 3.2471 Accuracy 0.4333\n",
      "Epoch 3 Batch 1550 Loss 3.2427 Accuracy 0.4339\n",
      "Epoch 3 Batch 1600 Loss 3.2372 Accuracy 0.4345\n",
      "Epoch 3 Batch 1650 Loss 3.2318 Accuracy 0.4351\n",
      "Epoch 3 Batch 1700 Loss 3.2277 Accuracy 0.4355\n",
      "Epoch 3 Batch 1750 Loss 3.2228 Accuracy 0.4360\n",
      "Epoch 3 Batch 1800 Loss 3.2172 Accuracy 0.4366\n",
      "Epoch 3 Batch 1850 Loss 3.2129 Accuracy 0.4371\n",
      "Epoch 3 Batch 1900 Loss 3.2091 Accuracy 0.4375\n",
      "Epoch 3 Batch 1950 Loss 3.2045 Accuracy 0.4380\n",
      "Epoch 3 Batch 2000 Loss 3.2004 Accuracy 0.4385\n",
      "Epoch 3 Batch 2050 Loss 3.1966 Accuracy 0.4389\n",
      "Epoch 3 Batch 2100 Loss 3.1921 Accuracy 0.4394\n",
      "Epoch 3 Batch 2150 Loss 3.1876 Accuracy 0.4399\n",
      "Epoch 3 Batch 2200 Loss 3.1838 Accuracy 0.4403\n",
      "Epoch 3 Batch 2250 Loss 3.1796 Accuracy 0.4408\n",
      "Epoch 3 Batch 2300 Loss 3.1754 Accuracy 0.4413\n",
      "Epoch 3 Batch 2350 Loss 3.1713 Accuracy 0.4418\n",
      "Epoch 3 Batch 2400 Loss 3.1673 Accuracy 0.4422\n",
      "Epoch 3 Batch 2450 Loss 3.1636 Accuracy 0.4427\n",
      "Epoch 3 Batch 2500 Loss 3.1599 Accuracy 0.4431\n",
      "Epoch 3 Batch 2550 Loss 3.1559 Accuracy 0.4436\n",
      "Epoch 3 Batch 2600 Loss 3.1520 Accuracy 0.4440\n",
      "Epoch 3 Loss 3.1501 Accuracy 0.4442\n",
      "Time taken for 1 epoch: 174.2656180858612 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.8550 Accuracy 0.4804\n",
      "Epoch 4 Batch 50 Loss 2.9187 Accuracy 0.4685\n",
      "Epoch 4 Batch 100 Loss 2.9207 Accuracy 0.4682\n",
      "Epoch 4 Batch 150 Loss 2.8963 Accuracy 0.4711\n",
      "Epoch 4 Batch 200 Loss 2.8992 Accuracy 0.4710\n",
      "Epoch 4 Batch 250 Loss 2.8999 Accuracy 0.4714\n",
      "Epoch 4 Batch 300 Loss 2.9022 Accuracy 0.4712\n",
      "Epoch 4 Batch 350 Loss 2.8994 Accuracy 0.4716\n",
      "Epoch 4 Batch 400 Loss 2.8986 Accuracy 0.4719\n",
      "Epoch 4 Batch 450 Loss 2.8938 Accuracy 0.4725\n",
      "Epoch 4 Batch 500 Loss 2.8887 Accuracy 0.4730\n",
      "Epoch 4 Batch 550 Loss 2.8877 Accuracy 0.4733\n",
      "Epoch 4 Batch 600 Loss 2.8870 Accuracy 0.4735\n",
      "Epoch 4 Batch 650 Loss 2.8831 Accuracy 0.4738\n",
      "Epoch 4 Batch 700 Loss 2.8825 Accuracy 0.4740\n",
      "Epoch 4 Batch 750 Loss 2.8799 Accuracy 0.4746\n",
      "Epoch 4 Batch 800 Loss 2.8793 Accuracy 0.4744\n",
      "Epoch 4 Batch 850 Loss 2.8760 Accuracy 0.4748\n",
      "Epoch 4 Batch 900 Loss 2.8727 Accuracy 0.4752\n",
      "Epoch 4 Batch 950 Loss 2.8690 Accuracy 0.4757\n",
      "Epoch 4 Batch 1000 Loss 2.8656 Accuracy 0.4763\n",
      "Epoch 4 Batch 1050 Loss 2.8634 Accuracy 0.4764\n",
      "Epoch 4 Batch 1100 Loss 2.8600 Accuracy 0.4769\n",
      "Epoch 4 Batch 1150 Loss 2.8590 Accuracy 0.4770\n",
      "Epoch 4 Batch 1200 Loss 2.8566 Accuracy 0.4773\n",
      "Epoch 4 Batch 1250 Loss 2.8550 Accuracy 0.4774\n",
      "Epoch 4 Batch 1300 Loss 2.8531 Accuracy 0.4776\n",
      "Epoch 4 Batch 1350 Loss 2.8510 Accuracy 0.4779\n",
      "Epoch 4 Batch 1400 Loss 2.8484 Accuracy 0.4783\n",
      "Epoch 4 Batch 1450 Loss 2.8454 Accuracy 0.4787\n",
      "Epoch 4 Batch 1500 Loss 2.8436 Accuracy 0.4788\n",
      "Epoch 4 Batch 1550 Loss 2.8417 Accuracy 0.4790\n",
      "Epoch 4 Batch 1600 Loss 2.8397 Accuracy 0.4793\n",
      "Epoch 4 Batch 1650 Loss 2.8382 Accuracy 0.4795\n",
      "Epoch 4 Batch 1700 Loss 2.8352 Accuracy 0.4800\n",
      "Epoch 4 Batch 1750 Loss 2.8333 Accuracy 0.4801\n",
      "Epoch 4 Batch 1800 Loss 2.8319 Accuracy 0.4803\n",
      "Epoch 4 Batch 1850 Loss 2.8300 Accuracy 0.4806\n",
      "Epoch 4 Batch 1900 Loss 2.8282 Accuracy 0.4808\n",
      "Epoch 4 Batch 1950 Loss 2.8258 Accuracy 0.4810\n",
      "Epoch 4 Batch 2000 Loss 2.8241 Accuracy 0.4812\n",
      "Epoch 4 Batch 2050 Loss 2.8217 Accuracy 0.4816\n",
      "Epoch 4 Batch 2100 Loss 2.8200 Accuracy 0.4817\n",
      "Epoch 4 Batch 2150 Loss 2.8179 Accuracy 0.4819\n",
      "Epoch 4 Batch 2200 Loss 2.8155 Accuracy 0.4822\n",
      "Epoch 4 Batch 2250 Loss 2.8139 Accuracy 0.4824\n",
      "Epoch 4 Batch 2300 Loss 2.8121 Accuracy 0.4826\n",
      "Epoch 4 Batch 2350 Loss 2.8097 Accuracy 0.4830\n",
      "Epoch 4 Batch 2400 Loss 2.8074 Accuracy 0.4832\n",
      "Epoch 4 Batch 2450 Loss 2.8051 Accuracy 0.4835\n",
      "Epoch 4 Batch 2500 Loss 2.8032 Accuracy 0.4836\n",
      "Epoch 4 Batch 2550 Loss 2.8014 Accuracy 0.4838\n",
      "Epoch 4 Batch 2600 Loss 2.7997 Accuracy 0.4840\n",
      "Epoch 4 Loss 2.7993 Accuracy 0.4840\n",
      "Time taken for 1 epoch: 176.53158235549927 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.7795 Accuracy 0.4883\n",
      "Epoch 5 Batch 50 Loss 2.6891 Accuracy 0.4986\n",
      "Epoch 5 Batch 100 Loss 2.6787 Accuracy 0.4981\n",
      "Epoch 5 Batch 150 Loss 2.6741 Accuracy 0.4988\n",
      "Epoch 5 Batch 200 Loss 2.6692 Accuracy 0.4996\n",
      "Epoch 5 Batch 250 Loss 2.6726 Accuracy 0.4987\n",
      "Epoch 5 Batch 300 Loss 2.6697 Accuracy 0.4994\n",
      "Epoch 5 Batch 350 Loss 2.6641 Accuracy 0.5001\n",
      "Epoch 5 Batch 400 Loss 2.6633 Accuracy 0.5003\n",
      "Epoch 5 Batch 450 Loss 2.6627 Accuracy 0.5001\n",
      "Epoch 5 Batch 500 Loss 2.6629 Accuracy 0.5000\n",
      "Epoch 5 Batch 550 Loss 2.6630 Accuracy 0.5001\n",
      "Epoch 5 Batch 600 Loss 2.6601 Accuracy 0.5002\n",
      "Epoch 5 Batch 650 Loss 2.6599 Accuracy 0.5001\n",
      "Epoch 5 Batch 700 Loss 2.6576 Accuracy 0.5004\n",
      "Epoch 5 Batch 750 Loss 2.6560 Accuracy 0.5006\n",
      "Epoch 5 Batch 800 Loss 2.6542 Accuracy 0.5009\n",
      "Epoch 5 Batch 850 Loss 2.6529 Accuracy 0.5010\n",
      "Epoch 5 Batch 900 Loss 2.6498 Accuracy 0.5014\n",
      "Epoch 5 Batch 950 Loss 2.6498 Accuracy 0.5016\n",
      "Epoch 5 Batch 1000 Loss 2.6474 Accuracy 0.5019\n",
      "Epoch 5 Batch 1050 Loss 2.6465 Accuracy 0.5020\n",
      "Epoch 5 Batch 1100 Loss 2.6441 Accuracy 0.5023\n",
      "Epoch 5 Batch 1150 Loss 2.6423 Accuracy 0.5026\n",
      "Epoch 5 Batch 1200 Loss 2.6417 Accuracy 0.5026\n",
      "Epoch 5 Batch 1250 Loss 2.6399 Accuracy 0.5029\n",
      "Epoch 5 Batch 1300 Loss 2.6396 Accuracy 0.5029\n",
      "Epoch 5 Batch 1350 Loss 2.6382 Accuracy 0.5031\n",
      "Epoch 5 Batch 1400 Loss 2.6375 Accuracy 0.5031\n",
      "Epoch 5 Batch 1450 Loss 2.6354 Accuracy 0.5035\n",
      "Epoch 5 Batch 1500 Loss 2.6341 Accuracy 0.5037\n",
      "Epoch 5 Batch 1550 Loss 2.6325 Accuracy 0.5040\n",
      "Epoch 5 Batch 1600 Loss 2.6306 Accuracy 0.5043\n",
      "Epoch 5 Batch 1650 Loss 2.6302 Accuracy 0.5043\n",
      "Epoch 5 Batch 1700 Loss 2.6283 Accuracy 0.5046\n",
      "Epoch 5 Batch 1750 Loss 2.6276 Accuracy 0.5047\n",
      "Epoch 5 Batch 1800 Loss 2.6261 Accuracy 0.5048\n",
      "Epoch 5 Batch 1850 Loss 2.6259 Accuracy 0.5049\n",
      "Epoch 5 Batch 1900 Loss 2.6239 Accuracy 0.5051\n",
      "Epoch 5 Batch 1950 Loss 2.6228 Accuracy 0.5053\n",
      "Epoch 5 Batch 2000 Loss 2.6223 Accuracy 0.5054\n",
      "Epoch 5 Batch 2050 Loss 2.6214 Accuracy 0.5054\n",
      "Epoch 5 Batch 2100 Loss 2.6204 Accuracy 0.5056\n",
      "Epoch 5 Batch 2150 Loss 2.6191 Accuracy 0.5058\n",
      "Epoch 5 Batch 2200 Loss 2.6178 Accuracy 0.5059\n",
      "Epoch 5 Batch 2250 Loss 2.6170 Accuracy 0.5060\n",
      "Epoch 5 Batch 2300 Loss 2.6157 Accuracy 0.5061\n",
      "Epoch 5 Batch 2350 Loss 2.6147 Accuracy 0.5062\n",
      "Epoch 5 Batch 2400 Loss 2.6136 Accuracy 0.5064\n",
      "Epoch 5 Batch 2450 Loss 2.6120 Accuracy 0.5066\n",
      "Epoch 5 Batch 2500 Loss 2.6108 Accuracy 0.5067\n",
      "Epoch 5 Batch 2550 Loss 2.6101 Accuracy 0.5068\n",
      "Epoch 5 Batch 2600 Loss 2.6088 Accuracy 0.5070\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 2.6083 Accuracy 0.5070\n",
      "Time taken for 1 epoch: 175.4071033000946 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.5599 Accuracy 0.4979\n",
      "Epoch 6 Batch 50 Loss 2.5274 Accuracy 0.5153\n",
      "Epoch 6 Batch 100 Loss 2.5249 Accuracy 0.5157\n",
      "Epoch 6 Batch 150 Loss 2.5165 Accuracy 0.5171\n",
      "Epoch 6 Batch 200 Loss 2.5188 Accuracy 0.5177\n",
      "Epoch 6 Batch 250 Loss 2.5205 Accuracy 0.5173\n",
      "Epoch 6 Batch 300 Loss 2.5181 Accuracy 0.5177\n",
      "Epoch 6 Batch 350 Loss 2.5165 Accuracy 0.5178\n",
      "Epoch 6 Batch 400 Loss 2.5167 Accuracy 0.5178\n",
      "Epoch 6 Batch 450 Loss 2.5133 Accuracy 0.5181\n",
      "Epoch 6 Batch 500 Loss 2.5151 Accuracy 0.5179\n",
      "Epoch 6 Batch 550 Loss 2.5172 Accuracy 0.5176\n",
      "Epoch 6 Batch 600 Loss 2.5150 Accuracy 0.5178\n",
      "Epoch 6 Batch 650 Loss 2.5141 Accuracy 0.5181\n",
      "Epoch 6 Batch 700 Loss 2.5130 Accuracy 0.5184\n",
      "Epoch 6 Batch 750 Loss 2.5127 Accuracy 0.5183\n",
      "Epoch 6 Batch 800 Loss 2.5118 Accuracy 0.5185\n",
      "Epoch 6 Batch 850 Loss 2.5125 Accuracy 0.5185\n",
      "Epoch 6 Batch 900 Loss 2.5133 Accuracy 0.5183\n",
      "Epoch 6 Batch 950 Loss 2.5116 Accuracy 0.5186\n",
      "Epoch 6 Batch 1000 Loss 2.5105 Accuracy 0.5186\n",
      "Epoch 6 Batch 1050 Loss 2.5092 Accuracy 0.5189\n",
      "Epoch 6 Batch 1100 Loss 2.5078 Accuracy 0.5190\n",
      "Epoch 6 Batch 1150 Loss 2.5058 Accuracy 0.5194\n",
      "Epoch 6 Batch 1200 Loss 2.5047 Accuracy 0.5195\n",
      "Epoch 6 Batch 1250 Loss 2.5042 Accuracy 0.5196\n",
      "Epoch 6 Batch 1300 Loss 2.5023 Accuracy 0.5198\n",
      "Epoch 6 Batch 1350 Loss 2.5013 Accuracy 0.5199\n",
      "Epoch 6 Batch 1400 Loss 2.5011 Accuracy 0.5199\n",
      "Epoch 6 Batch 1450 Loss 2.5006 Accuracy 0.5201\n",
      "Epoch 6 Batch 1500 Loss 2.4986 Accuracy 0.5204\n",
      "Epoch 6 Batch 1550 Loss 2.4974 Accuracy 0.5206\n",
      "Epoch 6 Batch 1600 Loss 2.4969 Accuracy 0.5208\n",
      "Epoch 6 Batch 1650 Loss 2.4964 Accuracy 0.5208\n",
      "Epoch 6 Batch 1700 Loss 2.4958 Accuracy 0.5209\n",
      "Epoch 6 Batch 1750 Loss 2.4946 Accuracy 0.5211\n",
      "Epoch 6 Batch 1800 Loss 2.4938 Accuracy 0.5212\n",
      "Epoch 6 Batch 1850 Loss 2.4937 Accuracy 0.5212\n",
      "Epoch 6 Batch 1900 Loss 2.4938 Accuracy 0.5212\n",
      "Epoch 6 Batch 1950 Loss 2.4931 Accuracy 0.5212\n",
      "Epoch 6 Batch 2000 Loss 2.4922 Accuracy 0.5213\n",
      "Epoch 6 Batch 2050 Loss 2.4908 Accuracy 0.5215\n",
      "Epoch 6 Batch 2100 Loss 2.4902 Accuracy 0.5215\n",
      "Epoch 6 Batch 2150 Loss 2.4894 Accuracy 0.5217\n",
      "Epoch 6 Batch 2200 Loss 2.4889 Accuracy 0.5217\n",
      "Epoch 6 Batch 2250 Loss 2.4883 Accuracy 0.5217\n",
      "Epoch 6 Batch 2300 Loss 2.4879 Accuracy 0.5218\n",
      "Epoch 6 Batch 2350 Loss 2.4872 Accuracy 0.5218\n",
      "Epoch 6 Batch 2400 Loss 2.4863 Accuracy 0.5220\n",
      "Epoch 6 Batch 2450 Loss 2.4854 Accuracy 0.5221\n",
      "Epoch 6 Batch 2500 Loss 2.4846 Accuracy 0.5222\n",
      "Epoch 6 Batch 2550 Loss 2.4839 Accuracy 0.5223\n",
      "Epoch 6 Batch 2600 Loss 2.4835 Accuracy 0.5224\n",
      "Epoch 6 Loss 2.4831 Accuracy 0.5224\n",
      "Time taken for 1 epoch: 174.83512091636658 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.2713 Accuracy 0.5660\n",
      "Epoch 7 Batch 50 Loss 2.4116 Accuracy 0.5316\n",
      "Epoch 7 Batch 100 Loss 2.4196 Accuracy 0.5299\n",
      "Epoch 7 Batch 150 Loss 2.4221 Accuracy 0.5293\n",
      "Epoch 7 Batch 200 Loss 2.4211 Accuracy 0.5296\n",
      "Epoch 7 Batch 250 Loss 2.4195 Accuracy 0.5297\n",
      "Epoch 7 Batch 300 Loss 2.4169 Accuracy 0.5301\n",
      "Epoch 7 Batch 350 Loss 2.4174 Accuracy 0.5300\n",
      "Epoch 7 Batch 400 Loss 2.4210 Accuracy 0.5296\n",
      "Epoch 7 Batch 450 Loss 2.4178 Accuracy 0.5297\n",
      "Epoch 7 Batch 500 Loss 2.4201 Accuracy 0.5295\n",
      "Epoch 7 Batch 550 Loss 2.4204 Accuracy 0.5295\n",
      "Epoch 7 Batch 600 Loss 2.4201 Accuracy 0.5296\n",
      "Epoch 7 Batch 650 Loss 2.4185 Accuracy 0.5300\n",
      "Epoch 7 Batch 700 Loss 2.4165 Accuracy 0.5301\n",
      "Epoch 7 Batch 750 Loss 2.4148 Accuracy 0.5302\n",
      "Epoch 7 Batch 800 Loss 2.4117 Accuracy 0.5307\n",
      "Epoch 7 Batch 850 Loss 2.4108 Accuracy 0.5309\n",
      "Epoch 7 Batch 900 Loss 2.4109 Accuracy 0.5310\n",
      "Epoch 7 Batch 950 Loss 2.4090 Accuracy 0.5312\n",
      "Epoch 7 Batch 1000 Loss 2.4090 Accuracy 0.5312\n",
      "Epoch 7 Batch 1050 Loss 2.4086 Accuracy 0.5313\n",
      "Epoch 7 Batch 1100 Loss 2.4081 Accuracy 0.5314\n",
      "Epoch 7 Batch 1150 Loss 2.4076 Accuracy 0.5315\n",
      "Epoch 7 Batch 1200 Loss 2.4064 Accuracy 0.5317\n",
      "Epoch 7 Batch 1250 Loss 2.4061 Accuracy 0.5318\n",
      "Epoch 7 Batch 1300 Loss 2.4043 Accuracy 0.5320\n",
      "Epoch 7 Batch 1350 Loss 2.4042 Accuracy 0.5320\n",
      "Epoch 7 Batch 1400 Loss 2.4031 Accuracy 0.5323\n",
      "Epoch 7 Batch 1450 Loss 2.4024 Accuracy 0.5324\n",
      "Epoch 7 Batch 1500 Loss 2.4012 Accuracy 0.5325\n",
      "Epoch 7 Batch 1550 Loss 2.4008 Accuracy 0.5325\n",
      "Epoch 7 Batch 1600 Loss 2.4011 Accuracy 0.5325\n",
      "Epoch 7 Batch 1650 Loss 2.4001 Accuracy 0.5327\n",
      "Epoch 7 Batch 1700 Loss 2.3999 Accuracy 0.5328\n",
      "Epoch 7 Batch 1750 Loss 2.4002 Accuracy 0.5327\n",
      "Epoch 7 Batch 1800 Loss 2.4000 Accuracy 0.5328\n",
      "Epoch 7 Batch 1850 Loss 2.3995 Accuracy 0.5328\n",
      "Epoch 7 Batch 1900 Loss 2.3993 Accuracy 0.5329\n",
      "Epoch 7 Batch 1950 Loss 2.3992 Accuracy 0.5330\n",
      "Epoch 7 Batch 2000 Loss 2.3986 Accuracy 0.5330\n",
      "Epoch 7 Batch 2050 Loss 2.3981 Accuracy 0.5331\n",
      "Epoch 7 Batch 2100 Loss 2.3972 Accuracy 0.5332\n",
      "Epoch 7 Batch 2150 Loss 2.3965 Accuracy 0.5333\n",
      "Epoch 7 Batch 2200 Loss 2.3965 Accuracy 0.5332\n",
      "Epoch 7 Batch 2250 Loss 2.3963 Accuracy 0.5333\n",
      "Epoch 7 Batch 2300 Loss 2.3954 Accuracy 0.5334\n",
      "Epoch 7 Batch 2350 Loss 2.3949 Accuracy 0.5335\n",
      "Epoch 7 Batch 2400 Loss 2.3944 Accuracy 0.5336\n",
      "Epoch 7 Batch 2450 Loss 2.3942 Accuracy 0.5336\n",
      "Epoch 7 Batch 2500 Loss 2.3935 Accuracy 0.5337\n",
      "Epoch 7 Batch 2550 Loss 2.3930 Accuracy 0.5338\n",
      "Epoch 7 Batch 2600 Loss 2.3926 Accuracy 0.5338\n",
      "Epoch 7 Loss 2.3924 Accuracy 0.5338\n",
      "Time taken for 1 epoch: 178.27588486671448 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.5685 Accuracy 0.5090\n",
      "Epoch 8 Batch 50 Loss 2.3347 Accuracy 0.5396\n",
      "Epoch 8 Batch 100 Loss 2.3354 Accuracy 0.5405\n",
      "Epoch 8 Batch 150 Loss 2.3371 Accuracy 0.5412\n",
      "Epoch 8 Batch 200 Loss 2.3337 Accuracy 0.5407\n",
      "Epoch 8 Batch 250 Loss 2.3345 Accuracy 0.5403\n",
      "Epoch 8 Batch 300 Loss 2.3324 Accuracy 0.5405\n",
      "Epoch 8 Batch 350 Loss 2.3350 Accuracy 0.5401\n",
      "Epoch 8 Batch 400 Loss 2.3400 Accuracy 0.5394\n",
      "Epoch 8 Batch 450 Loss 2.3392 Accuracy 0.5393\n",
      "Epoch 8 Batch 500 Loss 2.3395 Accuracy 0.5391\n",
      "Epoch 8 Batch 550 Loss 2.3395 Accuracy 0.5392\n",
      "Epoch 8 Batch 600 Loss 2.3369 Accuracy 0.5397\n",
      "Epoch 8 Batch 650 Loss 2.3353 Accuracy 0.5399\n",
      "Epoch 8 Batch 700 Loss 2.3372 Accuracy 0.5400\n",
      "Epoch 8 Batch 750 Loss 2.3360 Accuracy 0.5402\n",
      "Epoch 8 Batch 800 Loss 2.3363 Accuracy 0.5401\n",
      "Epoch 8 Batch 850 Loss 2.3381 Accuracy 0.5399\n",
      "Epoch 8 Batch 900 Loss 2.3388 Accuracy 0.5400\n",
      "Epoch 8 Batch 950 Loss 2.3386 Accuracy 0.5402\n",
      "Epoch 8 Batch 1000 Loss 2.3372 Accuracy 0.5404\n",
      "Epoch 8 Batch 1050 Loss 2.3371 Accuracy 0.5405\n",
      "Epoch 8 Batch 1100 Loss 2.3366 Accuracy 0.5405\n",
      "Epoch 8 Batch 1150 Loss 2.3363 Accuracy 0.5405\n",
      "Epoch 8 Batch 1200 Loss 2.3369 Accuracy 0.5404\n",
      "Epoch 8 Batch 1250 Loss 2.3350 Accuracy 0.5406\n",
      "Epoch 8 Batch 1300 Loss 2.3342 Accuracy 0.5408\n",
      "Epoch 8 Batch 1350 Loss 2.3343 Accuracy 0.5409\n",
      "Epoch 8 Batch 1400 Loss 2.3344 Accuracy 0.5410\n",
      "Epoch 8 Batch 1450 Loss 2.3339 Accuracy 0.5410\n",
      "Epoch 8 Batch 1500 Loss 2.3332 Accuracy 0.5412\n",
      "Epoch 8 Batch 1550 Loss 2.3323 Accuracy 0.5414\n",
      "Epoch 8 Batch 1600 Loss 2.3324 Accuracy 0.5413\n",
      "Epoch 8 Batch 1650 Loss 2.3321 Accuracy 0.5414\n",
      "Epoch 8 Batch 1700 Loss 2.3315 Accuracy 0.5415\n",
      "Epoch 8 Batch 1750 Loss 2.3308 Accuracy 0.5417\n",
      "Epoch 8 Batch 1800 Loss 2.3301 Accuracy 0.5417\n",
      "Epoch 8 Batch 1850 Loss 2.3296 Accuracy 0.5418\n",
      "Epoch 8 Batch 1900 Loss 2.3292 Accuracy 0.5419\n",
      "Epoch 8 Batch 1950 Loss 2.3287 Accuracy 0.5419\n",
      "Epoch 8 Batch 2000 Loss 2.3287 Accuracy 0.5419\n",
      "Epoch 8 Batch 2050 Loss 2.3285 Accuracy 0.5420\n",
      "Epoch 8 Batch 2100 Loss 2.3280 Accuracy 0.5420\n",
      "Epoch 8 Batch 2150 Loss 2.3281 Accuracy 0.5420\n",
      "Epoch 8 Batch 2200 Loss 2.3282 Accuracy 0.5420\n",
      "Epoch 8 Batch 2250 Loss 2.3274 Accuracy 0.5421\n",
      "Epoch 8 Batch 2300 Loss 2.3274 Accuracy 0.5421\n",
      "Epoch 8 Batch 2350 Loss 2.3273 Accuracy 0.5421\n",
      "Epoch 8 Batch 2400 Loss 2.3266 Accuracy 0.5422\n",
      "Epoch 8 Batch 2450 Loss 2.3256 Accuracy 0.5424\n",
      "Epoch 8 Batch 2500 Loss 2.3251 Accuracy 0.5424\n",
      "Epoch 8 Batch 2550 Loss 2.3245 Accuracy 0.5425\n",
      "Epoch 8 Batch 2600 Loss 2.3242 Accuracy 0.5425\n",
      "Epoch 8 Loss 2.3244 Accuracy 0.5425\n",
      "Time taken for 1 epoch: 183.58753442764282 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.2325 Accuracy 0.5659\n",
      "Epoch 9 Batch 50 Loss 2.2861 Accuracy 0.5487\n",
      "Epoch 9 Batch 100 Loss 2.2864 Accuracy 0.5478\n",
      "Epoch 9 Batch 150 Loss 2.2810 Accuracy 0.5480\n",
      "Epoch 9 Batch 200 Loss 2.2768 Accuracy 0.5483\n",
      "Epoch 9 Batch 250 Loss 2.2763 Accuracy 0.5481\n",
      "Epoch 9 Batch 300 Loss 2.2796 Accuracy 0.5479\n",
      "Epoch 9 Batch 350 Loss 2.2801 Accuracy 0.5481\n",
      "Epoch 9 Batch 400 Loss 2.2813 Accuracy 0.5480\n",
      "Epoch 9 Batch 450 Loss 2.2814 Accuracy 0.5480\n",
      "Epoch 9 Batch 500 Loss 2.2798 Accuracy 0.5483\n",
      "Epoch 9 Batch 550 Loss 2.2794 Accuracy 0.5482\n",
      "Epoch 9 Batch 600 Loss 2.2802 Accuracy 0.5480\n",
      "Epoch 9 Batch 650 Loss 2.2813 Accuracy 0.5479\n",
      "Epoch 9 Batch 700 Loss 2.2809 Accuracy 0.5481\n",
      "Epoch 9 Batch 750 Loss 2.2793 Accuracy 0.5482\n",
      "Epoch 9 Batch 800 Loss 2.2778 Accuracy 0.5484\n",
      "Epoch 9 Batch 850 Loss 2.2784 Accuracy 0.5486\n",
      "Epoch 9 Batch 900 Loss 2.2787 Accuracy 0.5485\n",
      "Epoch 9 Batch 950 Loss 2.2783 Accuracy 0.5486\n",
      "Epoch 9 Batch 1000 Loss 2.2778 Accuracy 0.5487\n",
      "Epoch 9 Batch 1050 Loss 2.2782 Accuracy 0.5487\n",
      "Epoch 9 Batch 1100 Loss 2.2782 Accuracy 0.5488\n",
      "Epoch 9 Batch 1150 Loss 2.2777 Accuracy 0.5489\n",
      "Epoch 9 Batch 1200 Loss 2.2773 Accuracy 0.5490\n",
      "Epoch 9 Batch 1250 Loss 2.2771 Accuracy 0.5491\n",
      "Epoch 9 Batch 1300 Loss 2.2777 Accuracy 0.5491\n",
      "Epoch 9 Batch 1350 Loss 2.2770 Accuracy 0.5492\n",
      "Epoch 9 Batch 1400 Loss 2.2759 Accuracy 0.5494\n",
      "Epoch 9 Batch 1450 Loss 2.2757 Accuracy 0.5493\n",
      "Epoch 9 Batch 1500 Loss 2.2752 Accuracy 0.5494\n",
      "Epoch 9 Batch 1550 Loss 2.2753 Accuracy 0.5494\n",
      "Epoch 9 Batch 1600 Loss 2.2741 Accuracy 0.5497\n",
      "Epoch 9 Batch 1650 Loss 2.2737 Accuracy 0.5497\n",
      "Epoch 9 Batch 1700 Loss 2.2730 Accuracy 0.5497\n",
      "Epoch 9 Batch 1750 Loss 2.2733 Accuracy 0.5497\n",
      "Epoch 9 Batch 1800 Loss 2.2731 Accuracy 0.5498\n",
      "Epoch 9 Batch 1850 Loss 2.2728 Accuracy 0.5498\n",
      "Epoch 9 Batch 1900 Loss 2.2722 Accuracy 0.5499\n",
      "Epoch 9 Batch 1950 Loss 2.2720 Accuracy 0.5499\n",
      "Epoch 9 Batch 2000 Loss 2.2717 Accuracy 0.5499\n",
      "Epoch 9 Batch 2050 Loss 2.2718 Accuracy 0.5499\n",
      "Epoch 9 Batch 2100 Loss 2.2712 Accuracy 0.5500\n",
      "Epoch 9 Batch 2150 Loss 2.2712 Accuracy 0.5500\n",
      "Epoch 9 Batch 2200 Loss 2.2708 Accuracy 0.5500\n",
      "Epoch 9 Batch 2250 Loss 2.2705 Accuracy 0.5501\n",
      "Epoch 9 Batch 2300 Loss 2.2701 Accuracy 0.5502\n",
      "Epoch 9 Batch 2350 Loss 2.2701 Accuracy 0.5502\n",
      "Epoch 9 Batch 2400 Loss 2.2700 Accuracy 0.5502\n",
      "Epoch 9 Batch 2450 Loss 2.2692 Accuracy 0.5503\n",
      "Epoch 9 Batch 2500 Loss 2.2691 Accuracy 0.5503\n",
      "Epoch 9 Batch 2550 Loss 2.2685 Accuracy 0.5504\n",
      "Epoch 9 Batch 2600 Loss 2.2685 Accuracy 0.5504\n",
      "Epoch 9 Loss 2.2682 Accuracy 0.5504\n",
      "Time taken for 1 epoch: 184.84063053131104 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.3009 Accuracy 0.5636\n",
      "Epoch 10 Batch 50 Loss 2.2342 Accuracy 0.5542\n",
      "Epoch 10 Batch 100 Loss 2.2218 Accuracy 0.5542\n",
      "Epoch 10 Batch 150 Loss 2.2260 Accuracy 0.5540\n",
      "Epoch 10 Batch 200 Loss 2.2300 Accuracy 0.5540\n",
      "Epoch 10 Batch 250 Loss 2.2353 Accuracy 0.5533\n",
      "Epoch 10 Batch 300 Loss 2.2359 Accuracy 0.5538\n",
      "Epoch 10 Batch 350 Loss 2.2339 Accuracy 0.5539\n",
      "Epoch 10 Batch 400 Loss 2.2349 Accuracy 0.5535\n",
      "Epoch 10 Batch 450 Loss 2.2373 Accuracy 0.5536\n",
      "Epoch 10 Batch 500 Loss 2.2365 Accuracy 0.5537\n",
      "Epoch 10 Batch 550 Loss 2.2335 Accuracy 0.5541\n",
      "Epoch 10 Batch 600 Loss 2.2341 Accuracy 0.5540\n",
      "Epoch 10 Batch 650 Loss 2.2347 Accuracy 0.5540\n",
      "Epoch 10 Batch 700 Loss 2.2346 Accuracy 0.5541\n",
      "Epoch 10 Batch 750 Loss 2.2338 Accuracy 0.5542\n",
      "Epoch 10 Batch 800 Loss 2.2333 Accuracy 0.5544\n",
      "Epoch 10 Batch 850 Loss 2.2347 Accuracy 0.5543\n",
      "Epoch 10 Batch 900 Loss 2.2333 Accuracy 0.5546\n",
      "Epoch 10 Batch 950 Loss 2.2338 Accuracy 0.5546\n",
      "Epoch 10 Batch 1000 Loss 2.2342 Accuracy 0.5546\n",
      "Epoch 10 Batch 1050 Loss 2.2330 Accuracy 0.5548\n",
      "Epoch 10 Batch 1100 Loss 2.2322 Accuracy 0.5550\n",
      "Epoch 10 Batch 1150 Loss 2.2307 Accuracy 0.5553\n",
      "Epoch 10 Batch 1200 Loss 2.2313 Accuracy 0.5553\n",
      "Epoch 10 Batch 1250 Loss 2.2308 Accuracy 0.5554\n",
      "Epoch 10 Batch 1300 Loss 2.2297 Accuracy 0.5556\n",
      "Epoch 10 Batch 1350 Loss 2.2292 Accuracy 0.5556\n",
      "Epoch 10 Batch 1400 Loss 2.2287 Accuracy 0.5557\n",
      "Epoch 10 Batch 1450 Loss 2.2286 Accuracy 0.5557\n",
      "Epoch 10 Batch 1500 Loss 2.2284 Accuracy 0.5557\n",
      "Epoch 10 Batch 1550 Loss 2.2282 Accuracy 0.5558\n",
      "Epoch 10 Batch 1600 Loss 2.2284 Accuracy 0.5558\n",
      "Epoch 10 Batch 1650 Loss 2.2282 Accuracy 0.5558\n",
      "Epoch 10 Batch 1700 Loss 2.2281 Accuracy 0.5557\n",
      "Epoch 10 Batch 1750 Loss 2.2280 Accuracy 0.5557\n",
      "Epoch 10 Batch 1800 Loss 2.2276 Accuracy 0.5557\n",
      "Epoch 10 Batch 1850 Loss 2.2281 Accuracy 0.5557\n",
      "Epoch 10 Batch 1900 Loss 2.2279 Accuracy 0.5557\n",
      "Epoch 10 Batch 1950 Loss 2.2272 Accuracy 0.5558\n",
      "Epoch 10 Batch 2000 Loss 2.2270 Accuracy 0.5559\n",
      "Epoch 10 Batch 2050 Loss 2.2266 Accuracy 0.5559\n",
      "Epoch 10 Batch 2100 Loss 2.2259 Accuracy 0.5560\n",
      "Epoch 10 Batch 2150 Loss 2.2256 Accuracy 0.5560\n",
      "Epoch 10 Batch 2200 Loss 2.2254 Accuracy 0.5561\n",
      "Epoch 10 Batch 2250 Loss 2.2256 Accuracy 0.5560\n",
      "Epoch 10 Batch 2300 Loss 2.2254 Accuracy 0.5561\n",
      "Epoch 10 Batch 2350 Loss 2.2249 Accuracy 0.5561\n",
      "Epoch 10 Batch 2400 Loss 2.2245 Accuracy 0.5562\n",
      "Epoch 10 Batch 2450 Loss 2.2242 Accuracy 0.5563\n",
      "Epoch 10 Batch 2500 Loss 2.2241 Accuracy 0.5563\n",
      "Epoch 10 Batch 2550 Loss 2.2242 Accuracy 0.5562\n",
      "Epoch 10 Batch 2600 Loss 2.2245 Accuracy 0.5562\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Epoch 10 Loss 2.2243 Accuracy 0.5562\n",
      "Time taken for 1 epoch: 182.687166929245 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.0249 Accuracy 0.5912\n",
      "Epoch 11 Batch 50 Loss 2.1820 Accuracy 0.5625\n",
      "Epoch 11 Batch 100 Loss 2.1871 Accuracy 0.5616\n",
      "Epoch 11 Batch 150 Loss 2.1843 Accuracy 0.5623\n",
      "Epoch 11 Batch 200 Loss 2.1891 Accuracy 0.5613\n",
      "Epoch 11 Batch 250 Loss 2.1902 Accuracy 0.5609\n",
      "Epoch 11 Batch 300 Loss 2.1878 Accuracy 0.5612\n",
      "Epoch 11 Batch 350 Loss 2.1853 Accuracy 0.5617\n",
      "Epoch 11 Batch 400 Loss 2.1876 Accuracy 0.5612\n",
      "Epoch 11 Batch 450 Loss 2.1879 Accuracy 0.5611\n",
      "Epoch 11 Batch 500 Loss 2.1886 Accuracy 0.5610\n",
      "Epoch 11 Batch 550 Loss 2.1882 Accuracy 0.5611\n",
      "Epoch 11 Batch 600 Loss 2.1900 Accuracy 0.5607\n",
      "Epoch 11 Batch 650 Loss 2.1906 Accuracy 0.5606\n",
      "Epoch 11 Batch 700 Loss 2.1900 Accuracy 0.5608\n",
      "Epoch 11 Batch 750 Loss 2.1896 Accuracy 0.5609\n",
      "Epoch 11 Batch 800 Loss 2.1882 Accuracy 0.5611\n",
      "Epoch 11 Batch 850 Loss 2.1892 Accuracy 0.5609\n",
      "Epoch 11 Batch 900 Loss 2.1895 Accuracy 0.5607\n",
      "Epoch 11 Batch 950 Loss 2.1909 Accuracy 0.5607\n",
      "Epoch 11 Batch 1000 Loss 2.1907 Accuracy 0.5607\n",
      "Epoch 11 Batch 1050 Loss 2.1903 Accuracy 0.5608\n",
      "Epoch 11 Batch 1100 Loss 2.1893 Accuracy 0.5609\n",
      "Epoch 11 Batch 1150 Loss 2.1887 Accuracy 0.5611\n",
      "Epoch 11 Batch 1200 Loss 2.1897 Accuracy 0.5609\n",
      "Epoch 11 Batch 1250 Loss 2.1897 Accuracy 0.5609\n",
      "Epoch 11 Batch 1300 Loss 2.1887 Accuracy 0.5611\n",
      "Epoch 11 Batch 1350 Loss 2.1877 Accuracy 0.5612\n",
      "Epoch 11 Batch 1400 Loss 2.1881 Accuracy 0.5612\n",
      "Epoch 11 Batch 1450 Loss 2.1872 Accuracy 0.5614\n",
      "Epoch 11 Batch 1500 Loss 2.1864 Accuracy 0.5616\n",
      "Epoch 11 Batch 1550 Loss 2.1869 Accuracy 0.5615\n",
      "Epoch 11 Batch 1600 Loss 2.1867 Accuracy 0.5615\n",
      "Epoch 11 Batch 1650 Loss 2.1865 Accuracy 0.5615\n",
      "Epoch 11 Batch 1700 Loss 2.1861 Accuracy 0.5616\n",
      "Epoch 11 Batch 1750 Loss 2.1869 Accuracy 0.5614\n",
      "Epoch 11 Batch 1800 Loss 2.1870 Accuracy 0.5614\n",
      "Epoch 11 Batch 1850 Loss 2.1868 Accuracy 0.5614\n",
      "Epoch 11 Batch 1900 Loss 2.1871 Accuracy 0.5614\n",
      "Epoch 11 Batch 1950 Loss 2.1868 Accuracy 0.5614\n",
      "Epoch 11 Batch 2000 Loss 2.1871 Accuracy 0.5614\n",
      "Epoch 11 Batch 2050 Loss 2.1868 Accuracy 0.5614\n",
      "Epoch 11 Batch 2100 Loss 2.1869 Accuracy 0.5614\n",
      "Epoch 11 Batch 2150 Loss 2.1865 Accuracy 0.5615\n",
      "Epoch 11 Batch 2200 Loss 2.1860 Accuracy 0.5616\n",
      "Epoch 11 Batch 2250 Loss 2.1862 Accuracy 0.5615\n",
      "Epoch 11 Batch 2300 Loss 2.1857 Accuracy 0.5616\n",
      "Epoch 11 Batch 2350 Loss 2.1852 Accuracy 0.5617\n",
      "Epoch 11 Batch 2400 Loss 2.1853 Accuracy 0.5616\n",
      "Epoch 11 Batch 2450 Loss 2.1850 Accuracy 0.5617\n",
      "Epoch 11 Batch 2500 Loss 2.1848 Accuracy 0.5617\n",
      "Epoch 11 Batch 2550 Loss 2.1850 Accuracy 0.5617\n",
      "Epoch 11 Batch 2600 Loss 2.1850 Accuracy 0.5617\n",
      "Epoch 11 Loss 2.1849 Accuracy 0.5617\n",
      "Time taken for 1 epoch: 185.04949522018433 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.1095 Accuracy 0.5761\n",
      "Epoch 12 Batch 50 Loss 2.1433 Accuracy 0.5662\n",
      "Epoch 12 Batch 100 Loss 2.1486 Accuracy 0.5646\n",
      "Epoch 12 Batch 150 Loss 2.1484 Accuracy 0.5652\n",
      "Epoch 12 Batch 200 Loss 2.1506 Accuracy 0.5652\n",
      "Epoch 12 Batch 250 Loss 2.1552 Accuracy 0.5649\n",
      "Epoch 12 Batch 300 Loss 2.1534 Accuracy 0.5654\n",
      "Epoch 12 Batch 350 Loss 2.1548 Accuracy 0.5651\n",
      "Epoch 12 Batch 400 Loss 2.1572 Accuracy 0.5647\n",
      "Epoch 12 Batch 450 Loss 2.1561 Accuracy 0.5649\n",
      "Epoch 12 Batch 500 Loss 2.1577 Accuracy 0.5648\n",
      "Epoch 12 Batch 550 Loss 2.1578 Accuracy 0.5649\n",
      "Epoch 12 Batch 600 Loss 2.1571 Accuracy 0.5650\n",
      "Epoch 12 Batch 650 Loss 2.1557 Accuracy 0.5655\n",
      "Epoch 12 Batch 700 Loss 2.1563 Accuracy 0.5654\n",
      "Epoch 12 Batch 750 Loss 2.1549 Accuracy 0.5656\n",
      "Epoch 12 Batch 800 Loss 2.1546 Accuracy 0.5656\n",
      "Epoch 12 Batch 850 Loss 2.1535 Accuracy 0.5657\n",
      "Epoch 12 Batch 900 Loss 2.1546 Accuracy 0.5655\n",
      "Epoch 12 Batch 950 Loss 2.1557 Accuracy 0.5654\n",
      "Epoch 12 Batch 1000 Loss 2.1552 Accuracy 0.5657\n",
      "Epoch 12 Batch 1050 Loss 2.1565 Accuracy 0.5655\n",
      "Epoch 12 Batch 1100 Loss 2.1570 Accuracy 0.5654\n",
      "Epoch 12 Batch 1150 Loss 2.1566 Accuracy 0.5655\n",
      "Epoch 12 Batch 1200 Loss 2.1560 Accuracy 0.5655\n",
      "Epoch 12 Batch 1250 Loss 2.1549 Accuracy 0.5656\n",
      "Epoch 12 Batch 1300 Loss 2.1543 Accuracy 0.5657\n",
      "Epoch 12 Batch 1350 Loss 2.1539 Accuracy 0.5658\n",
      "Epoch 12 Batch 1400 Loss 2.1537 Accuracy 0.5658\n",
      "Epoch 12 Batch 1450 Loss 2.1530 Accuracy 0.5659\n",
      "Epoch 12 Batch 1500 Loss 2.1530 Accuracy 0.5659\n",
      "Epoch 12 Batch 1550 Loss 2.1528 Accuracy 0.5659\n",
      "Epoch 12 Batch 1600 Loss 2.1525 Accuracy 0.5659\n",
      "Epoch 12 Batch 1650 Loss 2.1522 Accuracy 0.5660\n",
      "Epoch 12 Batch 1700 Loss 2.1524 Accuracy 0.5660\n",
      "Epoch 12 Batch 1750 Loss 2.1521 Accuracy 0.5661\n",
      "Epoch 12 Batch 1800 Loss 2.1522 Accuracy 0.5661\n",
      "Epoch 12 Batch 1850 Loss 2.1526 Accuracy 0.5661\n",
      "Epoch 12 Batch 1900 Loss 2.1530 Accuracy 0.5661\n",
      "Epoch 12 Batch 1950 Loss 2.1528 Accuracy 0.5661\n",
      "Epoch 12 Batch 2000 Loss 2.1526 Accuracy 0.5661\n",
      "Epoch 12 Batch 2050 Loss 2.1528 Accuracy 0.5662\n",
      "Epoch 12 Batch 2100 Loss 2.1527 Accuracy 0.5662\n",
      "Epoch 12 Batch 2150 Loss 2.1526 Accuracy 0.5661\n",
      "Epoch 12 Batch 2200 Loss 2.1520 Accuracy 0.5662\n",
      "Epoch 12 Batch 2250 Loss 2.1524 Accuracy 0.5661\n",
      "Epoch 12 Batch 2300 Loss 2.1522 Accuracy 0.5662\n",
      "Epoch 12 Batch 2350 Loss 2.1514 Accuracy 0.5662\n",
      "Epoch 12 Batch 2400 Loss 2.1514 Accuracy 0.5663\n",
      "Epoch 12 Batch 2450 Loss 2.1516 Accuracy 0.5662\n",
      "Epoch 12 Batch 2500 Loss 2.1515 Accuracy 0.5662\n",
      "Epoch 12 Batch 2550 Loss 2.1518 Accuracy 0.5662\n",
      "Epoch 12 Batch 2600 Loss 2.1516 Accuracy 0.5662\n",
      "Epoch 12 Loss 2.1516 Accuracy 0.5662\n",
      "Time taken for 1 epoch: 184.32781076431274 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.0049 Accuracy 0.5885\n",
      "Epoch 13 Batch 50 Loss 2.1531 Accuracy 0.5658\n",
      "Epoch 13 Batch 100 Loss 2.1314 Accuracy 0.5690\n",
      "Epoch 13 Batch 150 Loss 2.1295 Accuracy 0.5693\n",
      "Epoch 13 Batch 200 Loss 2.1256 Accuracy 0.5699\n",
      "Epoch 13 Batch 250 Loss 2.1252 Accuracy 0.5695\n",
      "Epoch 13 Batch 300 Loss 2.1250 Accuracy 0.5693\n",
      "Epoch 13 Batch 350 Loss 2.1260 Accuracy 0.5693\n",
      "Epoch 13 Batch 400 Loss 2.1267 Accuracy 0.5694\n",
      "Epoch 13 Batch 450 Loss 2.1280 Accuracy 0.5692\n",
      "Epoch 13 Batch 500 Loss 2.1257 Accuracy 0.5697\n",
      "Epoch 13 Batch 550 Loss 2.1251 Accuracy 0.5698\n",
      "Epoch 13 Batch 600 Loss 2.1272 Accuracy 0.5692\n",
      "Epoch 13 Batch 650 Loss 2.1289 Accuracy 0.5690\n",
      "Epoch 13 Batch 700 Loss 2.1280 Accuracy 0.5691\n",
      "Epoch 13 Batch 750 Loss 2.1271 Accuracy 0.5694\n",
      "Epoch 13 Batch 800 Loss 2.1258 Accuracy 0.5695\n",
      "Epoch 13 Batch 850 Loss 2.1262 Accuracy 0.5697\n",
      "Epoch 13 Batch 900 Loss 2.1250 Accuracy 0.5699\n",
      "Epoch 13 Batch 950 Loss 2.1265 Accuracy 0.5696\n",
      "Epoch 13 Batch 1000 Loss 2.1265 Accuracy 0.5696\n",
      "Epoch 13 Batch 1050 Loss 2.1260 Accuracy 0.5698\n",
      "Epoch 13 Batch 1100 Loss 2.1256 Accuracy 0.5699\n",
      "Epoch 13 Batch 1150 Loss 2.1254 Accuracy 0.5699\n",
      "Epoch 13 Batch 1200 Loss 2.1250 Accuracy 0.5701\n",
      "Epoch 13 Batch 1250 Loss 2.1253 Accuracy 0.5700\n",
      "Epoch 13 Batch 1300 Loss 2.1256 Accuracy 0.5699\n",
      "Epoch 13 Batch 1350 Loss 2.1255 Accuracy 0.5700\n",
      "Epoch 13 Batch 1400 Loss 2.1251 Accuracy 0.5701\n",
      "Epoch 13 Batch 1450 Loss 2.1247 Accuracy 0.5701\n",
      "Epoch 13 Batch 1500 Loss 2.1245 Accuracy 0.5703\n",
      "Epoch 13 Batch 1550 Loss 2.1253 Accuracy 0.5702\n",
      "Epoch 13 Batch 1600 Loss 2.1249 Accuracy 0.5702\n",
      "Epoch 13 Batch 1650 Loss 2.1247 Accuracy 0.5701\n",
      "Epoch 13 Batch 1700 Loss 2.1244 Accuracy 0.5702\n",
      "Epoch 13 Batch 1750 Loss 2.1245 Accuracy 0.5701\n",
      "Epoch 13 Batch 1800 Loss 2.1240 Accuracy 0.5702\n",
      "Epoch 13 Batch 1850 Loss 2.1237 Accuracy 0.5703\n",
      "Epoch 13 Batch 1900 Loss 2.1236 Accuracy 0.5703\n",
      "Epoch 13 Batch 1950 Loss 2.1232 Accuracy 0.5704\n",
      "Epoch 13 Batch 2000 Loss 2.1231 Accuracy 0.5704\n",
      "Epoch 13 Batch 2050 Loss 2.1234 Accuracy 0.5704\n",
      "Epoch 13 Batch 2100 Loss 2.1233 Accuracy 0.5705\n",
      "Epoch 13 Batch 2150 Loss 2.1236 Accuracy 0.5704\n",
      "Epoch 13 Batch 2200 Loss 2.1235 Accuracy 0.5704\n",
      "Epoch 13 Batch 2250 Loss 2.1237 Accuracy 0.5704\n",
      "Epoch 13 Batch 2300 Loss 2.1240 Accuracy 0.5703\n",
      "Epoch 13 Batch 2350 Loss 2.1241 Accuracy 0.5703\n",
      "Epoch 13 Batch 2400 Loss 2.1243 Accuracy 0.5703\n",
      "Epoch 13 Batch 2450 Loss 2.1241 Accuracy 0.5703\n",
      "Epoch 13 Batch 2500 Loss 2.1236 Accuracy 0.5704\n",
      "Epoch 13 Batch 2550 Loss 2.1234 Accuracy 0.5704\n",
      "Epoch 13 Batch 2600 Loss 2.1231 Accuracy 0.5704\n",
      "Epoch 13 Loss 2.1230 Accuracy 0.5705\n",
      "Time taken for 1 epoch: 181.62181305885315 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.0551 Accuracy 0.5963\n",
      "Epoch 14 Batch 50 Loss 2.0971 Accuracy 0.5754\n",
      "Epoch 14 Batch 100 Loss 2.0946 Accuracy 0.5757\n",
      "Epoch 14 Batch 150 Loss 2.1011 Accuracy 0.5737\n",
      "Epoch 14 Batch 200 Loss 2.0973 Accuracy 0.5737\n",
      "Epoch 14 Batch 250 Loss 2.0972 Accuracy 0.5735\n",
      "Epoch 14 Batch 300 Loss 2.0981 Accuracy 0.5738\n",
      "Epoch 14 Batch 350 Loss 2.0997 Accuracy 0.5732\n",
      "Epoch 14 Batch 400 Loss 2.1027 Accuracy 0.5730\n",
      "Epoch 14 Batch 450 Loss 2.1038 Accuracy 0.5726\n",
      "Epoch 14 Batch 500 Loss 2.1029 Accuracy 0.5726\n",
      "Epoch 14 Batch 550 Loss 2.1022 Accuracy 0.5730\n",
      "Epoch 14 Batch 600 Loss 2.0999 Accuracy 0.5734\n",
      "Epoch 14 Batch 650 Loss 2.1004 Accuracy 0.5735\n",
      "Epoch 14 Batch 700 Loss 2.1008 Accuracy 0.5733\n",
      "Epoch 14 Batch 750 Loss 2.1023 Accuracy 0.5732\n",
      "Epoch 14 Batch 800 Loss 2.1012 Accuracy 0.5736\n",
      "Epoch 14 Batch 850 Loss 2.1020 Accuracy 0.5734\n",
      "Epoch 14 Batch 900 Loss 2.1016 Accuracy 0.5733\n",
      "Epoch 14 Batch 950 Loss 2.1021 Accuracy 0.5732\n",
      "Epoch 14 Batch 1000 Loss 2.1024 Accuracy 0.5733\n",
      "Epoch 14 Batch 1050 Loss 2.1025 Accuracy 0.5732\n",
      "Epoch 14 Batch 1100 Loss 2.1017 Accuracy 0.5735\n",
      "Epoch 14 Batch 1150 Loss 2.1010 Accuracy 0.5735\n",
      "Epoch 14 Batch 1200 Loss 2.1007 Accuracy 0.5735\n",
      "Epoch 14 Batch 1250 Loss 2.1011 Accuracy 0.5735\n",
      "Epoch 14 Batch 1300 Loss 2.1001 Accuracy 0.5737\n",
      "Epoch 14 Batch 1350 Loss 2.0996 Accuracy 0.5736\n",
      "Epoch 14 Batch 1400 Loss 2.0988 Accuracy 0.5738\n",
      "Epoch 14 Batch 1450 Loss 2.0990 Accuracy 0.5737\n",
      "Epoch 14 Batch 1500 Loss 2.0986 Accuracy 0.5737\n",
      "Epoch 14 Batch 1550 Loss 2.0987 Accuracy 0.5738\n",
      "Epoch 14 Batch 1600 Loss 2.0989 Accuracy 0.5738\n",
      "Epoch 14 Batch 1650 Loss 2.0980 Accuracy 0.5740\n",
      "Epoch 14 Batch 1700 Loss 2.0977 Accuracy 0.5741\n",
      "Epoch 14 Batch 1750 Loss 2.0975 Accuracy 0.5741\n",
      "Epoch 14 Batch 1800 Loss 2.0984 Accuracy 0.5740\n",
      "Epoch 14 Batch 1850 Loss 2.0978 Accuracy 0.5741\n",
      "Epoch 14 Batch 1900 Loss 2.0970 Accuracy 0.5742\n",
      "Epoch 14 Batch 1950 Loss 2.0971 Accuracy 0.5742\n",
      "Epoch 14 Batch 2000 Loss 2.0973 Accuracy 0.5743\n",
      "Epoch 14 Batch 2050 Loss 2.0969 Accuracy 0.5744\n",
      "Epoch 14 Batch 2100 Loss 2.0975 Accuracy 0.5743\n",
      "Epoch 14 Batch 2150 Loss 2.0982 Accuracy 0.5741\n",
      "Epoch 14 Batch 2200 Loss 2.0980 Accuracy 0.5742\n",
      "Epoch 14 Batch 2250 Loss 2.0974 Accuracy 0.5742\n",
      "Epoch 14 Batch 2300 Loss 2.0977 Accuracy 0.5741\n",
      "Epoch 14 Batch 2350 Loss 2.0974 Accuracy 0.5742\n",
      "Epoch 14 Batch 2400 Loss 2.0975 Accuracy 0.5741\n",
      "Epoch 14 Batch 2450 Loss 2.0972 Accuracy 0.5742\n",
      "Epoch 14 Batch 2500 Loss 2.0974 Accuracy 0.5741\n",
      "Epoch 14 Batch 2550 Loss 2.0974 Accuracy 0.5742\n",
      "Epoch 14 Batch 2600 Loss 2.0975 Accuracy 0.5742\n",
      "Epoch 14 Loss 2.0974 Accuracy 0.5742\n",
      "Time taken for 1 epoch: 181.12108421325684 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.0466 Accuracy 0.5701\n",
      "Epoch 15 Batch 50 Loss 2.0422 Accuracy 0.5825\n",
      "Epoch 15 Batch 100 Loss 2.0668 Accuracy 0.5777\n",
      "Epoch 15 Batch 150 Loss 2.0635 Accuracy 0.5787\n",
      "Epoch 15 Batch 200 Loss 2.0648 Accuracy 0.5783\n",
      "Epoch 15 Batch 250 Loss 2.0713 Accuracy 0.5776\n",
      "Epoch 15 Batch 300 Loss 2.0723 Accuracy 0.5776\n",
      "Epoch 15 Batch 350 Loss 2.0716 Accuracy 0.5777\n",
      "Epoch 15 Batch 400 Loss 2.0759 Accuracy 0.5769\n",
      "Epoch 15 Batch 450 Loss 2.0740 Accuracy 0.5771\n",
      "Epoch 15 Batch 500 Loss 2.0738 Accuracy 0.5772\n",
      "Epoch 15 Batch 550 Loss 2.0733 Accuracy 0.5774\n",
      "Epoch 15 Batch 600 Loss 2.0730 Accuracy 0.5772\n",
      "Epoch 15 Batch 650 Loss 2.0739 Accuracy 0.5771\n",
      "Epoch 15 Batch 700 Loss 2.0736 Accuracy 0.5772\n",
      "Epoch 15 Batch 750 Loss 2.0760 Accuracy 0.5768\n",
      "Epoch 15 Batch 800 Loss 2.0757 Accuracy 0.5768\n",
      "Epoch 15 Batch 850 Loss 2.0763 Accuracy 0.5769\n",
      "Epoch 15 Batch 900 Loss 2.0770 Accuracy 0.5767\n",
      "Epoch 15 Batch 950 Loss 2.0758 Accuracy 0.5769\n",
      "Epoch 15 Batch 1000 Loss 2.0750 Accuracy 0.5770\n",
      "Epoch 15 Batch 1050 Loss 2.0754 Accuracy 0.5770\n",
      "Epoch 15 Batch 1100 Loss 2.0742 Accuracy 0.5771\n",
      "Epoch 15 Batch 1150 Loss 2.0748 Accuracy 0.5770\n",
      "Epoch 15 Batch 1200 Loss 2.0748 Accuracy 0.5768\n",
      "Epoch 15 Batch 1250 Loss 2.0752 Accuracy 0.5768\n",
      "Epoch 15 Batch 1300 Loss 2.0750 Accuracy 0.5768\n",
      "Epoch 15 Batch 1350 Loss 2.0745 Accuracy 0.5769\n",
      "Epoch 15 Batch 1400 Loss 2.0748 Accuracy 0.5770\n",
      "Epoch 15 Batch 1450 Loss 2.0749 Accuracy 0.5770\n",
      "Epoch 15 Batch 1500 Loss 2.0753 Accuracy 0.5770\n",
      "Epoch 15 Batch 1550 Loss 2.0747 Accuracy 0.5771\n",
      "Epoch 15 Batch 1600 Loss 2.0742 Accuracy 0.5772\n",
      "Epoch 15 Batch 1650 Loss 2.0742 Accuracy 0.5773\n",
      "Epoch 15 Batch 1700 Loss 2.0738 Accuracy 0.5774\n",
      "Epoch 15 Batch 1750 Loss 2.0741 Accuracy 0.5774\n",
      "Epoch 15 Batch 1800 Loss 2.0745 Accuracy 0.5774\n",
      "Epoch 15 Batch 1850 Loss 2.0747 Accuracy 0.5773\n",
      "Epoch 15 Batch 1900 Loss 2.0747 Accuracy 0.5772\n",
      "Epoch 15 Batch 1950 Loss 2.0740 Accuracy 0.5773\n",
      "Epoch 15 Batch 2000 Loss 2.0743 Accuracy 0.5773\n",
      "Epoch 15 Batch 2050 Loss 2.0743 Accuracy 0.5773\n",
      "Epoch 15 Batch 2100 Loss 2.0745 Accuracy 0.5772\n",
      "Epoch 15 Batch 2150 Loss 2.0743 Accuracy 0.5773\n",
      "Epoch 15 Batch 2200 Loss 2.0746 Accuracy 0.5773\n",
      "Epoch 15 Batch 2250 Loss 2.0749 Accuracy 0.5773\n",
      "Epoch 15 Batch 2300 Loss 2.0749 Accuracy 0.5772\n",
      "Epoch 15 Batch 2350 Loss 2.0743 Accuracy 0.5773\n",
      "Epoch 15 Batch 2400 Loss 2.0741 Accuracy 0.5773\n",
      "Epoch 15 Batch 2450 Loss 2.0745 Accuracy 0.5773\n",
      "Epoch 15 Batch 2500 Loss 2.0742 Accuracy 0.5774\n",
      "Epoch 15 Batch 2550 Loss 2.0745 Accuracy 0.5773\n",
      "Epoch 15 Batch 2600 Loss 2.0745 Accuracy 0.5773\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Epoch 15 Loss 2.0748 Accuracy 0.5773\n",
      "Time taken for 1 epoch: 181.69680047035217 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.1660 Accuracy 0.5689\n",
      "Epoch 16 Batch 50 Loss 2.0552 Accuracy 0.5802\n",
      "Epoch 16 Batch 100 Loss 2.0568 Accuracy 0.5801\n",
      "Epoch 16 Batch 150 Loss 2.0625 Accuracy 0.5795\n",
      "Epoch 16 Batch 200 Loss 2.0608 Accuracy 0.5794\n",
      "Epoch 16 Batch 250 Loss 2.0605 Accuracy 0.5795\n",
      "Epoch 16 Batch 300 Loss 2.0589 Accuracy 0.5799\n",
      "Epoch 16 Batch 350 Loss 2.0573 Accuracy 0.5800\n",
      "Epoch 16 Batch 400 Loss 2.0587 Accuracy 0.5799\n",
      "Epoch 16 Batch 450 Loss 2.0599 Accuracy 0.5799\n",
      "Epoch 16 Batch 500 Loss 2.0583 Accuracy 0.5801\n",
      "Epoch 16 Batch 550 Loss 2.0578 Accuracy 0.5800\n",
      "Epoch 16 Batch 600 Loss 2.0558 Accuracy 0.5802\n",
      "Epoch 16 Batch 650 Loss 2.0534 Accuracy 0.5804\n",
      "Epoch 16 Batch 700 Loss 2.0530 Accuracy 0.5804\n",
      "Epoch 16 Batch 750 Loss 2.0533 Accuracy 0.5804\n",
      "Epoch 16 Batch 800 Loss 2.0536 Accuracy 0.5802\n",
      "Epoch 16 Batch 850 Loss 2.0552 Accuracy 0.5800\n",
      "Epoch 16 Batch 900 Loss 2.0554 Accuracy 0.5800\n",
      "Epoch 16 Batch 950 Loss 2.0557 Accuracy 0.5798\n",
      "Epoch 16 Batch 1000 Loss 2.0565 Accuracy 0.5798\n",
      "Epoch 16 Batch 1050 Loss 2.0566 Accuracy 0.5797\n",
      "Epoch 16 Batch 1100 Loss 2.0549 Accuracy 0.5800\n",
      "Epoch 16 Batch 1150 Loss 2.0546 Accuracy 0.5801\n",
      "Epoch 16 Batch 1200 Loss 2.0544 Accuracy 0.5802\n",
      "Epoch 16 Batch 1250 Loss 2.0537 Accuracy 0.5803\n",
      "Epoch 16 Batch 1300 Loss 2.0533 Accuracy 0.5804\n",
      "Epoch 16 Batch 1350 Loss 2.0541 Accuracy 0.5802\n",
      "Epoch 16 Batch 1400 Loss 2.0545 Accuracy 0.5801\n",
      "Epoch 16 Batch 1450 Loss 2.0545 Accuracy 0.5801\n",
      "Epoch 16 Batch 1500 Loss 2.0537 Accuracy 0.5803\n",
      "Epoch 16 Batch 1550 Loss 2.0532 Accuracy 0.5804\n",
      "Epoch 16 Batch 1600 Loss 2.0529 Accuracy 0.5805\n",
      "Epoch 16 Batch 1650 Loss 2.0528 Accuracy 0.5805\n",
      "Epoch 16 Batch 1700 Loss 2.0525 Accuracy 0.5806\n",
      "Epoch 16 Batch 1750 Loss 2.0530 Accuracy 0.5805\n",
      "Epoch 16 Batch 1800 Loss 2.0530 Accuracy 0.5805\n",
      "Epoch 16 Batch 1850 Loss 2.0536 Accuracy 0.5804\n",
      "Epoch 16 Batch 1900 Loss 2.0539 Accuracy 0.5803\n",
      "Epoch 16 Batch 1950 Loss 2.0539 Accuracy 0.5803\n",
      "Epoch 16 Batch 2000 Loss 2.0543 Accuracy 0.5802\n",
      "Epoch 16 Batch 2050 Loss 2.0542 Accuracy 0.5803\n",
      "Epoch 16 Batch 2100 Loss 2.0541 Accuracy 0.5803\n",
      "Epoch 16 Batch 2150 Loss 2.0546 Accuracy 0.5803\n",
      "Epoch 16 Batch 2200 Loss 2.0542 Accuracy 0.5803\n",
      "Epoch 16 Batch 2250 Loss 2.0541 Accuracy 0.5803\n",
      "Epoch 16 Batch 2300 Loss 2.0544 Accuracy 0.5803\n",
      "Epoch 16 Batch 2350 Loss 2.0546 Accuracy 0.5803\n",
      "Epoch 16 Batch 2400 Loss 2.0547 Accuracy 0.5803\n",
      "Epoch 16 Batch 2450 Loss 2.0547 Accuracy 0.5803\n",
      "Epoch 16 Batch 2500 Loss 2.0547 Accuracy 0.5802\n",
      "Epoch 16 Batch 2550 Loss 2.0550 Accuracy 0.5802\n",
      "Epoch 16 Batch 2600 Loss 2.0548 Accuracy 0.5802\n",
      "Epoch 16 Loss 2.0547 Accuracy 0.5802\n",
      "Time taken for 1 epoch: 180.57558941841125 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.9136 Accuracy 0.6031\n",
      "Epoch 17 Batch 50 Loss 2.0397 Accuracy 0.5836\n",
      "Epoch 17 Batch 100 Loss 2.0338 Accuracy 0.5829\n",
      "Epoch 17 Batch 150 Loss 2.0321 Accuracy 0.5833\n",
      "Epoch 17 Batch 200 Loss 2.0346 Accuracy 0.5832\n",
      "Epoch 17 Batch 250 Loss 2.0320 Accuracy 0.5833\n",
      "Epoch 17 Batch 300 Loss 2.0343 Accuracy 0.5829\n",
      "Epoch 17 Batch 350 Loss 2.0329 Accuracy 0.5833\n",
      "Epoch 17 Batch 400 Loss 2.0350 Accuracy 0.5830\n",
      "Epoch 17 Batch 450 Loss 2.0358 Accuracy 0.5826\n",
      "Epoch 17 Batch 500 Loss 2.0359 Accuracy 0.5826\n",
      "Epoch 17 Batch 550 Loss 2.0354 Accuracy 0.5828\n",
      "Epoch 17 Batch 600 Loss 2.0367 Accuracy 0.5824\n",
      "Epoch 17 Batch 650 Loss 2.0362 Accuracy 0.5823\n",
      "Epoch 17 Batch 700 Loss 2.0374 Accuracy 0.5822\n",
      "Epoch 17 Batch 750 Loss 2.0368 Accuracy 0.5822\n",
      "Epoch 17 Batch 800 Loss 2.0359 Accuracy 0.5823\n",
      "Epoch 17 Batch 850 Loss 2.0365 Accuracy 0.5822\n",
      "Epoch 17 Batch 900 Loss 2.0356 Accuracy 0.5823\n",
      "Epoch 17 Batch 950 Loss 2.0356 Accuracy 0.5824\n",
      "Epoch 17 Batch 1000 Loss 2.0352 Accuracy 0.5824\n",
      "Epoch 17 Batch 1050 Loss 2.0351 Accuracy 0.5824\n",
      "Epoch 17 Batch 1100 Loss 2.0356 Accuracy 0.5824\n",
      "Epoch 17 Batch 1150 Loss 2.0360 Accuracy 0.5825\n",
      "Epoch 17 Batch 1200 Loss 2.0362 Accuracy 0.5825\n",
      "Epoch 17 Batch 1250 Loss 2.0355 Accuracy 0.5826\n",
      "Epoch 17 Batch 1300 Loss 2.0359 Accuracy 0.5825\n",
      "Epoch 17 Batch 1350 Loss 2.0354 Accuracy 0.5826\n",
      "Epoch 17 Batch 1400 Loss 2.0346 Accuracy 0.5828\n",
      "Epoch 17 Batch 1450 Loss 2.0345 Accuracy 0.5828\n",
      "Epoch 17 Batch 1500 Loss 2.0332 Accuracy 0.5830\n",
      "Epoch 17 Batch 1550 Loss 2.0332 Accuracy 0.5829\n",
      "Epoch 17 Batch 1600 Loss 2.0332 Accuracy 0.5829\n",
      "Epoch 17 Batch 1650 Loss 2.0334 Accuracy 0.5830\n",
      "Epoch 17 Batch 1700 Loss 2.0338 Accuracy 0.5829\n",
      "Epoch 17 Batch 1750 Loss 2.0343 Accuracy 0.5828\n",
      "Epoch 17 Batch 1800 Loss 2.0348 Accuracy 0.5827\n",
      "Epoch 17 Batch 1850 Loss 2.0342 Accuracy 0.5829\n",
      "Epoch 17 Batch 1900 Loss 2.0345 Accuracy 0.5828\n",
      "Epoch 17 Batch 1950 Loss 2.0343 Accuracy 0.5828\n",
      "Epoch 17 Batch 2000 Loss 2.0347 Accuracy 0.5828\n",
      "Epoch 17 Batch 2050 Loss 2.0346 Accuracy 0.5829\n",
      "Epoch 17 Batch 2100 Loss 2.0348 Accuracy 0.5829\n",
      "Epoch 17 Batch 2150 Loss 2.0350 Accuracy 0.5828\n",
      "Epoch 17 Batch 2200 Loss 2.0355 Accuracy 0.5827\n",
      "Epoch 17 Batch 2250 Loss 2.0355 Accuracy 0.5828\n",
      "Epoch 17 Batch 2300 Loss 2.0359 Accuracy 0.5827\n",
      "Epoch 17 Batch 2350 Loss 2.0357 Accuracy 0.5827\n",
      "Epoch 17 Batch 2400 Loss 2.0359 Accuracy 0.5826\n",
      "Epoch 17 Batch 2450 Loss 2.0355 Accuracy 0.5827\n",
      "Epoch 17 Batch 2500 Loss 2.0353 Accuracy 0.5827\n",
      "Epoch 17 Batch 2550 Loss 2.0352 Accuracy 0.5827\n",
      "Epoch 17 Batch 2600 Loss 2.0356 Accuracy 0.5827\n",
      "Epoch 17 Loss 2.0357 Accuracy 0.5827\n",
      "Time taken for 1 epoch: 180.66464710235596 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 2.0734 Accuracy 0.5756\n",
      "Epoch 18 Batch 50 Loss 2.0003 Accuracy 0.5868\n",
      "Epoch 18 Batch 100 Loss 2.0059 Accuracy 0.5868\n",
      "Epoch 18 Batch 150 Loss 2.0014 Accuracy 0.5880\n",
      "Epoch 18 Batch 200 Loss 2.0091 Accuracy 0.5875\n",
      "Epoch 18 Batch 250 Loss 2.0181 Accuracy 0.5858\n",
      "Epoch 18 Batch 300 Loss 2.0142 Accuracy 0.5861\n",
      "Epoch 18 Batch 350 Loss 2.0160 Accuracy 0.5860\n",
      "Epoch 18 Batch 400 Loss 2.0170 Accuracy 0.5858\n",
      "Epoch 18 Batch 450 Loss 2.0193 Accuracy 0.5854\n",
      "Epoch 18 Batch 500 Loss 2.0183 Accuracy 0.5856\n",
      "Epoch 18 Batch 550 Loss 2.0179 Accuracy 0.5857\n",
      "Epoch 18 Batch 600 Loss 2.0185 Accuracy 0.5856\n",
      "Epoch 18 Batch 650 Loss 2.0174 Accuracy 0.5859\n",
      "Epoch 18 Batch 700 Loss 2.0168 Accuracy 0.5859\n",
      "Epoch 18 Batch 750 Loss 2.0171 Accuracy 0.5858\n",
      "Epoch 18 Batch 800 Loss 2.0176 Accuracy 0.5857\n",
      "Epoch 18 Batch 850 Loss 2.0166 Accuracy 0.5858\n",
      "Epoch 18 Batch 900 Loss 2.0166 Accuracy 0.5859\n",
      "Epoch 18 Batch 950 Loss 2.0159 Accuracy 0.5860\n",
      "Epoch 18 Batch 1000 Loss 2.0156 Accuracy 0.5862\n",
      "Epoch 18 Batch 1050 Loss 2.0165 Accuracy 0.5860\n",
      "Epoch 18 Batch 1100 Loss 2.0165 Accuracy 0.5860\n",
      "Epoch 18 Batch 1150 Loss 2.0169 Accuracy 0.5859\n",
      "Epoch 18 Batch 1200 Loss 2.0172 Accuracy 0.5858\n",
      "Epoch 18 Batch 1250 Loss 2.0176 Accuracy 0.5857\n",
      "Epoch 18 Batch 1300 Loss 2.0179 Accuracy 0.5857\n",
      "Epoch 18 Batch 1350 Loss 2.0175 Accuracy 0.5858\n",
      "Epoch 18 Batch 1400 Loss 2.0170 Accuracy 0.5859\n",
      "Epoch 18 Batch 1450 Loss 2.0172 Accuracy 0.5858\n",
      "Epoch 18 Batch 1500 Loss 2.0175 Accuracy 0.5858\n",
      "Epoch 18 Batch 1550 Loss 2.0175 Accuracy 0.5857\n",
      "Epoch 18 Batch 1600 Loss 2.0177 Accuracy 0.5857\n",
      "Epoch 18 Batch 1650 Loss 2.0188 Accuracy 0.5855\n",
      "Epoch 18 Batch 1700 Loss 2.0184 Accuracy 0.5856\n",
      "Epoch 18 Batch 1750 Loss 2.0186 Accuracy 0.5856\n",
      "Epoch 18 Batch 1800 Loss 2.0195 Accuracy 0.5855\n",
      "Epoch 18 Batch 1850 Loss 2.0198 Accuracy 0.5854\n",
      "Epoch 18 Batch 1900 Loss 2.0196 Accuracy 0.5854\n",
      "Epoch 18 Batch 1950 Loss 2.0194 Accuracy 0.5855\n",
      "Epoch 18 Batch 2000 Loss 2.0189 Accuracy 0.5856\n",
      "Epoch 18 Batch 2050 Loss 2.0185 Accuracy 0.5856\n",
      "Epoch 18 Batch 2100 Loss 2.0180 Accuracy 0.5857\n",
      "Epoch 18 Batch 2150 Loss 2.0185 Accuracy 0.5856\n",
      "Epoch 18 Batch 2200 Loss 2.0184 Accuracy 0.5856\n",
      "Epoch 18 Batch 2250 Loss 2.0185 Accuracy 0.5856\n",
      "Epoch 18 Batch 2300 Loss 2.0185 Accuracy 0.5856\n",
      "Epoch 18 Batch 2350 Loss 2.0190 Accuracy 0.5856\n",
      "Epoch 18 Batch 2400 Loss 2.0194 Accuracy 0.5855\n",
      "Epoch 18 Batch 2450 Loss 2.0189 Accuracy 0.5855\n",
      "Epoch 18 Batch 2500 Loss 2.0190 Accuracy 0.5855\n",
      "Epoch 18 Batch 2550 Loss 2.0193 Accuracy 0.5855\n",
      "Epoch 18 Batch 2600 Loss 2.0195 Accuracy 0.5855\n",
      "Epoch 18 Loss 2.0195 Accuracy 0.5855\n",
      "Time taken for 1 epoch: 179.53962874412537 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.8673 Accuracy 0.6007\n",
      "Epoch 19 Batch 50 Loss 1.9915 Accuracy 0.5905\n",
      "Epoch 19 Batch 100 Loss 1.9867 Accuracy 0.5912\n",
      "Epoch 19 Batch 150 Loss 1.9888 Accuracy 0.5908\n",
      "Epoch 19 Batch 200 Loss 1.9978 Accuracy 0.5887\n",
      "Epoch 19 Batch 250 Loss 1.9993 Accuracy 0.5882\n",
      "Epoch 19 Batch 300 Loss 2.0005 Accuracy 0.5879\n",
      "Epoch 19 Batch 350 Loss 2.0001 Accuracy 0.5878\n",
      "Epoch 19 Batch 400 Loss 2.0026 Accuracy 0.5874\n",
      "Epoch 19 Batch 450 Loss 2.0012 Accuracy 0.5876\n",
      "Epoch 19 Batch 500 Loss 2.0018 Accuracy 0.5875\n",
      "Epoch 19 Batch 550 Loss 2.0020 Accuracy 0.5873\n",
      "Epoch 19 Batch 600 Loss 2.0001 Accuracy 0.5875\n",
      "Epoch 19 Batch 650 Loss 1.9998 Accuracy 0.5877\n",
      "Epoch 19 Batch 700 Loss 1.9997 Accuracy 0.5877\n",
      "Epoch 19 Batch 750 Loss 1.9998 Accuracy 0.5877\n",
      "Epoch 19 Batch 800 Loss 1.9989 Accuracy 0.5880\n",
      "Epoch 19 Batch 850 Loss 2.0000 Accuracy 0.5879\n",
      "Epoch 19 Batch 900 Loss 2.0000 Accuracy 0.5879\n",
      "Epoch 19 Batch 950 Loss 1.9995 Accuracy 0.5879\n",
      "Epoch 19 Batch 1000 Loss 2.0001 Accuracy 0.5879\n",
      "Epoch 19 Batch 1050 Loss 1.9998 Accuracy 0.5880\n",
      "Epoch 19 Batch 1100 Loss 1.9998 Accuracy 0.5881\n",
      "Epoch 19 Batch 1150 Loss 2.0011 Accuracy 0.5880\n",
      "Epoch 19 Batch 1200 Loss 2.0018 Accuracy 0.5880\n",
      "Epoch 19 Batch 1250 Loss 2.0022 Accuracy 0.5880\n",
      "Epoch 19 Batch 1300 Loss 2.0021 Accuracy 0.5879\n",
      "Epoch 19 Batch 1350 Loss 2.0010 Accuracy 0.5881\n",
      "Epoch 19 Batch 1400 Loss 2.0016 Accuracy 0.5880\n",
      "Epoch 19 Batch 1450 Loss 2.0015 Accuracy 0.5880\n",
      "Epoch 19 Batch 1500 Loss 2.0010 Accuracy 0.5881\n",
      "Epoch 19 Batch 1550 Loss 2.0011 Accuracy 0.5881\n",
      "Epoch 19 Batch 1600 Loss 2.0015 Accuracy 0.5881\n",
      "Epoch 19 Batch 1650 Loss 2.0023 Accuracy 0.5880\n",
      "Epoch 19 Batch 1700 Loss 2.0027 Accuracy 0.5879\n",
      "Epoch 19 Batch 1750 Loss 2.0025 Accuracy 0.5879\n",
      "Epoch 19 Batch 1800 Loss 2.0032 Accuracy 0.5877\n",
      "Epoch 19 Batch 1850 Loss 2.0029 Accuracy 0.5878\n",
      "Epoch 19 Batch 1900 Loss 2.0026 Accuracy 0.5879\n",
      "Epoch 19 Batch 1950 Loss 2.0029 Accuracy 0.5878\n",
      "Epoch 19 Batch 2000 Loss 2.0027 Accuracy 0.5879\n",
      "Epoch 19 Batch 2050 Loss 2.0028 Accuracy 0.5879\n",
      "Epoch 19 Batch 2100 Loss 2.0028 Accuracy 0.5879\n",
      "Epoch 19 Batch 2150 Loss 2.0030 Accuracy 0.5879\n",
      "Epoch 19 Batch 2200 Loss 2.0032 Accuracy 0.5879\n",
      "Epoch 19 Batch 2250 Loss 2.0030 Accuracy 0.5879\n",
      "Epoch 19 Batch 2300 Loss 2.0030 Accuracy 0.5878\n",
      "Epoch 19 Batch 2350 Loss 2.0029 Accuracy 0.5878\n",
      "Epoch 19 Batch 2400 Loss 2.0028 Accuracy 0.5878\n",
      "Epoch 19 Batch 2450 Loss 2.0032 Accuracy 0.5877\n",
      "Epoch 19 Batch 2500 Loss 2.0031 Accuracy 0.5878\n",
      "Epoch 19 Batch 2550 Loss 2.0032 Accuracy 0.5878\n",
      "Epoch 19 Batch 2600 Loss 2.0034 Accuracy 0.5878\n",
      "Epoch 19 Loss 2.0036 Accuracy 0.5877\n",
      "Time taken for 1 epoch: 183.2386772632599 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 2.0167 Accuracy 0.6012\n",
      "Epoch 20 Batch 50 Loss 1.9693 Accuracy 0.5911\n",
      "Epoch 20 Batch 100 Loss 1.9733 Accuracy 0.5907\n",
      "Epoch 20 Batch 150 Loss 1.9856 Accuracy 0.5889\n",
      "Epoch 20 Batch 200 Loss 1.9836 Accuracy 0.5896\n",
      "Epoch 20 Batch 250 Loss 1.9851 Accuracy 0.5897\n",
      "Epoch 20 Batch 300 Loss 1.9856 Accuracy 0.5900\n",
      "Epoch 20 Batch 350 Loss 1.9875 Accuracy 0.5897\n",
      "Epoch 20 Batch 400 Loss 1.9887 Accuracy 0.5895\n",
      "Epoch 20 Batch 450 Loss 1.9917 Accuracy 0.5889\n",
      "Epoch 20 Batch 500 Loss 1.9877 Accuracy 0.5896\n",
      "Epoch 20 Batch 550 Loss 1.9872 Accuracy 0.5898\n",
      "Epoch 20 Batch 600 Loss 1.9876 Accuracy 0.5898\n",
      "Epoch 20 Batch 650 Loss 1.9869 Accuracy 0.5897\n",
      "Epoch 20 Batch 700 Loss 1.9875 Accuracy 0.5896\n",
      "Epoch 20 Batch 750 Loss 1.9878 Accuracy 0.5896\n",
      "Epoch 20 Batch 800 Loss 1.9878 Accuracy 0.5896\n",
      "Epoch 20 Batch 850 Loss 1.9878 Accuracy 0.5895\n",
      "Epoch 20 Batch 900 Loss 1.9877 Accuracy 0.5897\n",
      "Epoch 20 Batch 950 Loss 1.9896 Accuracy 0.5894\n",
      "Epoch 20 Batch 1000 Loss 1.9895 Accuracy 0.5896\n",
      "Epoch 20 Batch 1050 Loss 1.9887 Accuracy 0.5897\n",
      "Epoch 20 Batch 1100 Loss 1.9892 Accuracy 0.5897\n",
      "Epoch 20 Batch 1150 Loss 1.9887 Accuracy 0.5897\n",
      "Epoch 20 Batch 1200 Loss 1.9886 Accuracy 0.5897\n",
      "Epoch 20 Batch 1250 Loss 1.9879 Accuracy 0.5898\n",
      "Epoch 20 Batch 1300 Loss 1.9877 Accuracy 0.5898\n",
      "Epoch 20 Batch 1350 Loss 1.9866 Accuracy 0.5900\n",
      "Epoch 20 Batch 1400 Loss 1.9870 Accuracy 0.5900\n",
      "Epoch 20 Batch 1450 Loss 1.9869 Accuracy 0.5900\n",
      "Epoch 20 Batch 1500 Loss 1.9871 Accuracy 0.5899\n",
      "Epoch 20 Batch 1550 Loss 1.9869 Accuracy 0.5900\n",
      "Epoch 20 Batch 1600 Loss 1.9865 Accuracy 0.5901\n",
      "Epoch 20 Batch 1650 Loss 1.9870 Accuracy 0.5899\n",
      "Epoch 20 Batch 1700 Loss 1.9874 Accuracy 0.5899\n",
      "Epoch 20 Batch 1750 Loss 1.9879 Accuracy 0.5898\n",
      "Epoch 20 Batch 1800 Loss 1.9878 Accuracy 0.5899\n",
      "Epoch 20 Batch 1850 Loss 1.9879 Accuracy 0.5899\n",
      "Epoch 20 Batch 1900 Loss 1.9886 Accuracy 0.5898\n",
      "Epoch 20 Batch 1950 Loss 1.9889 Accuracy 0.5898\n",
      "Epoch 20 Batch 2000 Loss 1.9888 Accuracy 0.5898\n",
      "Epoch 20 Batch 2050 Loss 1.9885 Accuracy 0.5898\n",
      "Epoch 20 Batch 2100 Loss 1.9883 Accuracy 0.5899\n",
      "Epoch 20 Batch 2150 Loss 1.9879 Accuracy 0.5899\n",
      "Epoch 20 Batch 2200 Loss 1.9881 Accuracy 0.5899\n",
      "Epoch 20 Batch 2250 Loss 1.9886 Accuracy 0.5898\n",
      "Epoch 20 Batch 2300 Loss 1.9891 Accuracy 0.5897\n",
      "Epoch 20 Batch 2350 Loss 1.9894 Accuracy 0.5897\n",
      "Epoch 20 Batch 2400 Loss 1.9891 Accuracy 0.5897\n",
      "Epoch 20 Batch 2450 Loss 1.9888 Accuracy 0.5898\n",
      "Epoch 20 Batch 2500 Loss 1.9886 Accuracy 0.5898\n",
      "Epoch 20 Batch 2550 Loss 1.9889 Accuracy 0.5898\n",
      "Epoch 20 Batch 2600 Loss 1.9891 Accuracy 0.5898\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Epoch 20 Loss 1.9894 Accuracy 0.5897\n",
      "Time taken for 1 epoch: 173.12081170082092 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 2.0112 Accuracy 0.5711\n",
      "Epoch 21 Batch 50 Loss 1.9986 Accuracy 0.5865\n",
      "Epoch 21 Batch 100 Loss 1.9809 Accuracy 0.5900\n",
      "Epoch 21 Batch 150 Loss 1.9697 Accuracy 0.5922\n",
      "Epoch 21 Batch 200 Loss 1.9717 Accuracy 0.5923\n",
      "Epoch 21 Batch 250 Loss 1.9692 Accuracy 0.5926\n",
      "Epoch 21 Batch 300 Loss 1.9684 Accuracy 0.5928\n",
      "Epoch 21 Batch 350 Loss 1.9733 Accuracy 0.5920\n",
      "Epoch 21 Batch 400 Loss 1.9718 Accuracy 0.5923\n",
      "Epoch 21 Batch 450 Loss 1.9754 Accuracy 0.5920\n",
      "Epoch 21 Batch 500 Loss 1.9770 Accuracy 0.5918\n",
      "Epoch 21 Batch 550 Loss 1.9776 Accuracy 0.5918\n",
      "Epoch 21 Batch 600 Loss 1.9771 Accuracy 0.5919\n",
      "Epoch 21 Batch 650 Loss 1.9769 Accuracy 0.5918\n",
      "Epoch 21 Batch 700 Loss 1.9780 Accuracy 0.5915\n",
      "Epoch 21 Batch 750 Loss 1.9772 Accuracy 0.5915\n",
      "Epoch 21 Batch 800 Loss 1.9770 Accuracy 0.5916\n",
      "Epoch 21 Batch 850 Loss 1.9777 Accuracy 0.5913\n",
      "Epoch 21 Batch 900 Loss 1.9785 Accuracy 0.5912\n",
      "Epoch 21 Batch 950 Loss 1.9791 Accuracy 0.5911\n",
      "Epoch 21 Batch 1000 Loss 1.9783 Accuracy 0.5911\n",
      "Epoch 21 Batch 1050 Loss 1.9786 Accuracy 0.5910\n",
      "Epoch 21 Batch 1100 Loss 1.9772 Accuracy 0.5912\n",
      "Epoch 21 Batch 1150 Loss 1.9762 Accuracy 0.5914\n",
      "Epoch 21 Batch 1200 Loss 1.9755 Accuracy 0.5917\n",
      "Epoch 21 Batch 1250 Loss 1.9751 Accuracy 0.5917\n",
      "Epoch 21 Batch 1300 Loss 1.9747 Accuracy 0.5917\n",
      "Epoch 21 Batch 1350 Loss 1.9747 Accuracy 0.5917\n",
      "Epoch 21 Batch 1400 Loss 1.9744 Accuracy 0.5918\n",
      "Epoch 21 Batch 1450 Loss 1.9744 Accuracy 0.5918\n",
      "Epoch 21 Batch 1500 Loss 1.9744 Accuracy 0.5918\n",
      "Epoch 21 Batch 1550 Loss 1.9747 Accuracy 0.5918\n",
      "Epoch 21 Batch 1600 Loss 1.9746 Accuracy 0.5918\n",
      "Epoch 21 Batch 1650 Loss 1.9748 Accuracy 0.5918\n",
      "Epoch 21 Batch 1700 Loss 1.9748 Accuracy 0.5917\n",
      "Epoch 21 Batch 1750 Loss 1.9748 Accuracy 0.5917\n",
      "Epoch 21 Batch 1800 Loss 1.9753 Accuracy 0.5916\n",
      "Epoch 21 Batch 1850 Loss 1.9747 Accuracy 0.5917\n",
      "Epoch 21 Batch 1900 Loss 1.9748 Accuracy 0.5917\n",
      "Epoch 21 Batch 1950 Loss 1.9752 Accuracy 0.5916\n",
      "Epoch 21 Batch 2000 Loss 1.9751 Accuracy 0.5917\n",
      "Epoch 21 Batch 2050 Loss 1.9754 Accuracy 0.5916\n",
      "Epoch 21 Batch 2100 Loss 1.9753 Accuracy 0.5917\n",
      "Epoch 21 Batch 2150 Loss 1.9754 Accuracy 0.5917\n",
      "Epoch 21 Batch 2200 Loss 1.9757 Accuracy 0.5916\n",
      "Epoch 21 Batch 2250 Loss 1.9762 Accuracy 0.5916\n",
      "Epoch 21 Batch 2300 Loss 1.9764 Accuracy 0.5915\n",
      "Epoch 21 Batch 2350 Loss 1.9762 Accuracy 0.5915\n",
      "Epoch 21 Batch 2400 Loss 1.9762 Accuracy 0.5915\n",
      "Epoch 21 Batch 2450 Loss 1.9759 Accuracy 0.5916\n",
      "Epoch 21 Batch 2500 Loss 1.9760 Accuracy 0.5916\n",
      "Epoch 21 Batch 2550 Loss 1.9760 Accuracy 0.5916\n",
      "Epoch 21 Batch 2600 Loss 1.9763 Accuracy 0.5915\n",
      "Epoch 21 Loss 1.9764 Accuracy 0.5915\n",
      "Time taken for 1 epoch: 172.13713693618774 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.9678 Accuracy 0.5871\n",
      "Epoch 22 Batch 50 Loss 1.9516 Accuracy 0.5948\n",
      "Epoch 22 Batch 100 Loss 1.9555 Accuracy 0.5958\n",
      "Epoch 22 Batch 150 Loss 1.9565 Accuracy 0.5955\n",
      "Epoch 22 Batch 200 Loss 1.9509 Accuracy 0.5963\n",
      "Epoch 22 Batch 250 Loss 1.9552 Accuracy 0.5949\n",
      "Epoch 22 Batch 300 Loss 1.9563 Accuracy 0.5944\n",
      "Epoch 22 Batch 350 Loss 1.9590 Accuracy 0.5939\n",
      "Epoch 22 Batch 400 Loss 1.9609 Accuracy 0.5935\n",
      "Epoch 22 Batch 450 Loss 1.9611 Accuracy 0.5934\n",
      "Epoch 22 Batch 500 Loss 1.9606 Accuracy 0.5935\n",
      "Epoch 22 Batch 550 Loss 1.9619 Accuracy 0.5933\n",
      "Epoch 22 Batch 600 Loss 1.9622 Accuracy 0.5931\n",
      "Epoch 22 Batch 650 Loss 1.9621 Accuracy 0.5930\n",
      "Epoch 22 Batch 700 Loss 1.9635 Accuracy 0.5928\n",
      "Epoch 22 Batch 750 Loss 1.9633 Accuracy 0.5929\n",
      "Epoch 22 Batch 800 Loss 1.9624 Accuracy 0.5929\n",
      "Epoch 22 Batch 850 Loss 1.9632 Accuracy 0.5928\n",
      "Epoch 22 Batch 900 Loss 1.9633 Accuracy 0.5929\n",
      "Epoch 22 Batch 950 Loss 1.9627 Accuracy 0.5931\n",
      "Epoch 22 Batch 1000 Loss 1.9625 Accuracy 0.5931\n",
      "Epoch 22 Batch 1050 Loss 1.9621 Accuracy 0.5933\n",
      "Epoch 22 Batch 1100 Loss 1.9623 Accuracy 0.5932\n",
      "Epoch 22 Batch 1150 Loss 1.9607 Accuracy 0.5936\n",
      "Epoch 22 Batch 1200 Loss 1.9605 Accuracy 0.5936\n",
      "Epoch 22 Batch 1250 Loss 1.9606 Accuracy 0.5936\n",
      "Epoch 22 Batch 1300 Loss 1.9607 Accuracy 0.5937\n",
      "Epoch 22 Batch 1350 Loss 1.9609 Accuracy 0.5936\n",
      "Epoch 22 Batch 1400 Loss 1.9617 Accuracy 0.5936\n",
      "Epoch 22 Batch 1450 Loss 1.9608 Accuracy 0.5937\n",
      "Epoch 22 Batch 1500 Loss 1.9609 Accuracy 0.5937\n",
      "Epoch 22 Batch 1550 Loss 1.9611 Accuracy 0.5937\n",
      "Epoch 22 Batch 1600 Loss 1.9613 Accuracy 0.5936\n",
      "Epoch 22 Batch 1650 Loss 1.9609 Accuracy 0.5937\n",
      "Epoch 22 Batch 1700 Loss 1.9610 Accuracy 0.5937\n",
      "Epoch 22 Batch 1750 Loss 1.9615 Accuracy 0.5937\n",
      "Epoch 22 Batch 1800 Loss 1.9612 Accuracy 0.5937\n",
      "Epoch 22 Batch 1850 Loss 1.9611 Accuracy 0.5938\n",
      "Epoch 22 Batch 1900 Loss 1.9615 Accuracy 0.5936\n",
      "Epoch 22 Batch 1950 Loss 1.9623 Accuracy 0.5935\n",
      "Epoch 22 Batch 2000 Loss 1.9621 Accuracy 0.5935\n",
      "Epoch 22 Batch 2050 Loss 1.9627 Accuracy 0.5935\n",
      "Epoch 22 Batch 2100 Loss 1.9628 Accuracy 0.5935\n",
      "Epoch 22 Batch 2150 Loss 1.9628 Accuracy 0.5935\n",
      "Epoch 22 Batch 2200 Loss 1.9628 Accuracy 0.5935\n",
      "Epoch 22 Batch 2250 Loss 1.9626 Accuracy 0.5935\n",
      "Epoch 22 Batch 2300 Loss 1.9629 Accuracy 0.5934\n",
      "Epoch 22 Batch 2350 Loss 1.9630 Accuracy 0.5934\n",
      "Epoch 22 Batch 2400 Loss 1.9626 Accuracy 0.5934\n",
      "Epoch 22 Batch 2450 Loss 1.9624 Accuracy 0.5934\n",
      "Epoch 22 Batch 2500 Loss 1.9620 Accuracy 0.5935\n",
      "Epoch 22 Batch 2550 Loss 1.9623 Accuracy 0.5934\n",
      "Epoch 22 Batch 2600 Loss 1.9625 Accuracy 0.5935\n",
      "Epoch 22 Loss 1.9625 Accuracy 0.5934\n",
      "Time taken for 1 epoch: 172.72800469398499 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.0376 Accuracy 0.5886\n",
      "Epoch 23 Batch 50 Loss 1.9549 Accuracy 0.5956\n",
      "Epoch 23 Batch 100 Loss 1.9477 Accuracy 0.5953\n",
      "Epoch 23 Batch 150 Loss 1.9451 Accuracy 0.5965\n",
      "Epoch 23 Batch 200 Loss 1.9415 Accuracy 0.5969\n",
      "Epoch 23 Batch 250 Loss 1.9416 Accuracy 0.5965\n",
      "Epoch 23 Batch 300 Loss 1.9424 Accuracy 0.5966\n",
      "Epoch 23 Batch 350 Loss 1.9435 Accuracy 0.5966\n",
      "Epoch 23 Batch 400 Loss 1.9436 Accuracy 0.5962\n",
      "Epoch 23 Batch 450 Loss 1.9406 Accuracy 0.5966\n",
      "Epoch 23 Batch 500 Loss 1.9422 Accuracy 0.5964\n",
      "Epoch 23 Batch 550 Loss 1.9437 Accuracy 0.5963\n",
      "Epoch 23 Batch 600 Loss 1.9453 Accuracy 0.5960\n",
      "Epoch 23 Batch 650 Loss 1.9461 Accuracy 0.5959\n",
      "Epoch 23 Batch 700 Loss 1.9467 Accuracy 0.5957\n",
      "Epoch 23 Batch 750 Loss 1.9466 Accuracy 0.5957\n",
      "Epoch 23 Batch 800 Loss 1.9467 Accuracy 0.5956\n",
      "Epoch 23 Batch 850 Loss 1.9475 Accuracy 0.5954\n",
      "Epoch 23 Batch 900 Loss 1.9496 Accuracy 0.5951\n",
      "Epoch 23 Batch 950 Loss 1.9492 Accuracy 0.5952\n",
      "Epoch 23 Batch 1000 Loss 1.9502 Accuracy 0.5950\n",
      "Epoch 23 Batch 1050 Loss 1.9491 Accuracy 0.5953\n",
      "Epoch 23 Batch 1100 Loss 1.9486 Accuracy 0.5954\n",
      "Epoch 23 Batch 1150 Loss 1.9488 Accuracy 0.5954\n",
      "Epoch 23 Batch 1200 Loss 1.9487 Accuracy 0.5954\n",
      "Epoch 23 Batch 1250 Loss 1.9480 Accuracy 0.5955\n",
      "Epoch 23 Batch 1300 Loss 1.9476 Accuracy 0.5956\n",
      "Epoch 23 Batch 1350 Loss 1.9475 Accuracy 0.5956\n",
      "Epoch 23 Batch 1400 Loss 1.9472 Accuracy 0.5957\n",
      "Epoch 23 Batch 1450 Loss 1.9464 Accuracy 0.5959\n",
      "Epoch 23 Batch 1500 Loss 1.9455 Accuracy 0.5960\n",
      "Epoch 23 Batch 1550 Loss 1.9460 Accuracy 0.5959\n",
      "Epoch 23 Batch 1600 Loss 1.9470 Accuracy 0.5958\n",
      "Epoch 23 Batch 1650 Loss 1.9474 Accuracy 0.5957\n",
      "Epoch 23 Batch 1700 Loss 1.9473 Accuracy 0.5956\n",
      "Epoch 23 Batch 1750 Loss 1.9476 Accuracy 0.5956\n",
      "Epoch 23 Batch 1800 Loss 1.9482 Accuracy 0.5956\n",
      "Epoch 23 Batch 1850 Loss 1.9488 Accuracy 0.5955\n",
      "Epoch 23 Batch 1900 Loss 1.9496 Accuracy 0.5954\n",
      "Epoch 23 Batch 1950 Loss 1.9498 Accuracy 0.5953\n",
      "Epoch 23 Batch 2000 Loss 1.9501 Accuracy 0.5954\n",
      "Epoch 23 Batch 2050 Loss 1.9500 Accuracy 0.5954\n",
      "Epoch 23 Batch 2100 Loss 1.9499 Accuracy 0.5954\n",
      "Epoch 23 Batch 2150 Loss 1.9503 Accuracy 0.5954\n",
      "Epoch 23 Batch 2200 Loss 1.9505 Accuracy 0.5953\n",
      "Epoch 23 Batch 2250 Loss 1.9510 Accuracy 0.5952\n",
      "Epoch 23 Batch 2300 Loss 1.9510 Accuracy 0.5952\n",
      "Epoch 23 Batch 2350 Loss 1.9508 Accuracy 0.5952\n",
      "Epoch 23 Batch 2400 Loss 1.9506 Accuracy 0.5952\n",
      "Epoch 23 Batch 2450 Loss 1.9507 Accuracy 0.5952\n",
      "Epoch 23 Batch 2500 Loss 1.9509 Accuracy 0.5952\n",
      "Epoch 23 Batch 2550 Loss 1.9511 Accuracy 0.5952\n",
      "Epoch 23 Batch 2600 Loss 1.9515 Accuracy 0.5951\n",
      "Epoch 23 Loss 1.9512 Accuracy 0.5952\n",
      "Time taken for 1 epoch: 175.30960035324097 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.9609 Accuracy 0.5880\n",
      "Epoch 24 Batch 50 Loss 1.9352 Accuracy 0.5967\n",
      "Epoch 24 Batch 100 Loss 1.9411 Accuracy 0.5964\n",
      "Epoch 24 Batch 150 Loss 1.9410 Accuracy 0.5964\n",
      "Epoch 24 Batch 200 Loss 1.9410 Accuracy 0.5965\n",
      "Epoch 24 Batch 250 Loss 1.9405 Accuracy 0.5964\n",
      "Epoch 24 Batch 300 Loss 1.9416 Accuracy 0.5962\n",
      "Epoch 24 Batch 350 Loss 1.9377 Accuracy 0.5970\n",
      "Epoch 24 Batch 400 Loss 1.9394 Accuracy 0.5969\n",
      "Epoch 24 Batch 450 Loss 1.9416 Accuracy 0.5964\n",
      "Epoch 24 Batch 500 Loss 1.9416 Accuracy 0.5961\n",
      "Epoch 24 Batch 550 Loss 1.9429 Accuracy 0.5957\n",
      "Epoch 24 Batch 600 Loss 1.9411 Accuracy 0.5963\n",
      "Epoch 24 Batch 650 Loss 1.9400 Accuracy 0.5967\n",
      "Epoch 24 Batch 700 Loss 1.9405 Accuracy 0.5965\n",
      "Epoch 24 Batch 750 Loss 1.9410 Accuracy 0.5963\n",
      "Epoch 24 Batch 800 Loss 1.9406 Accuracy 0.5965\n",
      "Epoch 24 Batch 850 Loss 1.9408 Accuracy 0.5964\n",
      "Epoch 24 Batch 900 Loss 1.9403 Accuracy 0.5964\n",
      "Epoch 24 Batch 950 Loss 1.9408 Accuracy 0.5963\n",
      "Epoch 24 Batch 1000 Loss 1.9397 Accuracy 0.5964\n",
      "Epoch 24 Batch 1050 Loss 1.9402 Accuracy 0.5964\n",
      "Epoch 24 Batch 1100 Loss 1.9392 Accuracy 0.5966\n",
      "Epoch 24 Batch 1150 Loss 1.9402 Accuracy 0.5965\n",
      "Epoch 24 Batch 1200 Loss 1.9405 Accuracy 0.5965\n",
      "Epoch 24 Batch 1250 Loss 1.9411 Accuracy 0.5964\n",
      "Epoch 24 Batch 1300 Loss 1.9406 Accuracy 0.5965\n",
      "Epoch 24 Batch 1350 Loss 1.9398 Accuracy 0.5966\n",
      "Epoch 24 Batch 1400 Loss 1.9398 Accuracy 0.5966\n",
      "Epoch 24 Batch 1450 Loss 1.9392 Accuracy 0.5966\n",
      "Epoch 24 Batch 1500 Loss 1.9391 Accuracy 0.5966\n",
      "Epoch 24 Batch 1550 Loss 1.9395 Accuracy 0.5966\n",
      "Epoch 24 Batch 1600 Loss 1.9397 Accuracy 0.5965\n",
      "Epoch 24 Batch 1650 Loss 1.9395 Accuracy 0.5966\n",
      "Epoch 24 Batch 1700 Loss 1.9397 Accuracy 0.5966\n",
      "Epoch 24 Batch 1750 Loss 1.9395 Accuracy 0.5967\n",
      "Epoch 24 Batch 1800 Loss 1.9396 Accuracy 0.5967\n",
      "Epoch 24 Batch 1850 Loss 1.9394 Accuracy 0.5967\n",
      "Epoch 24 Batch 1900 Loss 1.9388 Accuracy 0.5967\n",
      "Epoch 24 Batch 1950 Loss 1.9391 Accuracy 0.5967\n",
      "Epoch 24 Batch 2000 Loss 1.9387 Accuracy 0.5968\n",
      "Epoch 24 Batch 2050 Loss 1.9388 Accuracy 0.5968\n",
      "Epoch 24 Batch 2100 Loss 1.9386 Accuracy 0.5968\n",
      "Epoch 24 Batch 2150 Loss 1.9393 Accuracy 0.5966\n",
      "Epoch 24 Batch 2200 Loss 1.9398 Accuracy 0.5965\n",
      "Epoch 24 Batch 2250 Loss 1.9401 Accuracy 0.5965\n",
      "Epoch 24 Batch 2300 Loss 1.9401 Accuracy 0.5965\n",
      "Epoch 24 Batch 2350 Loss 1.9402 Accuracy 0.5965\n",
      "Epoch 24 Batch 2400 Loss 1.9399 Accuracy 0.5966\n",
      "Epoch 24 Batch 2450 Loss 1.9397 Accuracy 0.5966\n",
      "Epoch 24 Batch 2500 Loss 1.9398 Accuracy 0.5967\n",
      "Epoch 24 Batch 2550 Loss 1.9404 Accuracy 0.5966\n",
      "Epoch 24 Batch 2600 Loss 1.9408 Accuracy 0.5965\n",
      "Epoch 24 Loss 1.9410 Accuracy 0.5965\n",
      "Time taken for 1 epoch: 173.66587853431702 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.9328 Accuracy 0.5976\n",
      "Epoch 25 Batch 50 Loss 1.9315 Accuracy 0.5970\n",
      "Epoch 25 Batch 100 Loss 1.9287 Accuracy 0.5982\n",
      "Epoch 25 Batch 150 Loss 1.9275 Accuracy 0.5985\n",
      "Epoch 25 Batch 200 Loss 1.9297 Accuracy 0.5981\n",
      "Epoch 25 Batch 250 Loss 1.9280 Accuracy 0.5982\n",
      "Epoch 25 Batch 300 Loss 1.9274 Accuracy 0.5986\n",
      "Epoch 25 Batch 350 Loss 1.9290 Accuracy 0.5983\n",
      "Epoch 25 Batch 400 Loss 1.9279 Accuracy 0.5984\n",
      "Epoch 25 Batch 450 Loss 1.9307 Accuracy 0.5977\n",
      "Epoch 25 Batch 500 Loss 1.9310 Accuracy 0.5975\n",
      "Epoch 25 Batch 550 Loss 1.9299 Accuracy 0.5977\n",
      "Epoch 25 Batch 600 Loss 1.9283 Accuracy 0.5978\n",
      "Epoch 25 Batch 650 Loss 1.9274 Accuracy 0.5980\n",
      "Epoch 25 Batch 700 Loss 1.9261 Accuracy 0.5982\n",
      "Epoch 25 Batch 750 Loss 1.9268 Accuracy 0.5982\n",
      "Epoch 25 Batch 800 Loss 1.9283 Accuracy 0.5980\n",
      "Epoch 25 Batch 850 Loss 1.9293 Accuracy 0.5978\n",
      "Epoch 25 Batch 900 Loss 1.9286 Accuracy 0.5978\n",
      "Epoch 25 Batch 950 Loss 1.9284 Accuracy 0.5978\n",
      "Epoch 25 Batch 1000 Loss 1.9293 Accuracy 0.5978\n",
      "Epoch 25 Batch 1050 Loss 1.9294 Accuracy 0.5979\n",
      "Epoch 25 Batch 1100 Loss 1.9303 Accuracy 0.5977\n",
      "Epoch 25 Batch 1150 Loss 1.9304 Accuracy 0.5979\n",
      "Epoch 25 Batch 1200 Loss 1.9293 Accuracy 0.5980\n",
      "Epoch 25 Batch 1250 Loss 1.9293 Accuracy 0.5980\n",
      "Epoch 25 Batch 1300 Loss 1.9299 Accuracy 0.5980\n",
      "Epoch 25 Batch 1350 Loss 1.9302 Accuracy 0.5979\n",
      "Epoch 25 Batch 1400 Loss 1.9301 Accuracy 0.5980\n",
      "Epoch 25 Batch 1450 Loss 1.9293 Accuracy 0.5981\n",
      "Epoch 25 Batch 1500 Loss 1.9285 Accuracy 0.5982\n",
      "Epoch 25 Batch 1550 Loss 1.9290 Accuracy 0.5982\n",
      "Epoch 25 Batch 1600 Loss 1.9282 Accuracy 0.5984\n",
      "Epoch 25 Batch 1650 Loss 1.9283 Accuracy 0.5984\n",
      "Epoch 25 Batch 1700 Loss 1.9287 Accuracy 0.5984\n",
      "Epoch 25 Batch 1750 Loss 1.9290 Accuracy 0.5984\n",
      "Epoch 25 Batch 1800 Loss 1.9290 Accuracy 0.5984\n",
      "Epoch 25 Batch 1850 Loss 1.9290 Accuracy 0.5984\n",
      "Epoch 25 Batch 1900 Loss 1.9295 Accuracy 0.5983\n",
      "Epoch 25 Batch 1950 Loss 1.9297 Accuracy 0.5983\n",
      "Epoch 25 Batch 2000 Loss 1.9288 Accuracy 0.5984\n",
      "Epoch 25 Batch 2050 Loss 1.9287 Accuracy 0.5985\n",
      "Epoch 25 Batch 2100 Loss 1.9291 Accuracy 0.5984\n",
      "Epoch 25 Batch 2150 Loss 1.9294 Accuracy 0.5984\n",
      "Epoch 25 Batch 2200 Loss 1.9287 Accuracy 0.5985\n",
      "Epoch 25 Batch 2250 Loss 1.9290 Accuracy 0.5984\n",
      "Epoch 25 Batch 2300 Loss 1.9291 Accuracy 0.5984\n",
      "Epoch 25 Batch 2350 Loss 1.9290 Accuracy 0.5984\n",
      "Epoch 25 Batch 2400 Loss 1.9297 Accuracy 0.5984\n",
      "Epoch 25 Batch 2450 Loss 1.9297 Accuracy 0.5984\n",
      "Epoch 25 Batch 2500 Loss 1.9302 Accuracy 0.5984\n",
      "Epoch 25 Batch 2550 Loss 1.9299 Accuracy 0.5984\n",
      "Epoch 25 Batch 2600 Loss 1.9300 Accuracy 0.5983\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
      "Epoch 25 Loss 1.9300 Accuracy 0.5983\n",
      "Time taken for 1 epoch: 172.29648971557617 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.8686 Accuracy 0.5990\n",
      "Epoch 26 Batch 50 Loss 1.9274 Accuracy 0.5976\n",
      "Epoch 26 Batch 100 Loss 1.9204 Accuracy 0.5991\n",
      "Epoch 26 Batch 150 Loss 1.9175 Accuracy 0.5992\n",
      "Epoch 26 Batch 200 Loss 1.9184 Accuracy 0.5991\n",
      "Epoch 26 Batch 250 Loss 1.9175 Accuracy 0.5994\n",
      "Epoch 26 Batch 300 Loss 1.9148 Accuracy 0.5999\n",
      "Epoch 26 Batch 350 Loss 1.9155 Accuracy 0.6001\n",
      "Epoch 26 Batch 400 Loss 1.9175 Accuracy 0.5999\n",
      "Epoch 26 Batch 450 Loss 1.9169 Accuracy 0.5999\n",
      "Epoch 26 Batch 500 Loss 1.9155 Accuracy 0.6000\n",
      "Epoch 26 Batch 550 Loss 1.9167 Accuracy 0.5999\n",
      "Epoch 26 Batch 600 Loss 1.9166 Accuracy 0.5998\n",
      "Epoch 26 Batch 650 Loss 1.9176 Accuracy 0.5998\n",
      "Epoch 26 Batch 700 Loss 1.9190 Accuracy 0.5995\n",
      "Epoch 26 Batch 750 Loss 1.9205 Accuracy 0.5992\n",
      "Epoch 26 Batch 800 Loss 1.9217 Accuracy 0.5990\n",
      "Epoch 26 Batch 850 Loss 1.9217 Accuracy 0.5988\n",
      "Epoch 26 Batch 900 Loss 1.9213 Accuracy 0.5989\n",
      "Epoch 26 Batch 950 Loss 1.9200 Accuracy 0.5990\n",
      "Epoch 26 Batch 1000 Loss 1.9200 Accuracy 0.5990\n",
      "Epoch 26 Batch 1050 Loss 1.9200 Accuracy 0.5990\n",
      "Epoch 26 Batch 1100 Loss 1.9201 Accuracy 0.5990\n",
      "Epoch 26 Batch 1150 Loss 1.9198 Accuracy 0.5991\n",
      "Epoch 26 Batch 1200 Loss 1.9201 Accuracy 0.5992\n",
      "Epoch 26 Batch 1250 Loss 1.9191 Accuracy 0.5994\n",
      "Epoch 26 Batch 1300 Loss 1.9183 Accuracy 0.5996\n",
      "Epoch 26 Batch 1350 Loss 1.9177 Accuracy 0.5997\n",
      "Epoch 26 Batch 1400 Loss 1.9180 Accuracy 0.5997\n",
      "Epoch 26 Batch 1450 Loss 1.9178 Accuracy 0.5998\n",
      "Epoch 26 Batch 1500 Loss 1.9184 Accuracy 0.5998\n",
      "Epoch 26 Batch 1550 Loss 1.9187 Accuracy 0.5997\n",
      "Epoch 26 Batch 1600 Loss 1.9192 Accuracy 0.5997\n",
      "Epoch 26 Batch 1650 Loss 1.9191 Accuracy 0.5997\n",
      "Epoch 26 Batch 1700 Loss 1.9195 Accuracy 0.5997\n",
      "Epoch 26 Batch 1750 Loss 1.9191 Accuracy 0.5997\n",
      "Epoch 26 Batch 1800 Loss 1.9198 Accuracy 0.5996\n",
      "Epoch 26 Batch 1850 Loss 1.9196 Accuracy 0.5997\n",
      "Epoch 26 Batch 1900 Loss 1.9197 Accuracy 0.5997\n",
      "Epoch 26 Batch 1950 Loss 1.9192 Accuracy 0.5998\n",
      "Epoch 26 Batch 2000 Loss 1.9194 Accuracy 0.5998\n",
      "Epoch 26 Batch 2050 Loss 1.9199 Accuracy 0.5997\n",
      "Epoch 26 Batch 2100 Loss 1.9197 Accuracy 0.5998\n",
      "Epoch 26 Batch 2150 Loss 1.9200 Accuracy 0.5998\n",
      "Epoch 26 Batch 2200 Loss 1.9203 Accuracy 0.5997\n",
      "Epoch 26 Batch 2250 Loss 1.9200 Accuracy 0.5998\n",
      "Epoch 26 Batch 2300 Loss 1.9209 Accuracy 0.5996\n",
      "Epoch 26 Batch 2350 Loss 1.9210 Accuracy 0.5996\n",
      "Epoch 26 Batch 2400 Loss 1.9209 Accuracy 0.5996\n",
      "Epoch 26 Batch 2450 Loss 1.9212 Accuracy 0.5995\n",
      "Epoch 26 Batch 2500 Loss 1.9214 Accuracy 0.5995\n",
      "Epoch 26 Batch 2550 Loss 1.9213 Accuracy 0.5995\n",
      "Epoch 26 Batch 2600 Loss 1.9213 Accuracy 0.5995\n",
      "Epoch 26 Loss 1.9212 Accuracy 0.5995\n",
      "Time taken for 1 epoch: 169.2429461479187 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.7401 Accuracy 0.6171\n",
      "Epoch 27 Batch 50 Loss 1.9002 Accuracy 0.6010\n",
      "Epoch 27 Batch 100 Loss 1.9068 Accuracy 0.6008\n",
      "Epoch 27 Batch 150 Loss 1.9124 Accuracy 0.6004\n",
      "Epoch 27 Batch 200 Loss 1.9126 Accuracy 0.6004\n",
      "Epoch 27 Batch 250 Loss 1.9084 Accuracy 0.6007\n",
      "Epoch 27 Batch 300 Loss 1.9099 Accuracy 0.6004\n",
      "Epoch 27 Batch 350 Loss 1.9114 Accuracy 0.6006\n",
      "Epoch 27 Batch 400 Loss 1.9094 Accuracy 0.6009\n",
      "Epoch 27 Batch 450 Loss 1.9099 Accuracy 0.6007\n",
      "Epoch 27 Batch 500 Loss 1.9104 Accuracy 0.6006\n",
      "Epoch 27 Batch 550 Loss 1.9104 Accuracy 0.6007\n",
      "Epoch 27 Batch 600 Loss 1.9091 Accuracy 0.6009\n",
      "Epoch 27 Batch 650 Loss 1.9091 Accuracy 0.6009\n",
      "Epoch 27 Batch 700 Loss 1.9075 Accuracy 0.6012\n",
      "Epoch 27 Batch 750 Loss 1.9080 Accuracy 0.6011\n",
      "Epoch 27 Batch 800 Loss 1.9087 Accuracy 0.6010\n",
      "Epoch 27 Batch 850 Loss 1.9091 Accuracy 0.6008\n",
      "Epoch 27 Batch 900 Loss 1.9100 Accuracy 0.6008\n",
      "Epoch 27 Batch 950 Loss 1.9097 Accuracy 0.6009\n",
      "Epoch 27 Batch 1000 Loss 1.9091 Accuracy 0.6010\n",
      "Epoch 27 Batch 1050 Loss 1.9096 Accuracy 0.6010\n",
      "Epoch 27 Batch 1100 Loss 1.9097 Accuracy 0.6010\n",
      "Epoch 27 Batch 1150 Loss 1.9117 Accuracy 0.6007\n",
      "Epoch 27 Batch 1200 Loss 1.9118 Accuracy 0.6008\n",
      "Epoch 27 Batch 1250 Loss 1.9107 Accuracy 0.6009\n",
      "Epoch 27 Batch 1300 Loss 1.9104 Accuracy 0.6010\n",
      "Epoch 27 Batch 1350 Loss 1.9097 Accuracy 0.6012\n",
      "Epoch 27 Batch 1400 Loss 1.9091 Accuracy 0.6013\n",
      "Epoch 27 Batch 1450 Loss 1.9091 Accuracy 0.6013\n",
      "Epoch 27 Batch 1500 Loss 1.9097 Accuracy 0.6012\n",
      "Epoch 27 Batch 1550 Loss 1.9101 Accuracy 0.6012\n",
      "Epoch 27 Batch 1600 Loss 1.9100 Accuracy 0.6012\n",
      "Epoch 27 Batch 1650 Loss 1.9100 Accuracy 0.6012\n",
      "Epoch 27 Batch 1700 Loss 1.9099 Accuracy 0.6012\n",
      "Epoch 27 Batch 1750 Loss 1.9097 Accuracy 0.6012\n",
      "Epoch 27 Batch 1800 Loss 1.9096 Accuracy 0.6013\n",
      "Epoch 27 Batch 1850 Loss 1.9096 Accuracy 0.6013\n",
      "Epoch 27 Batch 1900 Loss 1.9098 Accuracy 0.6013\n",
      "Epoch 27 Batch 1950 Loss 1.9103 Accuracy 0.6012\n",
      "Epoch 27 Batch 2000 Loss 1.9110 Accuracy 0.6011\n",
      "Epoch 27 Batch 2050 Loss 1.9112 Accuracy 0.6011\n",
      "Epoch 27 Batch 2100 Loss 1.9112 Accuracy 0.6011\n",
      "Epoch 27 Batch 2150 Loss 1.9115 Accuracy 0.6010\n",
      "Epoch 27 Batch 2200 Loss 1.9113 Accuracy 0.6011\n",
      "Epoch 27 Batch 2250 Loss 1.9116 Accuracy 0.6010\n",
      "Epoch 27 Batch 2300 Loss 1.9117 Accuracy 0.6010\n",
      "Epoch 27 Batch 2350 Loss 1.9114 Accuracy 0.6010\n",
      "Epoch 27 Batch 2400 Loss 1.9117 Accuracy 0.6010\n",
      "Epoch 27 Batch 2450 Loss 1.9122 Accuracy 0.6010\n",
      "Epoch 27 Batch 2500 Loss 1.9124 Accuracy 0.6009\n",
      "Epoch 27 Batch 2550 Loss 1.9120 Accuracy 0.6010\n",
      "Epoch 27 Batch 2600 Loss 1.9117 Accuracy 0.6010\n",
      "Epoch 27 Loss 1.9117 Accuracy 0.6010\n",
      "Time taken for 1 epoch: 174.70496249198914 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.8162 Accuracy 0.6021\n",
      "Epoch 28 Batch 50 Loss 1.9248 Accuracy 0.5980\n",
      "Epoch 28 Batch 100 Loss 1.9194 Accuracy 0.5985\n",
      "Epoch 28 Batch 150 Loss 1.9060 Accuracy 0.6010\n",
      "Epoch 28 Batch 200 Loss 1.9012 Accuracy 0.6022\n",
      "Epoch 28 Batch 250 Loss 1.9012 Accuracy 0.6025\n",
      "Epoch 28 Batch 300 Loss 1.8992 Accuracy 0.6031\n",
      "Epoch 28 Batch 350 Loss 1.8979 Accuracy 0.6030\n",
      "Epoch 28 Batch 400 Loss 1.9005 Accuracy 0.6026\n",
      "Epoch 28 Batch 450 Loss 1.9008 Accuracy 0.6025\n",
      "Epoch 28 Batch 500 Loss 1.9007 Accuracy 0.6023\n",
      "Epoch 28 Batch 550 Loss 1.9018 Accuracy 0.6022\n",
      "Epoch 28 Batch 600 Loss 1.9034 Accuracy 0.6020\n",
      "Epoch 28 Batch 650 Loss 1.9042 Accuracy 0.6018\n",
      "Epoch 28 Batch 700 Loss 1.9033 Accuracy 0.6020\n",
      "Epoch 28 Batch 750 Loss 1.9029 Accuracy 0.6022\n",
      "Epoch 28 Batch 800 Loss 1.9026 Accuracy 0.6022\n",
      "Epoch 28 Batch 850 Loss 1.9031 Accuracy 0.6021\n",
      "Epoch 28 Batch 900 Loss 1.9029 Accuracy 0.6023\n",
      "Epoch 28 Batch 950 Loss 1.9024 Accuracy 0.6023\n",
      "Epoch 28 Batch 1000 Loss 1.9013 Accuracy 0.6025\n",
      "Epoch 28 Batch 1050 Loss 1.9013 Accuracy 0.6025\n",
      "Epoch 28 Batch 1100 Loss 1.9014 Accuracy 0.6025\n",
      "Epoch 28 Batch 1150 Loss 1.9010 Accuracy 0.6026\n",
      "Epoch 28 Batch 1200 Loss 1.9021 Accuracy 0.6025\n",
      "Epoch 28 Batch 1250 Loss 1.9014 Accuracy 0.6026\n",
      "Epoch 28 Batch 1300 Loss 1.9002 Accuracy 0.6028\n",
      "Epoch 28 Batch 1350 Loss 1.8999 Accuracy 0.6029\n",
      "Epoch 28 Batch 1400 Loss 1.9003 Accuracy 0.6029\n",
      "Epoch 28 Batch 1450 Loss 1.9003 Accuracy 0.6029\n",
      "Epoch 28 Batch 1500 Loss 1.8997 Accuracy 0.6030\n",
      "Epoch 28 Batch 1550 Loss 1.8994 Accuracy 0.6030\n",
      "Epoch 28 Batch 1600 Loss 1.8998 Accuracy 0.6030\n",
      "Epoch 28 Batch 1650 Loss 1.8996 Accuracy 0.6030\n",
      "Epoch 28 Batch 1700 Loss 1.8999 Accuracy 0.6029\n",
      "Epoch 28 Batch 1750 Loss 1.8999 Accuracy 0.6030\n",
      "Epoch 28 Batch 1800 Loss 1.9003 Accuracy 0.6030\n",
      "Epoch 28 Batch 1850 Loss 1.9013 Accuracy 0.6028\n",
      "Epoch 28 Batch 1900 Loss 1.9016 Accuracy 0.6027\n",
      "Epoch 28 Batch 1950 Loss 1.9015 Accuracy 0.6028\n",
      "Epoch 28 Batch 2000 Loss 1.9015 Accuracy 0.6028\n",
      "Epoch 28 Batch 2050 Loss 1.9010 Accuracy 0.6029\n",
      "Epoch 28 Batch 2100 Loss 1.9011 Accuracy 0.6029\n",
      "Epoch 28 Batch 2150 Loss 1.9016 Accuracy 0.6027\n",
      "Epoch 28 Batch 2200 Loss 1.9020 Accuracy 0.6027\n",
      "Epoch 28 Batch 2250 Loss 1.9022 Accuracy 0.6027\n",
      "Epoch 28 Batch 2300 Loss 1.9029 Accuracy 0.6026\n",
      "Epoch 28 Batch 2350 Loss 1.9022 Accuracy 0.6026\n",
      "Epoch 28 Batch 2400 Loss 1.9018 Accuracy 0.6027\n",
      "Epoch 28 Batch 2450 Loss 1.9022 Accuracy 0.6027\n",
      "Epoch 28 Batch 2500 Loss 1.9022 Accuracy 0.6027\n",
      "Epoch 28 Batch 2550 Loss 1.9028 Accuracy 0.6026\n",
      "Epoch 28 Batch 2600 Loss 1.9030 Accuracy 0.6025\n",
      "Epoch 28 Loss 1.9034 Accuracy 0.6025\n",
      "Time taken for 1 epoch: 180.585608959198 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.9003 Accuracy 0.6030\n",
      "Epoch 29 Batch 50 Loss 1.9014 Accuracy 0.5993\n",
      "Epoch 29 Batch 100 Loss 1.9021 Accuracy 0.6009\n",
      "Epoch 29 Batch 150 Loss 1.8977 Accuracy 0.6017\n",
      "Epoch 29 Batch 200 Loss 1.8934 Accuracy 0.6029\n",
      "Epoch 29 Batch 250 Loss 1.8924 Accuracy 0.6037\n",
      "Epoch 29 Batch 300 Loss 1.8921 Accuracy 0.6035\n",
      "Epoch 29 Batch 350 Loss 1.8929 Accuracy 0.6034\n",
      "Epoch 29 Batch 400 Loss 1.8945 Accuracy 0.6031\n",
      "Epoch 29 Batch 450 Loss 1.8932 Accuracy 0.6037\n",
      "Epoch 29 Batch 500 Loss 1.8929 Accuracy 0.6036\n",
      "Epoch 29 Batch 550 Loss 1.8919 Accuracy 0.6034\n",
      "Epoch 29 Batch 600 Loss 1.8939 Accuracy 0.6031\n",
      "Epoch 29 Batch 650 Loss 1.8938 Accuracy 0.6032\n",
      "Epoch 29 Batch 700 Loss 1.8921 Accuracy 0.6035\n",
      "Epoch 29 Batch 750 Loss 1.8931 Accuracy 0.6034\n",
      "Epoch 29 Batch 800 Loss 1.8933 Accuracy 0.6033\n",
      "Epoch 29 Batch 850 Loss 1.8932 Accuracy 0.6034\n",
      "Epoch 29 Batch 900 Loss 1.8940 Accuracy 0.6033\n",
      "Epoch 29 Batch 950 Loss 1.8943 Accuracy 0.6034\n",
      "Epoch 29 Batch 1000 Loss 1.8938 Accuracy 0.6035\n",
      "Epoch 29 Batch 1050 Loss 1.8937 Accuracy 0.6035\n",
      "Epoch 29 Batch 1100 Loss 1.8937 Accuracy 0.6036\n",
      "Epoch 29 Batch 1150 Loss 1.8928 Accuracy 0.6037\n",
      "Epoch 29 Batch 1200 Loss 1.8935 Accuracy 0.6036\n",
      "Epoch 29 Batch 1250 Loss 1.8932 Accuracy 0.6037\n",
      "Epoch 29 Batch 1300 Loss 1.8934 Accuracy 0.6037\n",
      "Epoch 29 Batch 1350 Loss 1.8932 Accuracy 0.6037\n",
      "Epoch 29 Batch 1400 Loss 1.8940 Accuracy 0.6036\n",
      "Epoch 29 Batch 1450 Loss 1.8940 Accuracy 0.6036\n",
      "Epoch 29 Batch 1500 Loss 1.8924 Accuracy 0.6038\n",
      "Epoch 29 Batch 1550 Loss 1.8934 Accuracy 0.6037\n",
      "Epoch 29 Batch 1600 Loss 1.8935 Accuracy 0.6037\n",
      "Epoch 29 Batch 1650 Loss 1.8938 Accuracy 0.6037\n",
      "Epoch 29 Batch 1700 Loss 1.8940 Accuracy 0.6038\n",
      "Epoch 29 Batch 1750 Loss 1.8933 Accuracy 0.6039\n",
      "Epoch 29 Batch 1800 Loss 1.8934 Accuracy 0.6039\n",
      "Epoch 29 Batch 1850 Loss 1.8935 Accuracy 0.6039\n",
      "Epoch 29 Batch 1900 Loss 1.8933 Accuracy 0.6039\n",
      "Epoch 29 Batch 1950 Loss 1.8937 Accuracy 0.6039\n",
      "Epoch 29 Batch 2000 Loss 1.8944 Accuracy 0.6038\n",
      "Epoch 29 Batch 2050 Loss 1.8948 Accuracy 0.6037\n",
      "Epoch 29 Batch 2100 Loss 1.8944 Accuracy 0.6038\n",
      "Epoch 29 Batch 2150 Loss 1.8944 Accuracy 0.6038\n",
      "Epoch 29 Batch 2200 Loss 1.8946 Accuracy 0.6037\n",
      "Epoch 29 Batch 2250 Loss 1.8945 Accuracy 0.6038\n",
      "Epoch 29 Batch 2300 Loss 1.8948 Accuracy 0.6037\n",
      "Epoch 29 Batch 2350 Loss 1.8943 Accuracy 0.6038\n",
      "Epoch 29 Batch 2400 Loss 1.8949 Accuracy 0.6037\n",
      "Epoch 29 Batch 2450 Loss 1.8947 Accuracy 0.6037\n",
      "Epoch 29 Batch 2500 Loss 1.8947 Accuracy 0.6037\n",
      "Epoch 29 Batch 2550 Loss 1.8951 Accuracy 0.6037\n",
      "Epoch 29 Batch 2600 Loss 1.8953 Accuracy 0.6037\n",
      "Epoch 29 Loss 1.8953 Accuracy 0.6036\n",
      "Time taken for 1 epoch: 179.61391615867615 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.9993 Accuracy 0.5824\n",
      "Epoch 30 Batch 50 Loss 1.8968 Accuracy 0.6023\n",
      "Epoch 30 Batch 100 Loss 1.8741 Accuracy 0.6068\n",
      "Epoch 30 Batch 150 Loss 1.8745 Accuracy 0.6073\n",
      "Epoch 30 Batch 200 Loss 1.8787 Accuracy 0.6064\n",
      "Epoch 30 Batch 250 Loss 1.8775 Accuracy 0.6061\n",
      "Epoch 30 Batch 300 Loss 1.8801 Accuracy 0.6057\n",
      "Epoch 30 Batch 350 Loss 1.8808 Accuracy 0.6057\n",
      "Epoch 30 Batch 400 Loss 1.8851 Accuracy 0.6046\n",
      "Epoch 30 Batch 450 Loss 1.8848 Accuracy 0.6046\n",
      "Epoch 30 Batch 500 Loss 1.8854 Accuracy 0.6046\n",
      "Epoch 30 Batch 550 Loss 1.8842 Accuracy 0.6046\n",
      "Epoch 30 Batch 600 Loss 1.8816 Accuracy 0.6050\n",
      "Epoch 30 Batch 650 Loss 1.8829 Accuracy 0.6050\n",
      "Epoch 30 Batch 700 Loss 1.8833 Accuracy 0.6049\n",
      "Epoch 30 Batch 750 Loss 1.8822 Accuracy 0.6050\n",
      "Epoch 30 Batch 800 Loss 1.8820 Accuracy 0.6051\n",
      "Epoch 30 Batch 850 Loss 1.8832 Accuracy 0.6050\n",
      "Epoch 30 Batch 900 Loss 1.8834 Accuracy 0.6051\n",
      "Epoch 30 Batch 950 Loss 1.8846 Accuracy 0.6050\n",
      "Epoch 30 Batch 1000 Loss 1.8846 Accuracy 0.6051\n",
      "Epoch 30 Batch 1050 Loss 1.8852 Accuracy 0.6050\n",
      "Epoch 30 Batch 1100 Loss 1.8847 Accuracy 0.6050\n",
      "Epoch 30 Batch 1150 Loss 1.8854 Accuracy 0.6049\n",
      "Epoch 30 Batch 1200 Loss 1.8860 Accuracy 0.6049\n",
      "Epoch 30 Batch 1250 Loss 1.8862 Accuracy 0.6049\n",
      "Epoch 30 Batch 1300 Loss 1.8862 Accuracy 0.6048\n",
      "Epoch 30 Batch 1350 Loss 1.8859 Accuracy 0.6049\n",
      "Epoch 30 Batch 1400 Loss 1.8856 Accuracy 0.6050\n",
      "Epoch 30 Batch 1450 Loss 1.8857 Accuracy 0.6051\n",
      "Epoch 30 Batch 1500 Loss 1.8853 Accuracy 0.6051\n",
      "Epoch 30 Batch 1550 Loss 1.8852 Accuracy 0.6051\n",
      "Epoch 30 Batch 1600 Loss 1.8854 Accuracy 0.6051\n",
      "Epoch 30 Batch 1650 Loss 1.8859 Accuracy 0.6051\n",
      "Epoch 30 Batch 1700 Loss 1.8860 Accuracy 0.6050\n",
      "Epoch 30 Batch 1750 Loss 1.8853 Accuracy 0.6050\n",
      "Epoch 30 Batch 1800 Loss 1.8857 Accuracy 0.6050\n",
      "Epoch 30 Batch 1850 Loss 1.8852 Accuracy 0.6051\n",
      "Epoch 30 Batch 1900 Loss 1.8849 Accuracy 0.6051\n",
      "Epoch 30 Batch 1950 Loss 1.8847 Accuracy 0.6051\n",
      "Epoch 30 Batch 2000 Loss 1.8845 Accuracy 0.6052\n",
      "Epoch 30 Batch 2050 Loss 1.8846 Accuracy 0.6052\n",
      "Epoch 30 Batch 2100 Loss 1.8847 Accuracy 0.6052\n",
      "Epoch 30 Batch 2150 Loss 1.8853 Accuracy 0.6051\n",
      "Epoch 30 Batch 2200 Loss 1.8856 Accuracy 0.6051\n",
      "Epoch 30 Batch 2250 Loss 1.8859 Accuracy 0.6050\n",
      "Epoch 30 Batch 2300 Loss 1.8865 Accuracy 0.6049\n",
      "Epoch 30 Batch 2350 Loss 1.8870 Accuracy 0.6048\n",
      "Epoch 30 Batch 2400 Loss 1.8871 Accuracy 0.6048\n",
      "Epoch 30 Batch 2450 Loss 1.8873 Accuracy 0.6048\n",
      "Epoch 30 Batch 2500 Loss 1.8873 Accuracy 0.6048\n",
      "Epoch 30 Batch 2550 Loss 1.8870 Accuracy 0.6049\n",
      "Epoch 30 Batch 2600 Loss 1.8868 Accuracy 0.6049\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
      "Epoch 30 Loss 1.8869 Accuracy 0.6049\n",
      "Time taken for 1 epoch: 181.0542197227478 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6APsFrgImLW"
   },
   "source": [
    "Для оценки используются следующие шаги:\n",
    "\n",
    "Закодируйте входное предложение с помощью русского токенизатора ( tokenizer_pt ). Кроме того, добавьте начальный и конечный токены, чтобы ввод был эквивалентен тому, с чем обучается модель. Это вход энкодера.\n",
    "Вход декодера - это start token == tokenizer_en.vocab_size .\n",
    "Рассчитайте маски заполнения и маски прогнозирования.\n",
    "Затем decoder выводит прогнозы, глядя на encoder output и собственные выходные данные (самовнимание).\n",
    "Выберите последнее слово и вычислите его argmax.\n",
    "Конкатентируйте предсказанное слово на вход декодера при передаче его в декодер.\n",
    "В этом подходе декодер предсказывает следующее слово на основе предсказанных им предыдущих слов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_ru.vocab_size]\n",
    "  end_token = [tokenizer_ru.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_ru.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_ru.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_ru.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot='decoder_layer4_block2'):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  # print(attention_weights)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5m_4DBHaV7-"
   },
   "source": [
    "После 15-ти эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHzndZ1xNHKn",
    "outputId": "074e9820-d3b7-49ea-e18e-d5d0a09c2765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Осенним вечером шёл дождь!\n",
      "Predicted translation: the eyeball is rained at night .\n"
     ]
    }
   ],
   "source": [
    "translate('Осенним вечером шёл дождь!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAMWA3xvCuDi",
    "outputId": "aca0a9f2-29fe-411c-ceef-fa80b0725801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: он собирается домой\n",
      "Predicted translation: he 's going to come home .\n"
     ]
    }
   ],
   "source": [
    "translate('он собирается домой')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_QhGu_BlhGu"
   },
   "source": [
    "После 30 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "vVDReCSWh9ky",
    "outputId": "b1b85fb5-a97a-4f3b-c213-b01671538853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Осенним вечером шёл дождь!\n",
      "Predicted translation: it was a rained evening rain .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('Осенним вечером шёл дождь!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "jtDIiLQTh9kz",
    "outputId": "ce179e9a-4060-420d-81d1-1a758ea43e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: он собирается домой\n",
      "Predicted translation: he 's going home .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('он собирается домой')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
